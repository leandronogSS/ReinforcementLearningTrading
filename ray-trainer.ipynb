{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gym\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
    "from ray.tune.logger import pretty_print\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ray.init(num_cpus=3, ignore_reinit_error=True, log_to_driver=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = DEFAULT_CONFIG.copy()\n",
    "# config['num_workers'] = 3\n",
    "# config['num_sgd_iter'] = 30\n",
    "# config['sgd_minibatch_size'] = 1024\n",
    "# config['model']['fcnet_hiddens'] = [100, 100]\n",
    "# config['num_cpus_per_worker'] = 0  # This avoids running out of resources in the notebook environment when this cell is re-executed\n",
    "\n",
    "# agent = PPOTrainer(config, 'CartPole-v0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(20):\n",
    "#     result = agent.train()\n",
    "#     print(pretty_print(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = agent.save()\n",
    "# print(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained_config = config.copy()\n",
    "\n",
    "# test_agent = PPOTrainer(trained_config, 'CartPole-v0')\n",
    "# test_agent.restore(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('CartPole-v0')\n",
    "# state = env.reset()\n",
    "# done = False\n",
    "# cumulative_reward = 0\n",
    "\n",
    "# while not done:\n",
    "#     action = test_agent.compute_action(state)\n",
    "#     state, reward, done, _ = env.step(action)\n",
    "#     cumulative_reward += reward\n",
    "\n",
    "# print(cumulative_reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard --logdir=~/ray_results --host=0.0.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import ta\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensortrade.env.default as default\n",
    "from tensortrade.feed.core import Stream, DataFeed, NameSpace\n",
    "from tensortrade.oms.exchanges import Exchange\n",
    "from tensortrade.oms.services.execution.simulated import execute_order\n",
    "from tensortrade.oms.instruments import USD, BTC, ETH\n",
    "from tensortrade.oms.wallets import Wallet, Portfolio\n",
    "from tensortrade.agents import A2CAgent\n",
    "import tensortrade.stochastic as sp \n",
    "from tensortrade.oms.instruments import Instrument\n",
    "from tensortrade.env.default.actions import SimpleOrders, BSH, ManagedRiskOrders\n",
    "from collections import OrderedDict\n",
    "from tensortrade.oms.orders.criteria import Stop, StopDirection\n",
    "from tensortrade.env.default.actions import ManagedRiskOrders\n",
    "from tensortrade.env.default.rewards import RiskAdjustedReturns, SimpleProfit\n",
    "from scipy.signal import savgol_filter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archive/ALICE_USDT-15m.json\n",
      "empty or small\n",
      "archive/BNT_USDT-15m.json\n",
      "empty or small\n",
      "archive/BTC_USDT-15m.json\n",
      "archive/BNB_USDT-15m.json\n",
      "archive/ETC_USDT-15m.json\n",
      "archive/BCH_USDT-15m.json\n",
      "archive/EOS_USDT-15m.json\n",
      "archive/PAXG_USDT-15m.json\n",
      "empty or small\n",
      "archive/ICX_USDT-15m.json\n",
      "archive/NANO_USDT-15m.json\n",
      "archive/ZEC_USDT-15m.json\n",
      "archive/OMG_USDT-15m.json\n",
      "archive/LTC_USDT-15m.json\n",
      "archive/DOT_USDT-15m.json\n",
      "empty or small\n",
      "archive/ATOM_USDT-15m.json\n",
      "archive/XMR_USDT-15m.json\n",
      "archive/WAVES_USDT-15m.json\n",
      "archive/NEO_USDT-15m.json\n",
      "archive/ETH_USDT-15m.json\n",
      "archive/ENJ_USDT-15m.json\n",
      "archive/ADA_USDT-15m.json\n",
      "archive/XRP_USDT-15m.json\n",
      "archive/LINK_USDT-15m.json\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "dfs = []\n",
    "\n",
    "for dirname, _, filenames in os.walk('archive/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        df = pd.read_json(os.path.join(dirname, filename))\n",
    "        if not df.empty and len(df) >= 51197:\n",
    "            df.columns = [\"unix\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "            df.drop(columns=[\"unix\"], inplace=True)\n",
    "            dfs.append(df)\n",
    "        else:\n",
    "            print(\"empty or small\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "numCoins = 4\n",
    "def fetchTaFeatures(data):\n",
    "    data = ta.add_all_ta_features(data, 'open', 'high', 'low', 'close', 'volume', fillna=True)\n",
    "    data.columns = [name.lower() for name in data.columns]\n",
    "    return data\n",
    "\n",
    "def createEnv(config):\n",
    "#     print(config)\n",
    "\n",
    "    coins = [\"coin{}\".format(x) for x in range(numCoins)]\n",
    "    bitfinex_streams = []\n",
    "\n",
    "    with NameSpace(\"bitfinex\"):\n",
    "        for coin in coins:\n",
    "            coinColumns = filter(lambda name: name.startswith(coin), config[\"data\"].columns)\n",
    "            bitfinex_streams += [\n",
    "                Stream.source(list(config[\"data\"][c]), dtype=\"float\").rename(c) for c in coinColumns\n",
    "            ]\n",
    "\n",
    "\n",
    "    feed = DataFeed(bitfinex_streams)\n",
    "    \n",
    "    streams = []\n",
    "    for coin in coins:\n",
    "         streams.append(Stream.source(list(data[coin+\":\"+\"close\"]), dtype=\"float\").rename(\"USD-\"+coin))\n",
    "    streams = tuple(streams)\n",
    "\n",
    "\n",
    "    bitstamp = Exchange(\"bitfinex\", service=execute_order)(\n",
    "        Stream.source(list(config[\"data\"][\"coin0:close\"]), dtype=\"float\").rename(\"USD-TTC0\"),\n",
    "        Stream.source(list(config[\"data\"][\"coin1:close\"]), dtype=\"float\").rename(\"USD-TTC1\"),\n",
    "        Stream.source(list(config[\"data\"][\"coin2:close\"]), dtype=\"float\").rename(\"USD-TTC2\"),\n",
    "        Stream.source(list(config[\"data\"][\"coin3:close\"]), dtype=\"float\").rename(\"USD-TTC3\"),\n",
    "#         Stream.source(list(data[\"coin4:close\"]), dtype=\"float\").rename(\"USD-TTC4\"),\n",
    "#         Stream.source(list(data[\"coin5:close\"]), dtype=\"float\").rename(\"USD-TTC5\"),\n",
    "#         Stream.source(list(data[\"coin6:close\"]), dtype=\"float\").rename(\"USD-TTC6\"),\n",
    "#         Stream.source(list(data[\"coin7:close\"]), dtype=\"float\").rename(\"USD-TTC7\"),\n",
    "#         Stream.source(list(data[\"coin8:close\"]), dtype=\"float\").rename(\"USD-TTC8\"),\n",
    "#         Stream.source(list(data[\"coin9:close\"]), dtype=\"float\").rename(\"USD-TTC9\"),\n",
    "\n",
    "#         Stream.source(list(data[\"coin10:close\"]), dtype=\"float\").rename(\"USD-TTC10\"),\n",
    "#         Stream.source(list(data[\"coin11:close\"]), dtype=\"float\").rename(\"USD-TTC11\"),\n",
    "#         Stream.source(list(data[\"coin12:close\"]), dtype=\"float\").rename(\"USD-TTC12\"),\n",
    "#         Stream.source(list(data[\"coin13:close\"]), dtype=\"float\").rename(\"USD-TTC13\"),\n",
    "#         Stream.source(list(data[\"coin14:close\"]), dtype=\"float\").rename(\"USD-TTC14\"),\n",
    "#         Stream.source(list(data[\"coin15:close\"]), dtype=\"float\").rename(\"USD-TTC15\"),\n",
    "#         Stream.source(list(data[\"coin16:close\"]), dtype=\"float\").rename(\"USD-TTC16\"),\n",
    "#         Stream.source(list(data[\"coin17:close\"]), dtype=\"float\").rename(\"USD-TTC17\"),\n",
    "#         Stream.source(list(data[\"coin18:close\"]), dtype=\"float\").rename(\"USD-TTC18\"),\n",
    "        \n",
    "    )\n",
    "    instruments = []\n",
    "    assets = []\n",
    "    for i, coin in enumerate(coins):\n",
    "        instrument = Instrument(f\"TTC{i}\", 8, f\"TensorTrade Coin{i}\")\n",
    "        instruments.append(instrument)\n",
    "        assets.append(Wallet(bitstamp, 0 * instrument))\n",
    "\n",
    "    \n",
    "    cash = Wallet(bitstamp, 1000 * USD)\n",
    "\n",
    "\n",
    "    portfolio = Portfolio(USD, [cash] + assets)\n",
    "\n",
    "#     portfolio = Portfolio(USD, [cash, asset, asset1     \n",
    "#     ])\n",
    "# https://tensortradex.readthedocs.io/en/latest/api/tensortrade.actions.managed_risk_orders.html\n",
    "#     reward = RiskAdjustedReturns(return_algorithm = \"sortino\", window_size=1)\n",
    "    reward = SimpleProfit()\n",
    "\n",
    "    action_scheme = ManagedRiskOrders(stop=[0.2], trade_sizes=[4], take=[0.05, 0.1, 0.04, 0.01, 0.02])\n",
    "    env = default.create(\n",
    "      feed=feed,\n",
    "      portfolio=portfolio,\n",
    "      action_scheme=action_scheme,\n",
    "      reward_scheme=reward,\n",
    "      window_size=config[\"window_size\"]\n",
    "    )\n",
    "    \n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "\n",
    "coins = [\"coin{}\".format(x) for x in range(numCoins)]\n",
    "dfFinal = []\n",
    "for i, df in enumerate(dfs[:numCoins]):\n",
    "    coin = coins[i]\n",
    "    for column in [\"close\", \"open\", \"high\", \"low\", \"volume\"]:\n",
    "        df[f\"diff_{column}\"] = df[f\"{column}\"].apply(np.log).diff().dropna()\n",
    "        df[f\"soft_{column}\"] = savgol_filter(df[column], 35, 2)\n",
    "        \n",
    "\n",
    "#     ta.add_all_ta_features(\n",
    "#     df,\n",
    "#     **{k: k for k in ['open', 'high', 'low', 'close', 'volume']})\n",
    "    # testar ta normal e ta no soft\n",
    "    dfFinal.append(df.add_prefix(f\"{coin}:\"))\n",
    "\n",
    "data = pd.concat(dfFinal, axis=1)\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler(feature_range=(0.5, 1.5))\n",
    "# # def clean_dataset(df):\n",
    "#     assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n",
    "#     df.dropna(inplace=True)\n",
    "#     indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n",
    "#     return df[indices_to_keep].astype(np.float64)\n",
    "\n",
    "# data = clean_dataset(data)\n",
    "data = data.reset_index(drop=True)\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# data[data.columns] = scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.agents.ppo import PPOTrainer, DEFAULT_CONFIG\n",
    "# from ray.rllib.agents.impala import DEFAULT_CONFIG, ImpalaTrainer\n",
    "\n",
    "config = DEFAULT_CONFIG.copy()\n",
    "config['model'][\"use_lstm\"] = True\n",
    "# config['model'][\"max_seq_len\"] = 32\n",
    "# config['model'][\"lstm_cell_size\"]= 256\n",
    "# config['model'][\"lstm_use_prev_reward\"]=True\n",
    "# config['num_workers'] = 3\n",
    "# config['num_sgd_iter'] = 30\n",
    "# config['sgd_minibatch_size'] = 30\n",
    "# config['model']['fcnet_hiddens'] = [100, 100]\n",
    "# config['num_cpus_per_worker'] = 0  # This avoids running out of resources in the notebook environment when this cell is re-executed\n",
    "# config['vf_clip_param'] = 30\n",
    "# config['train_batch_size']= 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_workers': 2,\n",
       " 'num_envs_per_worker': 1,\n",
       " 'create_env_on_driver': False,\n",
       " 'rollout_fragment_length': 200,\n",
       " 'batch_mode': 'truncate_episodes',\n",
       " 'train_batch_size': 4000,\n",
       " 'model': {'_use_default_native_models': False,\n",
       "  'fcnet_hiddens': [256, 256],\n",
       "  'fcnet_activation': 'tanh',\n",
       "  'conv_filters': None,\n",
       "  'conv_activation': 'relu',\n",
       "  'post_fcnet_hiddens': [],\n",
       "  'post_fcnet_activation': 'relu',\n",
       "  'free_log_std': False,\n",
       "  'no_final_linear': False,\n",
       "  'vf_share_layers': False,\n",
       "  'use_lstm': True,\n",
       "  'max_seq_len': 32,\n",
       "  'lstm_cell_size': 256,\n",
       "  'lstm_use_prev_action': False,\n",
       "  'lstm_use_prev_reward': False,\n",
       "  '_time_major': False,\n",
       "  'use_attention': False,\n",
       "  'attention_num_transformer_units': 1,\n",
       "  'attention_dim': 64,\n",
       "  'attention_num_heads': 1,\n",
       "  'attention_head_dim': 32,\n",
       "  'attention_memory_inference': 50,\n",
       "  'attention_memory_training': 50,\n",
       "  'attention_position_wise_mlp_dim': 32,\n",
       "  'attention_init_gru_gate_bias': 2.0,\n",
       "  'attention_use_n_prev_actions': 0,\n",
       "  'attention_use_n_prev_rewards': 0,\n",
       "  'num_framestacks': 'auto',\n",
       "  'dim': 84,\n",
       "  'grayscale': False,\n",
       "  'zero_mean': True,\n",
       "  'custom_model': None,\n",
       "  'custom_model_config': {},\n",
       "  'custom_action_dist': None,\n",
       "  'custom_preprocessor': None,\n",
       "  'lstm_use_prev_action_reward': -1,\n",
       "  'framestack': True},\n",
       " 'optimizer': {},\n",
       " 'gamma': 0.99,\n",
       " 'horizon': None,\n",
       " 'soft_horizon': False,\n",
       " 'no_done_at_end': False,\n",
       " 'env': None,\n",
       " 'env_config': {},\n",
       " 'env_task_fn': None,\n",
       " 'render_env': False,\n",
       " 'record_env': False,\n",
       " 'normalize_actions': False,\n",
       " 'clip_rewards': None,\n",
       " 'clip_actions': True,\n",
       " 'preprocessor_pref': 'deepmind',\n",
       " 'lr': 5e-05,\n",
       " 'log_level': 'WARN',\n",
       " 'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       " 'ignore_worker_failures': False,\n",
       " 'log_sys_usage': True,\n",
       " 'fake_sampler': False,\n",
       " 'framework': 'tf',\n",
       " 'eager_tracing': False,\n",
       " 'explore': True,\n",
       " 'exploration_config': {'type': 'StochasticSampling'},\n",
       " 'evaluation_interval': None,\n",
       " 'evaluation_num_episodes': 10,\n",
       " 'evaluation_parallel_to_training': False,\n",
       " 'in_evaluation': False,\n",
       " 'evaluation_config': {},\n",
       " 'evaluation_num_workers': 0,\n",
       " 'custom_eval_function': None,\n",
       " 'sample_async': False,\n",
       " 'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       " 'observation_filter': 'NoFilter',\n",
       " 'synchronize_filters': True,\n",
       " 'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "  'inter_op_parallelism_threads': 2,\n",
       "  'gpu_options': {'allow_growth': True},\n",
       "  'log_device_placement': False,\n",
       "  'device_count': {'CPU': 1},\n",
       "  'allow_soft_placement': True},\n",
       " 'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "  'inter_op_parallelism_threads': 8},\n",
       " 'compress_observations': False,\n",
       " 'collect_metrics_timeout': 180,\n",
       " 'metrics_smoothing_episodes': 100,\n",
       " 'remote_worker_envs': False,\n",
       " 'remote_env_batch_wait_ms': 0,\n",
       " 'min_iter_time_s': 0,\n",
       " 'timesteps_per_iteration': 0,\n",
       " 'seed': None,\n",
       " 'extra_python_environs_for_driver': {},\n",
       " 'extra_python_environs_for_worker': {},\n",
       " 'num_gpus': 0,\n",
       " '_fake_gpus': False,\n",
       " 'num_cpus_per_worker': 1,\n",
       " 'num_gpus_per_worker': 0,\n",
       " 'custom_resources_per_worker': {},\n",
       " 'num_cpus_for_driver': 1,\n",
       " 'placement_strategy': 'PACK',\n",
       " 'input': 'sampler',\n",
       " 'input_evaluation': ['is', 'wis'],\n",
       " 'postprocess_inputs': False,\n",
       " 'shuffle_buffer_size': 0,\n",
       " 'output': None,\n",
       " 'output_compress_columns': ['obs', 'new_obs'],\n",
       " 'output_max_file_size': 67108864,\n",
       " 'multiagent': {'policies': {},\n",
       "  'policy_mapping_fn': None,\n",
       "  'policies_to_train': None,\n",
       "  'observation_fn': None,\n",
       "  'replay_mode': 'independent',\n",
       "  'count_steps_by': 'env_steps'},\n",
       " 'logger_config': None,\n",
       " 'simple_optimizer': -1,\n",
       " 'monitor': -1,\n",
       " 'use_critic': True,\n",
       " 'use_gae': True,\n",
       " 'lambda': 1.0,\n",
       " 'kl_coeff': 0.2,\n",
       " 'sgd_minibatch_size': 128,\n",
       " 'shuffle_sequences': True,\n",
       " 'num_sgd_iter': 30,\n",
       " 'lr_schedule': None,\n",
       " 'vf_loss_coeff': 1.0,\n",
       " 'entropy_coeff': 0.0,\n",
       " 'entropy_coeff_schedule': None,\n",
       " 'clip_param': 0.3,\n",
       " 'vf_clip_param': 10.0,\n",
       " 'grad_clip': None,\n",
       " 'kl_target': 0.01,\n",
       " 'vf_share_layers': -1}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del config[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-06 09:17:10,847\tINFO trainer.py:671 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2021-07-06 09:17:10,848\tINFO trainer.py:698 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "seq_in (InputLayer)             [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_default_policy/Sequ [()]                 0           seq_in[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_default_policy/Sequ [()]                 0           tf_op_layer_default_policy/Sequen\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_default_policy/Sequ [(None, 1)]          0           seq_in[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_default_policy/Sequ [(None,)]            0           tf_op_layer_default_policy/Sequen\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_default_policy/Sequ [(None, 1)]          0           tf_op_layer_default_policy/Sequen\n",
      "__________________________________________________________________________________________________\n",
      "inputs (InputLayer)             [(None, None, 256)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "h (InputLayer)                  [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "c (InputLayer)                  [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_default_policy/Sequ [(None, None)]       0           tf_op_layer_default_policy/Sequen\n",
      "                                                                 tf_op_layer_default_policy/Sequen\n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, None, 256),  525312      inputs[0][0]                     \n",
      "                                                                 h[0][0]                          \n",
      "                                                                 c[0][0]                          \n",
      "                                                                 tf_op_layer_default_policy/Sequen\n",
      "__________________________________________________________________________________________________\n",
      "logits (Dense)                  (None, None, 41)     10537       lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "values (Dense)                  (None, None, 1)      257         lstm[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 536,106\n",
      "Trainable params: 536,106\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-06 09:17:31,471\tINFO trainable.py:104 -- Trainable.setup took 20.761 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2021-07-06 09:17:31,474\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "# ray.init()\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "register_env(\"TradingEnv\", createEnv)\n",
    "config[\"env_config\"][\"data\"]=norm_data\n",
    "config[\"env_config\"][\"window_size\"]=10\n",
    "\n",
    "agent = PPOTrainer(config, 'TradingEnv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 4000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_09-19-08\n",
      "done: false\n",
      "episode_len_mean: .nan\n",
      "episode_media: {}\n",
      "episode_reward_max: .nan\n",
      "episode_reward_mean: .nan\n",
      "episode_reward_min: .nan\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 0\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.6364643573760986\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.04399183392524719\n",
      "        model: {}\n",
      "        policy_loss: -0.13473521173000336\n",
      "        total_loss: -0.12228156626224518\n",
      "        vf_explained_var: -0.4977198541164398\n",
      "        vf_loss: 0.00365527905523777\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_steps_sampled: 4000\n",
      "  num_steps_trained: 4000\n",
      "iterations_since_restore: 1\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 89.01007194244605\n",
      "  ram_util_percent: 60.30935251798561\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf: {}\n",
      "time_since_restore: 97.45520401000977\n",
      "time_this_iter_s: 97.45520401000977\n",
      "time_total_s: 97.45520401000977\n",
      "timers:\n",
      "  learn_throughput: 45.942\n",
      "  learn_time_ms: 87066.226\n",
      "  load_throughput: 26130.944\n",
      "  load_time_ms: 153.075\n",
      "  sample_throughput: 393.567\n",
      "  sample_time_ms: 10163.452\n",
      "  update_time_ms: 6.83\n",
      "timestamp: 1625573948\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 4000\n",
      "training_iteration: 1\n",
      "\n",
      "agent_timesteps_total: 8000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_09-20-43\n",
      "done: false\n",
      "episode_len_mean: .nan\n",
      "episode_media: {}\n",
      "episode_reward_max: .nan\n",
      "episode_reward_mean: .nan\n",
      "episode_reward_min: .nan\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 0\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.6074564456939697\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.029353786259889603\n",
      "        model: {}\n",
      "        policy_loss: -0.15356580913066864\n",
      "        total_loss: -0.14224262535572052\n",
      "        vf_explained_var: -0.5982229709625244\n",
      "        vf_loss: 0.002517056418582797\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_steps_sampled: 8000\n",
      "  num_steps_trained: 8000\n",
      "iterations_since_restore: 2\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 85.03851851851853\n",
      "  ram_util_percent: 66.25037037037038\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf: {}\n",
      "time_since_restore: 192.20758605003357\n",
      "time_this_iter_s: 94.7523820400238\n",
      "time_total_s: 192.20758605003357\n",
      "timers:\n",
      "  learn_throughput: 47.692\n",
      "  learn_time_ms: 83871.243\n",
      "  load_throughput: 37065.007\n",
      "  load_time_ms: 107.919\n",
      "  sample_throughput: 331.155\n",
      "  sample_time_ms: 12078.952\n",
      "  update_time_ms: 7.4\n",
      "timestamp: 1625574043\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 8000\n",
      "training_iteration: 2\n",
      "\n",
      "agent_timesteps_total: 12000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_09-22-17\n",
      "done: false\n",
      "episode_len_mean: .nan\n",
      "episode_media: {}\n",
      "episode_reward_max: .nan\n",
      "episode_reward_mean: .nan\n",
      "episode_reward_min: .nan\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 0\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.44999998807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.5863535404205322\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.035556115210056305\n",
      "        model: {}\n",
      "        policy_loss: -0.1666514128446579\n",
      "        total_loss: -0.14790908992290497\n",
      "        vf_explained_var: -0.7637589573860168\n",
      "        vf_loss: 0.0027421058621257544\n",
      "  num_agent_steps_sampled: 12000\n",
      "  num_agent_steps_trained: 12000\n",
      "  num_steps_sampled: 12000\n",
      "  num_steps_trained: 12000\n",
      "iterations_since_restore: 3\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 80.72537313432836\n",
      "  ram_util_percent: 69.14029850746267\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf: {}\n",
      "time_since_restore: 285.72464394569397\n",
      "time_this_iter_s: 93.5170578956604\n",
      "time_total_s: 285.72464394569397\n",
      "timers:\n",
      "  learn_throughput: 49.593\n",
      "  learn_time_ms: 80656.376\n",
      "  load_throughput: 42760.805\n",
      "  load_time_ms: 93.544\n",
      "  sample_throughput: 276.689\n",
      "  sample_time_ms: 14456.643\n",
      "  update_time_ms: 6.516\n",
      "timestamp: 1625574137\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 12000\n",
      "training_iteration: 3\n",
      "\n",
      "agent_timesteps_total: 16000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_09-23-52\n",
      "done: false\n",
      "episode_len_mean: .nan\n",
      "episode_media: {}\n",
      "episode_reward_max: .nan\n",
      "episode_reward_mean: .nan\n",
      "episode_reward_min: .nan\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 0\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.675000011920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.545494318008423\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.027875175699591637\n",
      "        model: {}\n",
      "        policy_loss: -0.14699521660804749\n",
      "        total_loss: -0.1250336915254593\n",
      "        vf_explained_var: -0.2150077223777771\n",
      "        vf_loss: 0.0031457848381251097\n",
      "  num_agent_steps_sampled: 16000\n",
      "  num_agent_steps_trained: 16000\n",
      "  num_steps_sampled: 16000\n",
      "  num_steps_trained: 16000\n",
      "iterations_since_restore: 4\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 79.30661764705883\n",
      "  ram_util_percent: 70.79926470588236\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf: {}\n",
      "time_since_restore: 381.3112428188324\n",
      "time_this_iter_s: 95.58659887313843\n",
      "time_total_s: 381.3112428188324\n",
      "timers:\n",
      "  learn_throughput: 50.72\n",
      "  learn_time_ms: 78864.483\n",
      "  load_throughput: 46676.537\n",
      "  load_time_ms: 85.696\n",
      "  sample_throughput: 244.679\n",
      "  sample_time_ms: 16347.983\n",
      "  update_time_ms: 6.093\n",
      "timestamp: 1625574232\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 16000\n",
      "training_iteration: 4\n",
      "\n",
      "agent_timesteps_total: 20000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_09-25-31\n",
      "done: false\n",
      "episode_len_mean: .nan\n",
      "episode_media: {}\n",
      "episode_reward_max: .nan\n",
      "episode_reward_mean: .nan\n",
      "episode_reward_min: .nan\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 0\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.0125000476837158\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.5284297466278076\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.022620540112257004\n",
      "        model: {}\n",
      "        policy_loss: -0.12163561582565308\n",
      "        total_loss: -0.09639380127191544\n",
      "        vf_explained_var: -0.6469826102256775\n",
      "        vf_loss: 0.002338519785553217\n",
      "  num_agent_steps_sampled: 20000\n",
      "  num_agent_steps_trained: 20000\n",
      "  num_steps_sampled: 20000\n",
      "  num_steps_trained: 20000\n",
      "iterations_since_restore: 5\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 78.1\n",
      "  ram_util_percent: 70.69928571428571\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf: {}\n",
      "time_since_restore: 479.7661669254303\n",
      "time_this_iter_s: 98.4549241065979\n",
      "time_total_s: 479.7661669254303\n",
      "timers:\n",
      "  learn_throughput: 51.603\n",
      "  learn_time_ms: 77515.153\n",
      "  load_throughput: 48747.255\n",
      "  load_time_ms: 82.056\n",
      "  sample_throughput: 218.229\n",
      "  sample_time_ms: 18329.33\n",
      "  update_time_ms: 6.187\n",
      "timestamp: 1625574331\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 20000\n",
      "training_iteration: 5\n",
      "\n",
      "agent_timesteps_total: 24000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_09-27-23\n",
      "done: false\n",
      "episode_len_mean: .nan\n",
      "episode_media: {}\n",
      "episode_reward_max: .nan\n",
      "episode_reward_mean: .nan\n",
      "episode_reward_min: .nan\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 0\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.5286827087402344\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018297171220183372\n",
      "        model: {}\n",
      "        policy_loss: -0.1505245417356491\n",
      "        total_loss: -0.1204022541642189\n",
      "        vf_explained_var: -0.2469504475593567\n",
      "        vf_loss: 0.0023334661964327097\n",
      "  num_agent_steps_sampled: 24000\n",
      "  num_agent_steps_trained: 24000\n",
      "  num_steps_sampled: 24000\n",
      "  num_steps_trained: 24000\n",
      "iterations_since_restore: 6\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 82.00377358490564\n",
      "  ram_util_percent: 71.75974842767295\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf: {}\n",
      "time_since_restore: 591.6695780754089\n",
      "time_this_iter_s: 111.90341114997864\n",
      "time_total_s: 591.6695780754089\n",
      "timers:\n",
      "  learn_throughput: 51.46\n",
      "  learn_time_ms: 77729.656\n",
      "  load_throughput: 49714.492\n",
      "  load_time_ms: 80.459\n",
      "  sample_throughput: 192.512\n",
      "  sample_time_ms: 20777.895\n",
      "  update_time_ms: 6.049\n",
      "timestamp: 1625574443\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 24000\n",
      "training_iteration: 6\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 28000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_09-29-18\n",
      "done: false\n",
      "episode_len_mean: .nan\n",
      "episode_media: {}\n",
      "episode_reward_max: .nan\n",
      "episode_reward_mean: .nan\n",
      "episode_reward_min: .nan\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 0\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.535214424133301\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019405603408813477\n",
      "        model: {}\n",
      "        policy_loss: -0.15024806559085846\n",
      "        total_loss: -0.11861623823642731\n",
      "        vf_explained_var: -0.7679751515388489\n",
      "        vf_loss: 0.002159563824534416\n",
      "  num_agent_steps_sampled: 28000\n",
      "  num_agent_steps_trained: 28000\n",
      "  num_steps_sampled: 28000\n",
      "  num_steps_trained: 28000\n",
      "iterations_since_restore: 7\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 78.6951219512195\n",
      "  ram_util_percent: 72.23536585365852\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf: {}\n",
      "time_since_restore: 706.2404282093048\n",
      "time_this_iter_s: 114.57085013389587\n",
      "time_total_s: 706.2404282093048\n",
      "timers:\n",
      "  learn_throughput: 51.352\n",
      "  learn_time_ms: 77894.256\n",
      "  load_throughput: 51720.574\n",
      "  load_time_ms: 77.339\n",
      "  sample_throughput: 174.689\n",
      "  sample_time_ms: 22897.885\n",
      "  update_time_ms: 5.88\n",
      "timestamp: 1625574558\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 28000\n",
      "training_iteration: 7\n",
      "\n",
      "agent_timesteps_total: 32000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_09-31-13\n",
      "done: false\n",
      "episode_len_mean: .nan\n",
      "episode_media: {}\n",
      "episode_reward_max: .nan\n",
      "episode_reward_mean: .nan\n",
      "episode_reward_min: .nan\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 0\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 1.5187499523162842\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.4996302127838135\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02130175195634365\n",
      "        model: {}\n",
      "        policy_loss: -0.1277896910905838\n",
      "        total_loss: -0.09333191066980362\n",
      "        vf_explained_var: -0.784670352935791\n",
      "        vf_loss: 0.0021057426929473877\n",
      "  num_agent_steps_sampled: 32000\n",
      "  num_agent_steps_trained: 32000\n",
      "  num_steps_sampled: 32000\n",
      "  num_steps_trained: 32000\n",
      "iterations_since_restore: 8\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 78.51158536585366\n",
      "  ram_util_percent: 72.33048780487805\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf: {}\n",
      "time_since_restore: 821.4773001670837\n",
      "time_this_iter_s: 115.23687195777893\n",
      "time_total_s: 821.4773001670837\n",
      "timers:\n",
      "  learn_throughput: 51.726\n",
      "  learn_time_ms: 77330.305\n",
      "  load_throughput: 53043.986\n",
      "  load_time_ms: 75.409\n",
      "  sample_throughput: 158.365\n",
      "  sample_time_ms: 25258.089\n",
      "  update_time_ms: 5.786\n",
      "timestamp: 1625574673\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 32000\n",
      "training_iteration: 8\n",
      "\n",
      "agent_timesteps_total: 36000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_09-33-07\n",
      "done: false\n",
      "episode_len_mean: .nan\n",
      "episode_media: {}\n",
      "episode_reward_max: .nan\n",
      "episode_reward_mean: .nan\n",
      "episode_reward_min: .nan\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 0\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.475473165512085\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014477661810815334\n",
      "        model: {}\n",
      "        policy_loss: -0.11773277074098587\n",
      "        total_loss: -0.08275655657052994\n",
      "        vf_explained_var: -0.5903685688972473\n",
      "        vf_loss: 0.001994289690628648\n",
      "  num_agent_steps_sampled: 36000\n",
      "  num_agent_steps_trained: 36000\n",
      "  num_steps_sampled: 36000\n",
      "  num_steps_trained: 36000\n",
      "iterations_since_restore: 9\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 72.51296296296296\n",
      "  ram_util_percent: 71.96172839506173\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf: {}\n",
      "time_since_restore: 935.3392641544342\n",
      "time_this_iter_s: 113.86196398735046\n",
      "time_total_s: 935.3392641544342\n",
      "timers:\n",
      "  learn_throughput: 52.226\n",
      "  learn_time_ms: 76589.8\n",
      "  load_throughput: 53971.388\n",
      "  load_time_ms: 74.113\n",
      "  sample_throughput: 146.828\n",
      "  sample_time_ms: 27242.805\n",
      "  update_time_ms: 5.672\n",
      "timestamp: 1625574787\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 36000\n",
      "training_iteration: 9\n",
      "\n",
      "agent_timesteps_total: 40000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_09-35-05\n",
      "done: false\n",
      "episode_len_mean: .nan\n",
      "episode_media: {}\n",
      "episode_reward_max: .nan\n",
      "episode_reward_mean: .nan\n",
      "episode_reward_min: .nan\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 0\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.5028979778289795\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014778881333768368\n",
      "        model: {}\n",
      "        policy_loss: -0.1338895559310913\n",
      "        total_loss: -0.09739139676094055\n",
      "        vf_explained_var: -0.43649452924728394\n",
      "        vf_loss: 0.002830020384863019\n",
      "  num_agent_steps_sampled: 40000\n",
      "  num_agent_steps_trained: 40000\n",
      "  num_steps_sampled: 40000\n",
      "  num_steps_trained: 40000\n",
      "iterations_since_restore: 10\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 72.67321428571428\n",
      "  ram_util_percent: 72.16904761904763\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf: {}\n",
      "time_since_restore: 1053.5912160873413\n",
      "time_this_iter_s: 118.2519519329071\n",
      "time_total_s: 1053.5912160873413\n",
      "timers:\n",
      "  learn_throughput: 52.631\n",
      "  learn_time_ms: 76001.43\n",
      "  load_throughput: 54671.974\n",
      "  load_time_ms: 73.164\n",
      "  sample_throughput: 136.68\n",
      "  sample_time_ms: 29265.436\n",
      "  update_time_ms: 5.597\n",
      "timestamp: 1625574905\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 40000\n",
      "training_iteration: 10\n",
      "\n",
      "agent_timesteps_total: 44000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_09-37-09\n",
      "done: false\n",
      "episode_len_mean: .nan\n",
      "episode_media: {}\n",
      "episode_reward_max: .nan\n",
      "episode_reward_mean: .nan\n",
      "episode_reward_min: .nan\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 0\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.4864609241485596\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014196476899087429\n",
      "        model: {}\n",
      "        policy_loss: -0.11625947803258896\n",
      "        total_loss: -0.08205333352088928\n",
      "        vf_explained_var: -0.691288411617279\n",
      "        vf_loss: 0.0018647890537977219\n",
      "  num_agent_steps_sampled: 44000\n",
      "  num_agent_steps_trained: 44000\n",
      "  num_steps_sampled: 44000\n",
      "  num_steps_trained: 44000\n",
      "iterations_since_restore: 11\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 73.39830508474576\n",
      "  ram_util_percent: 72.91299435028247\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf: {}\n",
      "time_since_restore: 1177.6892709732056\n",
      "time_this_iter_s: 124.09805488586426\n",
      "time_total_s: 1177.6892709732056\n",
      "timers:\n",
      "  learn_throughput: 53.701\n",
      "  learn_time_ms: 74486.536\n",
      "  load_throughput: 62698.688\n",
      "  load_time_ms: 63.797\n",
      "  sample_throughput: 119.565\n",
      "  sample_time_ms: 33454.584\n",
      "  update_time_ms: 10.697\n",
      "timestamp: 1625575029\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 44000\n",
      "training_iteration: 11\n",
      "\n",
      "agent_timesteps_total: 48000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_09-39-24\n",
      "done: false\n",
      "episode_len_mean: .nan\n",
      "episode_media: {}\n",
      "episode_reward_max: .nan\n",
      "episode_reward_mean: .nan\n",
      "episode_reward_min: .nan\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 0\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.470257043838501\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015054509043693542\n",
      "        model: {}\n",
      "        policy_loss: -0.14387471973896027\n",
      "        total_loss: -0.10745692998170853\n",
      "        vf_explained_var: -0.4272478520870209\n",
      "        vf_loss: 0.002121732337400317\n",
      "  num_agent_steps_sampled: 48000\n",
      "  num_agent_steps_trained: 48000\n",
      "  num_steps_sampled: 48000\n",
      "  num_steps_trained: 48000\n",
      "iterations_since_restore: 12\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 75.99947916666666\n",
      "  ram_util_percent: 73.99010416666668\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf: {}\n",
      "time_since_restore: 1312.5214221477509\n",
      "time_this_iter_s: 134.8321511745453\n",
      "time_total_s: 1312.5214221477509\n",
      "timers:\n",
      "  learn_throughput: 54.156\n",
      "  learn_time_ms: 73860.832\n",
      "  load_throughput: 62756.312\n",
      "  load_time_ms: 63.739\n",
      "  sample_throughput: 105.017\n",
      "  sample_time_ms: 38088.986\n",
      "  update_time_ms: 10.366\n",
      "timestamp: 1625575164\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 48000\n",
      "training_iteration: 12\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 52000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_09-41-40\n",
      "done: false\n",
      "episode_len_mean: .nan\n",
      "episode_media: {}\n",
      "episode_reward_max: .nan\n",
      "episode_reward_mean: .nan\n",
      "episode_reward_min: .nan\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 0\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.4932808876037598\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013356160372495651\n",
      "        model: {}\n",
      "        policy_loss: -0.14708316326141357\n",
      "        total_loss: -0.11458097398281097\n",
      "        vf_explained_var: -0.470047265291214\n",
      "        vf_loss: 0.002075171796604991\n",
      "  num_agent_steps_sampled: 52000\n",
      "  num_agent_steps_trained: 52000\n",
      "  num_steps_sampled: 52000\n",
      "  num_steps_trained: 52000\n",
      "iterations_since_restore: 13\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 70.91030927835052\n",
      "  ram_util_percent: 74.6520618556701\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf: {}\n",
      "time_since_restore: 1448.8983159065247\n",
      "time_this_iter_s: 136.3768937587738\n",
      "time_total_s: 1448.8983159065247\n",
      "timers:\n",
      "  learn_throughput: 54.272\n",
      "  learn_time_ms: 73702.634\n",
      "  load_throughput: 63083.316\n",
      "  load_time_ms: 63.408\n",
      "  sample_throughput: 94.043\n",
      "  sample_time_ms: 42533.689\n",
      "  update_time_ms: 10.365\n",
      "timestamp: 1625575300\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 52000\n",
      "training_iteration: 13\n",
      "\n",
      "agent_timesteps_total: 56000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_09-44-08\n",
      "done: false\n",
      "episode_len_mean: .nan\n",
      "episode_media: {}\n",
      "episode_reward_max: .nan\n",
      "episode_reward_mean: .nan\n",
      "episode_reward_min: .nan\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 0\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.4753146171569824\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013523569330573082\n",
      "        model: {}\n",
      "        policy_loss: -0.133308544754982\n",
      "        total_loss: -0.10066228359937668\n",
      "        vf_explained_var: -0.6906779408454895\n",
      "        vf_loss: 0.001837894320487976\n",
      "  num_agent_steps_sampled: 56000\n",
      "  num_agent_steps_trained: 56000\n",
      "  num_steps_sampled: 56000\n",
      "  num_steps_trained: 56000\n",
      "iterations_since_restore: 14\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 72.7957345971564\n",
      "  ram_util_percent: 75.32369668246447\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf: {}\n",
      "time_since_restore: 1596.809693813324\n",
      "time_this_iter_s: 147.91137790679932\n",
      "time_total_s: 1596.809693813324\n",
      "timers:\n",
      "  learn_throughput: 54.088\n",
      "  learn_time_ms: 73954.153\n",
      "  load_throughput: 62787.619\n",
      "  load_time_ms: 63.707\n",
      "  sample_throughput: 84.185\n",
      "  sample_time_ms: 47514.562\n",
      "  update_time_ms: 10.368\n",
      "timestamp: 1625575448\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 56000\n",
      "training_iteration: 14\n",
      "\n",
      "agent_timesteps_total: 60000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_09-46-42\n",
      "done: false\n",
      "episode_len_mean: .nan\n",
      "episode_media: {}\n",
      "episode_reward_max: .nan\n",
      "episode_reward_mean: .nan\n",
      "episode_reward_min: .nan\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 0\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.4328901767730713\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015403206460177898\n",
      "        model: {}\n",
      "        policy_loss: -0.11321631819009781\n",
      "        total_loss: -0.07637752592563629\n",
      "        vf_explained_var: -0.563779890537262\n",
      "        vf_loss: 0.0017483619740232825\n",
      "  num_agent_steps_sampled: 60000\n",
      "  num_agent_steps_trained: 60000\n",
      "  num_steps_sampled: 60000\n",
      "  num_steps_trained: 60000\n",
      "iterations_since_restore: 15\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 71.09311926605504\n",
      "  ram_util_percent: 75.87110091743118\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf: {}\n",
      "time_since_restore: 1750.2047038078308\n",
      "time_this_iter_s: 153.39500999450684\n",
      "time_total_s: 1750.2047038078308\n",
      "timers:\n",
      "  learn_throughput: 53.691\n",
      "  learn_time_ms: 74499.874\n",
      "  load_throughput: 63212.687\n",
      "  load_time_ms: 63.278\n",
      "  sample_throughput: 76.243\n",
      "  sample_time_ms: 52463.52\n",
      "  update_time_ms: 10.22\n",
      "timestamp: 1625575602\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 60000\n",
      "training_iteration: 15\n",
      "\n",
      "agent_timesteps_total: 64000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_09-49-16\n",
      "done: false\n",
      "episode_len_mean: .nan\n",
      "episode_media: {}\n",
      "episode_reward_max: .nan\n",
      "episode_reward_mean: .nan\n",
      "episode_reward_min: .nan\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 0\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.5065908432006836\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013992078602313995\n",
      "        model: {}\n",
      "        policy_loss: -0.1293441206216812\n",
      "        total_loss: -0.09508489072322845\n",
      "        vf_explained_var: -0.08875274658203125\n",
      "        vf_loss: 0.002383548766374588\n",
      "  num_agent_steps_sampled: 64000\n",
      "  num_agent_steps_trained: 64000\n",
      "  num_steps_sampled: 64000\n",
      "  num_steps_trained: 64000\n",
      "iterations_since_restore: 16\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 70.44337899543379\n",
      "  ram_util_percent: 76.72465753424657\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf: {}\n",
      "time_since_restore: 1903.8442516326904\n",
      "time_this_iter_s: 153.63954782485962\n",
      "time_total_s: 1903.8442516326904\n",
      "timers:\n",
      "  learn_throughput: 54.1\n",
      "  learn_time_ms: 73936.762\n",
      "  load_throughput: 63680.823\n",
      "  load_time_ms: 62.813\n",
      "  sample_throughput: 69.93\n",
      "  sample_time_ms: 57200.288\n",
      "  update_time_ms: 10.202\n",
      "timestamp: 1625575756\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 64000\n",
      "training_iteration: 16\n",
      "\n",
      "agent_timesteps_total: 68000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_09-52-08\n",
      "done: false\n",
      "episode_len_mean: .nan\n",
      "episode_media: {}\n",
      "episode_reward_max: .nan\n",
      "episode_reward_mean: .nan\n",
      "episode_reward_min: .nan\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 0\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.4880428314208984\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012940529733896255\n",
      "        model: {}\n",
      "        policy_loss: -0.10955847054719925\n",
      "        total_loss: -0.07842973619699478\n",
      "        vf_explained_var: -0.5949919819831848\n",
      "        vf_loss: 0.0016486035892739892\n",
      "  num_agent_steps_sampled: 68000\n",
      "  num_agent_steps_trained: 68000\n",
      "  num_steps_sampled: 68000\n",
      "  num_steps_trained: 68000\n",
      "iterations_since_restore: 17\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 78.20367346938775\n",
      "  ram_util_percent: 77.01714285714284\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf: {}\n",
      "time_since_restore: 2076.289781332016\n",
      "time_this_iter_s: 172.44552969932556\n",
      "time_total_s: 2076.289781332016\n",
      "timers:\n",
      "  learn_throughput: 53.774\n",
      "  learn_time_ms: 74386.053\n",
      "  load_throughput: 63494.527\n",
      "  load_time_ms: 62.998\n",
      "  sample_throughput: 63.961\n",
      "  sample_time_ms: 62538.141\n",
      "  update_time_ms: 10.266\n",
      "timestamp: 1625575928\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 68000\n",
      "training_iteration: 17\n",
      "\n",
      "agent_timesteps_total: 72000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_09-55-22\n",
      "done: false\n",
      "episode_len_mean: .nan\n",
      "episode_media: {}\n",
      "episode_reward_max: .nan\n",
      "episode_reward_mean: .nan\n",
      "episode_reward_min: .nan\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 0\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.4860239028930664\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01360455621033907\n",
      "        model: {}\n",
      "        policy_loss: -0.11929197609424591\n",
      "        total_loss: -0.08658231794834137\n",
      "        vf_explained_var: -0.3081724941730499\n",
      "        vf_loss: 0.001716785947792232\n",
      "  num_agent_steps_sampled: 72000\n",
      "  num_agent_steps_trained: 72000\n",
      "  num_steps_sampled: 72000\n",
      "  num_steps_trained: 72000\n",
      "iterations_since_restore: 18\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 84.271119133574\n",
      "  ram_util_percent: 79.1841155234657\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf: {}\n",
      "time_since_restore: 2270.4845654964447\n",
      "time_this_iter_s: 194.1947841644287\n",
      "time_total_s: 2270.4845654964447\n",
      "timers:\n",
      "  learn_throughput: 52.47\n",
      "  learn_time_ms: 76234.376\n",
      "  load_throughput: 63149.635\n",
      "  load_time_ms: 63.342\n",
      "  sample_throughput: 58.321\n",
      "  sample_time_ms: 68585.37\n",
      "  update_time_ms: 10.279\n",
      "timestamp: 1625576122\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 72000\n",
      "training_iteration: 18\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 76000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_09-58-25\n",
      "done: false\n",
      "episode_len_mean: .nan\n",
      "episode_media: {}\n",
      "episode_reward_max: .nan\n",
      "episode_reward_mean: .nan\n",
      "episode_reward_min: .nan\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 0\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.4653847217559814\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01238240860402584\n",
      "        model: {}\n",
      "        policy_loss: -0.11440712958574295\n",
      "        total_loss: -0.0845009833574295\n",
      "        vf_explained_var: -0.5264458060264587\n",
      "        vf_loss: 0.0016974672907963395\n",
      "  num_agent_steps_sampled: 76000\n",
      "  num_agent_steps_trained: 76000\n",
      "  num_steps_sampled: 76000\n",
      "  num_steps_trained: 76000\n",
      "iterations_since_restore: 19\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 80.24807692307692\n",
      "  ram_util_percent: 81.76038461538462\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf: {}\n",
      "time_since_restore: 2453.307934522629\n",
      "time_this_iter_s: 182.82336902618408\n",
      "time_total_s: 2453.307934522629\n",
      "timers:\n",
      "  learn_throughput: 51.922\n",
      "  learn_time_ms: 77038.258\n",
      "  load_throughput: 63586.141\n",
      "  load_time_ms: 62.907\n",
      "  sample_throughput: 53.563\n",
      "  sample_time_ms: 74677.76\n",
      "  update_time_ms: 10.473\n",
      "timestamp: 1625576305\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 76000\n",
      "training_iteration: 19\n",
      "\n",
      "agent_timesteps_total: 80000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_10-01-39\n",
      "done: false\n",
      "episode_len_mean: .nan\n",
      "episode_media: {}\n",
      "episode_reward_max: .nan\n",
      "episode_reward_mean: .nan\n",
      "episode_reward_min: .nan\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 0\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.421215772628784\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01455003209412098\n",
      "        model: {}\n",
      "        policy_loss: -0.16322065889835358\n",
      "        total_loss: -0.12816086411476135\n",
      "        vf_explained_var: -0.4144851863384247\n",
      "        vf_loss: 0.0019129959400743246\n",
      "  num_agent_steps_sampled: 80000\n",
      "  num_agent_steps_trained: 80000\n",
      "  num_steps_sampled: 80000\n",
      "  num_steps_trained: 80000\n",
      "iterations_since_restore: 20\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 73.7536231884058\n",
      "  ram_util_percent: 82.4199275362319\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf: {}\n",
      "time_since_restore: 2647.3379378318787\n",
      "time_this_iter_s: 194.03000330924988\n",
      "time_total_s: 2647.3379378318787\n",
      "timers:\n",
      "  learn_throughput: 50.887\n",
      "  learn_time_ms: 78605.327\n",
      "  load_throughput: 63517.628\n",
      "  load_time_ms: 62.975\n",
      "  sample_throughput: 49.574\n",
      "  sample_time_ms: 80687.938\n",
      "  update_time_ms: 11.142\n",
      "timestamp: 1625576499\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 80000\n",
      "training_iteration: 20\n",
      "\n",
      "agent_timesteps_total: 84000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_10-05-07\n",
      "done: false\n",
      "episode_len_mean: .nan\n",
      "episode_media: {}\n",
      "episode_reward_max: .nan\n",
      "episode_reward_mean: .nan\n",
      "episode_reward_min: .nan\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 0\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.401479959487915\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014632383361458778\n",
      "        model: {}\n",
      "        policy_loss: -0.12200111895799637\n",
      "        total_loss: -0.08602812886238098\n",
      "        vf_explained_var: -0.06998306512832642\n",
      "        vf_loss: 0.0026385902892798185\n",
      "  num_agent_steps_sampled: 84000\n",
      "  num_agent_steps_trained: 84000\n",
      "  num_steps_sampled: 84000\n",
      "  num_steps_trained: 84000\n",
      "iterations_since_restore: 21\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 81.70777027027027\n",
      "  ram_util_percent: 64.79527027027028\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf: {}\n",
      "time_since_restore: 2855.039212703705\n",
      "time_this_iter_s: 207.70127487182617\n",
      "time_total_s: 2855.039212703705\n",
      "timers:\n",
      "  learn_throughput: 49.994\n",
      "  learn_time_ms: 80009.153\n",
      "  load_throughput: 63137.514\n",
      "  load_time_ms: 63.354\n",
      "  sample_throughput: 45.636\n",
      "  sample_time_ms: 87649.545\n",
      "  update_time_ms: 5.864\n",
      "timestamp: 1625576707\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 84000\n",
      "training_iteration: 21\n",
      "\n",
      "agent_timesteps_total: 88000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_10-08-15\n",
      "done: false\n",
      "episode_len_mean: .nan\n",
      "episode_media: {}\n",
      "episode_reward_max: .nan\n",
      "episode_reward_mean: .nan\n",
      "episode_reward_min: .nan\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 0\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.4429330825805664\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01329054869711399\n",
      "        model: {}\n",
      "        policy_loss: -0.07185889035463333\n",
      "        total_loss: -0.03981778398156166\n",
      "        vf_explained_var: -0.4989619553089142\n",
      "        vf_loss: 0.0017635850235819817\n",
      "  num_agent_steps_sampled: 88000\n",
      "  num_agent_steps_trained: 88000\n",
      "  num_steps_sampled: 88000\n",
      "  num_steps_trained: 88000\n",
      "iterations_since_restore: 22\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 65.62471910112359\n",
      "  ram_util_percent: 58.292134831460665\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf: {}\n",
      "time_since_restore: 3042.858997821808\n",
      "time_this_iter_s: 187.81978511810303\n",
      "time_total_s: 3042.858997821808\n",
      "timers:\n",
      "  learn_throughput: 49.867\n",
      "  learn_time_ms: 80213.325\n",
      "  load_throughput: 63172.723\n",
      "  load_time_ms: 63.318\n",
      "  sample_throughput: 43.129\n",
      "  sample_time_ms: 92744.145\n",
      "  update_time_ms: 5.984\n",
      "timestamp: 1625576895\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 88000\n",
      "training_iteration: 22\n",
      "\n",
      "agent_timesteps_total: 92000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_10-11-31\n",
      "done: false\n",
      "episode_len_mean: .nan\n",
      "episode_media: {}\n",
      "episode_reward_max: .nan\n",
      "episode_reward_mean: .nan\n",
      "episode_reward_min: .nan\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 0\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.394953727722168\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012793038971722126\n",
      "        model: {}\n",
      "        policy_loss: -0.10830827057361603\n",
      "        total_loss: -0.07708624750375748\n",
      "        vf_explained_var: 0.15782013535499573\n",
      "        vf_loss: 0.002077878685668111\n",
      "  num_agent_steps_sampled: 92000\n",
      "  num_agent_steps_trained: 92000\n",
      "  num_steps_sampled: 92000\n",
      "  num_steps_trained: 92000\n",
      "iterations_since_restore: 23\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 72.3575\n",
      "  ram_util_percent: 59.153928571428565\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf: {}\n",
      "time_since_restore: 3239.0209197998047\n",
      "time_this_iter_s: 196.16192197799683\n",
      "time_total_s: 3239.0209197998047\n",
      "timers:\n",
      "  learn_throughput: 49.686\n",
      "  learn_time_ms: 80505.193\n",
      "  load_throughput: 63126.634\n",
      "  load_time_ms: 63.365\n",
      "  sample_throughput: 40.638\n",
      "  sample_time_ms: 98430.776\n",
      "  update_time_ms: 6.02\n",
      "timestamp: 1625577091\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 92000\n",
      "training_iteration: 23\n",
      "\n",
      "agent_timesteps_total: 96000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_10-14-53\n",
      "done: false\n",
      "episode_len_mean: .nan\n",
      "episode_media: {}\n",
      "episode_reward_max: .nan\n",
      "episode_reward_mean: .nan\n",
      "episode_reward_min: .nan\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 0\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.4095208644866943\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01199264731258154\n",
      "        model: {}\n",
      "        policy_loss: -0.11785556375980377\n",
      "        total_loss: -0.08845771849155426\n",
      "        vf_explained_var: -0.5983709692955017\n",
      "        vf_loss: 0.0020770966075360775\n",
      "  num_agent_steps_sampled: 96000\n",
      "  num_agent_steps_trained: 96000\n",
      "  num_steps_sampled: 96000\n",
      "  num_steps_trained: 96000\n",
      "iterations_since_restore: 24\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 68.53159722222222\n",
      "  ram_util_percent: 61.05416666666666\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf: {}\n",
      "time_since_restore: 3441.2997176647186\n",
      "time_this_iter_s: 202.27879786491394\n",
      "time_total_s: 3441.2997176647186\n",
      "timers:\n",
      "  learn_throughput: 49.243\n",
      "  learn_time_ms: 81229.377\n",
      "  load_throughput: 63372.498\n",
      "  load_time_ms: 63.119\n",
      "  sample_throughput: 38.781\n",
      "  sample_time_ms: 103143.5\n",
      "  update_time_ms: 6.172\n",
      "timestamp: 1625577293\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 96000\n",
      "training_iteration: 24\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 100000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_10-18-30\n",
      "done: false\n",
      "episode_len_mean: .nan\n",
      "episode_media: {}\n",
      "episode_reward_max: .nan\n",
      "episode_reward_mean: .nan\n",
      "episode_reward_min: .nan\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 0\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.38265061378479\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015221665613353252\n",
      "        model: {}\n",
      "        policy_loss: -0.14253821969032288\n",
      "        total_loss: -0.10519733279943466\n",
      "        vf_explained_var: 0.2808285057544708\n",
      "        vf_loss: 0.0026640230789780617\n",
      "  num_agent_steps_sampled: 100000\n",
      "  num_agent_steps_trained: 100000\n",
      "  num_steps_sampled: 100000\n",
      "  num_steps_trained: 100000\n",
      "iterations_since_restore: 25\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 70.91954397394136\n",
      "  ram_util_percent: 62.719218241042356\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf: {}\n",
      "time_since_restore: 3657.3026616573334\n",
      "time_this_iter_s: 216.00294399261475\n",
      "time_total_s: 3657.3026616573334\n",
      "timers:\n",
      "  learn_throughput: 49.03\n",
      "  learn_time_ms: 81582.426\n",
      "  load_throughput: 63234.535\n",
      "  load_time_ms: 63.257\n",
      "  sample_throughput: 36.68\n",
      "  sample_time_ms: 109051.23\n",
      "  update_time_ms: 6.19\n",
      "timestamp: 1625577510\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 100000\n",
      "training_iteration: 25\n",
      "\n",
      "agent_timesteps_total: 104000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_10-21-17\n",
      "done: false\n",
      "episode_len_mean: 51196.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.799390616673324\n",
      "episode_reward_mean: 1.7149572447046255\n",
      "episode_reward_min: 0.6305238727359271\n",
      "episodes_this_iter: 2\n",
      "episodes_total: 2\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.4143826961517334\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015211273916065693\n",
      "        model: {}\n",
      "        policy_loss: -0.15057234466075897\n",
      "        total_loss: -0.11368008702993393\n",
      "        vf_explained_var: -0.23215819895267487\n",
      "        vf_loss: 0.0022390715312212706\n",
      "  num_agent_steps_sampled: 104000\n",
      "  num_agent_steps_trained: 104000\n",
      "  num_steps_sampled: 104000\n",
      "  num_steps_trained: 104000\n",
      "iterations_since_restore: 26\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 72.81464435146445\n",
      "  ram_util_percent: 63.258158995815904\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12612316929875209\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 30.695693001985823\n",
      "  mean_inference_ms: 2.4943383335459703\n",
      "  mean_raw_obs_processing_ms: 0.16756226325461032\n",
      "time_since_restore: 3824.816302537918\n",
      "time_this_iter_s: 167.51364088058472\n",
      "time_total_s: 3824.816302537918\n",
      "timers:\n",
      "  learn_throughput: 48.634\n",
      "  learn_time_ms: 82246.665\n",
      "  load_throughput: 63781.365\n",
      "  load_time_ms: 62.714\n",
      "  sample_throughput: 36.438\n",
      "  sample_time_ms: 109774.784\n",
      "  update_time_ms: 6.197\n",
      "timestamp: 1625577677\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 104000\n",
      "training_iteration: 26\n",
      "\n",
      "agent_timesteps_total: 108000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_10-22-58\n",
      "done: false\n",
      "episode_len_mean: 51196.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.799390616673324\n",
      "episode_reward_mean: 1.7149572447046255\n",
      "episode_reward_min: 0.6305238727359271\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 2\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.447855234146118\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014964340254664421\n",
      "        model: {}\n",
      "        policy_loss: -0.13343386352062225\n",
      "        total_loss: -0.09696739912033081\n",
      "        vf_explained_var: -0.5147411823272705\n",
      "        vf_loss: 0.002375857438892126\n",
      "  num_agent_steps_sampled: 108000\n",
      "  num_agent_steps_trained: 108000\n",
      "  num_steps_sampled: 108000\n",
      "  num_steps_trained: 108000\n",
      "iterations_since_restore: 27\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 81.35454545454546\n",
      "  ram_util_percent: 64.78951048951049\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12612316929875209\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 30.695693001985823\n",
      "  mean_inference_ms: 2.4943383335459703\n",
      "  mean_raw_obs_processing_ms: 0.16756226325461032\n",
      "time_since_restore: 3925.652448415756\n",
      "time_this_iter_s: 100.83614587783813\n",
      "time_total_s: 3925.652448415756\n",
      "timers:\n",
      "  learn_throughput: 48.314\n",
      "  learn_time_ms: 82791.772\n",
      "  load_throughput: 63445.712\n",
      "  load_time_ms: 63.046\n",
      "  sample_throughput: 39.189\n",
      "  sample_time_ms: 102068.299\n",
      "  update_time_ms: 6.286\n",
      "timestamp: 1625577778\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 108000\n",
      "training_iteration: 27\n",
      "\n",
      "agent_timesteps_total: 112000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_10-24-38\n",
      "done: false\n",
      "episode_len_mean: 51196.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.799390616673324\n",
      "episode_reward_mean: 1.7149572447046255\n",
      "episode_reward_min: 0.6305238727359271\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 2\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.4514999389648438\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014998478814959526\n",
      "        model: {}\n",
      "        policy_loss: -0.1506771296262741\n",
      "        total_loss: -0.11382880061864853\n",
      "        vf_explained_var: -0.3097537159919739\n",
      "        vf_loss: 0.002679933561012149\n",
      "  num_agent_steps_sampled: 112000\n",
      "  num_agent_steps_trained: 112000\n",
      "  num_steps_sampled: 112000\n",
      "  num_steps_trained: 112000\n",
      "iterations_since_restore: 28\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 74.33450704225352\n",
      "  ram_util_percent: 65.64295774647886\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12612316929875209\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 30.695693001985823\n",
      "  mean_inference_ms: 2.4943383335459703\n",
      "  mean_raw_obs_processing_ms: 0.16756226325461032\n",
      "time_since_restore: 4025.4880664348602\n",
      "time_this_iter_s: 99.835618019104\n",
      "time_total_s: 4025.4880664348602\n",
      "timers:\n",
      "  learn_throughput: 48.844\n",
      "  learn_time_ms: 81893.152\n",
      "  load_throughput: 63132.976\n",
      "  load_time_ms: 63.358\n",
      "  sample_throughput: 42.767\n",
      "  sample_time_ms: 93530.586\n",
      "  update_time_ms: 6.229\n",
      "timestamp: 1625577878\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 112000\n",
      "training_iteration: 28\n",
      "\n",
      "agent_timesteps_total: 116000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_10-26-24\n",
      "done: false\n",
      "episode_len_mean: 51196.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.799390616673324\n",
      "episode_reward_mean: 1.7149572447046255\n",
      "episode_reward_min: 0.6305238727359271\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 2\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.4469480514526367\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01720343343913555\n",
      "        model: {}\n",
      "        policy_loss: -0.14931043982505798\n",
      "        total_loss: -0.10786432772874832\n",
      "        vf_explained_var: -0.5856810212135315\n",
      "        vf_loss: 0.0022545307874679565\n",
      "  num_agent_steps_sampled: 116000\n",
      "  num_agent_steps_trained: 116000\n",
      "  num_steps_sampled: 116000\n",
      "  num_steps_trained: 116000\n",
      "iterations_since_restore: 29\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 79.35921052631578\n",
      "  ram_util_percent: 65.97500000000001\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12612316929875209\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 30.695693001985823\n",
      "  mean_inference_ms: 2.4943383335459703\n",
      "  mean_raw_obs_processing_ms: 0.16756226325461032\n",
      "time_since_restore: 4131.682134628296\n",
      "time_this_iter_s: 106.19406819343567\n",
      "time_total_s: 4131.682134628296\n",
      "timers:\n",
      "  learn_throughput: 48.472\n",
      "  learn_time_ms: 82522.552\n",
      "  load_throughput: 62219.575\n",
      "  load_time_ms: 64.288\n",
      "  sample_throughput: 46.928\n",
      "  sample_time_ms: 85237.419\n",
      "  update_time_ms: 6.086\n",
      "timestamp: 1625577984\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 116000\n",
      "training_iteration: 29\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 120000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_10-28-25\n",
      "done: false\n",
      "episode_len_mean: 36425.333333333336\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.799390616673324\n",
      "episode_reward_mean: 0.9341094029073039\n",
      "episode_reward_min: -0.6275862806873393\n",
      "episodes_this_iter: 1\n",
      "episodes_total: 3\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.4475085735321045\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01663333550095558\n",
      "        model: {}\n",
      "        policy_loss: -0.1617010235786438\n",
      "        total_loss: -0.11916829645633698\n",
      "        vf_explained_var: -0.33341896533966064\n",
      "        vf_loss: 0.004639922175556421\n",
      "  num_agent_steps_sampled: 120000\n",
      "  num_agent_steps_trained: 120000\n",
      "  num_steps_sampled: 120000\n",
      "  num_steps_trained: 120000\n",
      "iterations_since_restore: 30\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 80.3813953488372\n",
      "  ram_util_percent: 66.84651162790696\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12591144943638757\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 29.248921769787597\n",
      "  mean_inference_ms: 2.4886373317127473\n",
      "  mean_raw_obs_processing_ms: 0.16795648171458855\n",
      "time_since_restore: 4252.956335306168\n",
      "time_this_iter_s: 121.2742006778717\n",
      "time_total_s: 4252.956335306168\n",
      "timers:\n",
      "  learn_throughput: 47.927\n",
      "  learn_time_ms: 83460.505\n",
      "  load_throughput: 62255.177\n",
      "  load_time_ms: 64.252\n",
      "  sample_throughput: 51.932\n",
      "  sample_time_ms: 77024.071\n",
      "  update_time_ms: 5.407\n",
      "timestamp: 1625578105\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 120000\n",
      "training_iteration: 30\n",
      "\n",
      "agent_timesteps_total: 124000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_10-30-43\n",
      "done: false\n",
      "episode_len_mean: 36425.333333333336\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.799390616673324\n",
      "episode_reward_mean: 0.9341094029073039\n",
      "episode_reward_min: -0.6275862806873393\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 3\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.4238760471343994\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01524853054434061\n",
      "        model: {}\n",
      "        policy_loss: -0.11902835220098495\n",
      "        total_loss: -0.08063669502735138\n",
      "        vf_explained_var: -0.6063840985298157\n",
      "        vf_loss: 0.0036535849794745445\n",
      "  num_agent_steps_sampled: 124000\n",
      "  num_agent_steps_trained: 124000\n",
      "  num_steps_sampled: 124000\n",
      "  num_steps_trained: 124000\n",
      "iterations_since_restore: 31\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 87.01275510204081\n",
      "  ram_util_percent: 70.56581632653061\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12591144943638757\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 29.248921769787597\n",
      "  mean_inference_ms: 2.4886373317127473\n",
      "  mean_raw_obs_processing_ms: 0.16795648171458855\n",
      "time_since_restore: 4390.524409294128\n",
      "time_this_iter_s: 137.56807398796082\n",
      "time_total_s: 4390.524409294128\n",
      "timers:\n",
      "  learn_throughput: 47.301\n",
      "  learn_time_ms: 84565.049\n",
      "  load_throughput: 61349.631\n",
      "  load_time_ms: 65.2\n",
      "  sample_throughput: 58.051\n",
      "  sample_time_ms: 68904.941\n",
      "  update_time_ms: 5.436\n",
      "timestamp: 1625578243\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 124000\n",
      "training_iteration: 31\n",
      "\n",
      "agent_timesteps_total: 128000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_10-33-00\n",
      "done: false\n",
      "episode_len_mean: 36425.333333333336\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.799390616673324\n",
      "episode_reward_mean: 0.9341094029073039\n",
      "episode_reward_min: -0.6275862806873393\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 3\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.4089367389678955\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017438871785998344\n",
      "        model: {}\n",
      "        policy_loss: -0.1638937145471573\n",
      "        total_loss: -0.11986878514289856\n",
      "        vf_explained_var: -0.6065086722373962\n",
      "        vf_loss: 0.004297010134905577\n",
      "  num_agent_steps_sampled: 128000\n",
      "  num_agent_steps_trained: 128000\n",
      "  num_steps_sampled: 128000\n",
      "  num_steps_trained: 128000\n",
      "iterations_since_restore: 32\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 83.29132653061224\n",
      "  ram_util_percent: 63.78316326530612\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12591144943638757\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 29.248921769787597\n",
      "  mean_inference_ms: 2.4886373317127473\n",
      "  mean_raw_obs_processing_ms: 0.16795648171458855\n",
      "time_since_restore: 4527.72918844223\n",
      "time_this_iter_s: 137.2047791481018\n",
      "time_total_s: 4527.72918844223\n",
      "timers:\n",
      "  learn_throughput: 46.272\n",
      "  learn_time_ms: 86445.009\n",
      "  load_throughput: 60649.827\n",
      "  load_time_ms: 65.952\n",
      "  sample_throughput: 64.555\n",
      "  sample_time_ms: 61962.481\n",
      "  update_time_ms: 5.444\n",
      "timestamp: 1625578380\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 128000\n",
      "training_iteration: 32\n",
      "\n",
      "agent_timesteps_total: 132000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_10-35-05\n",
      "done: false\n",
      "episode_len_mean: 36425.333333333336\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.799390616673324\n",
      "episode_reward_mean: 0.9341094029073039\n",
      "episode_reward_min: -0.6275862806873393\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 3\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.413384437561035\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018104560673236847\n",
      "        model: {}\n",
      "        policy_loss: -0.16038282215595245\n",
      "        total_loss: -0.11554954946041107\n",
      "        vf_explained_var: -0.7177898287773132\n",
      "        vf_loss: 0.0035888166166841984\n",
      "  num_agent_steps_sampled: 132000\n",
      "  num_agent_steps_trained: 132000\n",
      "  num_steps_sampled: 132000\n",
      "  num_steps_trained: 132000\n",
      "iterations_since_restore: 33\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 78.4497175141243\n",
      "  ram_util_percent: 61.32090395480225\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12591144943638757\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 29.248921769787597\n",
      "  mean_inference_ms: 2.4886373317127473\n",
      "  mean_raw_obs_processing_ms: 0.16795648171458855\n",
      "time_since_restore: 4652.189804315567\n",
      "time_this_iter_s: 124.46061587333679\n",
      "time_total_s: 4652.189804315567\n",
      "timers:\n",
      "  learn_throughput: 45.828\n",
      "  learn_time_ms: 87282.631\n",
      "  load_throughput: 60216.528\n",
      "  load_time_ms: 66.427\n",
      "  sample_throughput: 74.137\n",
      "  sample_time_ms: 53954.051\n",
      "  update_time_ms: 5.453\n",
      "timestamp: 1625578505\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 132000\n",
      "training_iteration: 33\n",
      "\n",
      "agent_timesteps_total: 136000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_10-37-21\n",
      "done: false\n",
      "episode_len_mean: 36425.333333333336\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.799390616673324\n",
      "episode_reward_mean: 0.9341094029073039\n",
      "episode_reward_min: -0.6275862806873393\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 3\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.3610546588897705\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016409270465373993\n",
      "        model: {}\n",
      "        policy_loss: -0.14820098876953125\n",
      "        total_loss: -0.10728991031646729\n",
      "        vf_explained_var: -0.6754025816917419\n",
      "        vf_loss: 0.0035287239588797092\n",
      "  num_agent_steps_sampled: 136000\n",
      "  num_agent_steps_trained: 136000\n",
      "  num_steps_sampled: 136000\n",
      "  num_steps_trained: 136000\n",
      "iterations_since_restore: 34\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 75.0298969072165\n",
      "  ram_util_percent: 61.33247422680412\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12591144943638757\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 29.248921769787597\n",
      "  mean_inference_ms: 2.4886373317127473\n",
      "  mean_raw_obs_processing_ms: 0.16795648171458855\n",
      "time_since_restore: 4788.717552185059\n",
      "time_this_iter_s: 136.52774786949158\n",
      "time_total_s: 4788.717552185059\n",
      "timers:\n",
      "  learn_throughput: 45.617\n",
      "  learn_time_ms: 87687.253\n",
      "  load_throughput: 59705.969\n",
      "  load_time_ms: 66.995\n",
      "  sample_throughput: 85.154\n",
      "  sample_time_ms: 46973.61\n",
      "  update_time_ms: 5.43\n",
      "timestamp: 1625578641\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 136000\n",
      "training_iteration: 34\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 140000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_10-39-38\n",
      "done: false\n",
      "episode_len_mean: 36425.333333333336\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.799390616673324\n",
      "episode_reward_mean: 0.9341094029073039\n",
      "episode_reward_min: -0.6275862806873393\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 3\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.341752767562866\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01614585518836975\n",
      "        model: {}\n",
      "        policy_loss: -0.15755116939544678\n",
      "        total_loss: -0.11772286891937256\n",
      "        vf_explained_var: -0.6834304928779602\n",
      "        vf_loss: 0.0030460203997790813\n",
      "  num_agent_steps_sampled: 140000\n",
      "  num_agent_steps_trained: 140000\n",
      "  num_steps_sampled: 140000\n",
      "  num_steps_trained: 140000\n",
      "iterations_since_restore: 35\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 73.00153846153846\n",
      "  ram_util_percent: 61.85897435897436\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12591144943638757\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 29.248921769787597\n",
      "  mean_inference_ms: 2.4886373317127473\n",
      "  mean_raw_obs_processing_ms: 0.16795648171458855\n",
      "time_since_restore: 4925.566263437271\n",
      "time_this_iter_s: 136.84871125221252\n",
      "time_total_s: 4925.566263437271\n",
      "timers:\n",
      "  learn_throughput: 45.308\n",
      "  learn_time_ms: 88283.983\n",
      "  load_throughput: 59045.327\n",
      "  load_time_ms: 67.745\n",
      "  sample_throughput: 104.004\n",
      "  sample_time_ms: 38460.178\n",
      "  update_time_ms: 5.707\n",
      "timestamp: 1625578778\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 140000\n",
      "training_iteration: 35\n",
      "\n",
      "agent_timesteps_total: 144000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_10-42-01\n",
      "done: false\n",
      "episode_len_mean: 36425.333333333336\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.799390616673324\n",
      "episode_reward_mean: 0.9341094029073039\n",
      "episode_reward_min: -0.6275862806873393\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 3\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.337245225906372\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01731634885072708\n",
      "        model: {}\n",
      "        policy_loss: -0.156107559800148\n",
      "        total_loss: -0.11333462595939636\n",
      "        vf_explained_var: -0.654761552810669\n",
      "        vf_loss: 0.0033241224009543657\n",
      "  num_agent_steps_sampled: 144000\n",
      "  num_agent_steps_trained: 144000\n",
      "  num_steps_sampled: 144000\n",
      "  num_steps_trained: 144000\n",
      "iterations_since_restore: 36\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 75.2921568627451\n",
      "  ram_util_percent: 61.64754901960784\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12591144943638757\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 29.248921769787597\n",
      "  mean_inference_ms: 2.4886373317127473\n",
      "  mean_raw_obs_processing_ms: 0.16795648171458855\n",
      "time_since_restore: 5068.569096565247\n",
      "time_this_iter_s: 143.00283312797546\n",
      "time_total_s: 5068.569096565247\n",
      "timers:\n",
      "  learn_throughput: 45.043\n",
      "  learn_time_ms: 88804.437\n",
      "  load_throughput: 59106.464\n",
      "  load_time_ms: 67.674\n",
      "  sample_throughput: 112.712\n",
      "  sample_time_ms: 35488.828\n",
      "  update_time_ms: 5.76\n",
      "timestamp: 1625578921\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 144000\n",
      "training_iteration: 36\n",
      "\n",
      "agent_timesteps_total: 148000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_10-44-42\n",
      "done: false\n",
      "episode_len_mean: 36425.333333333336\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.799390616673324\n",
      "episode_reward_mean: 0.9341094029073039\n",
      "episode_reward_min: -0.6275862806873393\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 3\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.3100290298461914\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019326340407133102\n",
      "        model: {}\n",
      "        policy_loss: -0.15335722267627716\n",
      "        total_loss: -0.10604915767908096\n",
      "        vf_explained_var: -0.7562084794044495\n",
      "        vf_loss: 0.0032802310306578875\n",
      "  num_agent_steps_sampled: 148000\n",
      "  num_agent_steps_trained: 148000\n",
      "  num_steps_sampled: 148000\n",
      "  num_steps_trained: 148000\n",
      "iterations_since_restore: 37\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 78.58947368421053\n",
      "  ram_util_percent: 61.96140350877192\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12591144943638757\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 29.248921769787597\n",
      "  mean_inference_ms: 2.4886373317127473\n",
      "  mean_raw_obs_processing_ms: 0.16795648171458855\n",
      "time_since_restore: 5229.224505662918\n",
      "time_this_iter_s: 160.6554090976715\n",
      "time_total_s: 5229.224505662918\n",
      "timers:\n",
      "  learn_throughput: 44.922\n",
      "  learn_time_ms: 89043.099\n",
      "  load_throughput: 57787.408\n",
      "  load_time_ms: 69.219\n",
      "  sample_throughput: 97.017\n",
      "  sample_time_ms: 41230.094\n",
      "  update_time_ms: 6.101\n",
      "timestamp: 1625579082\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 148000\n",
      "training_iteration: 37\n",
      "\n",
      "agent_timesteps_total: 152000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_10-47-18\n",
      "done: false\n",
      "episode_len_mean: 36425.333333333336\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.799390616673324\n",
      "episode_reward_mean: 0.9341094029073039\n",
      "episode_reward_min: -0.6275862806873393\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 3\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.3141140937805176\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016264179721474648\n",
      "        model: {}\n",
      "        policy_loss: -0.14159490168094635\n",
      "        total_loss: -0.10136369615793228\n",
      "        vf_explained_var: -0.5687224268913269\n",
      "        vf_loss: 0.003179362276569009\n",
      "  num_agent_steps_sampled: 152000\n",
      "  num_agent_steps_trained: 152000\n",
      "  num_steps_sampled: 152000\n",
      "  num_steps_trained: 152000\n",
      "iterations_since_restore: 38\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 74.12702702702703\n",
      "  ram_util_percent: 62.5036036036036\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12591144943638757\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 29.248921769787597\n",
      "  mean_inference_ms: 2.4886373317127473\n",
      "  mean_raw_obs_processing_ms: 0.16795648171458855\n",
      "time_since_restore: 5384.8437695503235\n",
      "time_this_iter_s: 155.6192638874054\n",
      "time_total_s: 5384.8437695503235\n",
      "timers:\n",
      "  learn_throughput: 44.72\n",
      "  learn_time_ms: 89445.564\n",
      "  load_throughput: 57633.541\n",
      "  load_time_ms: 69.404\n",
      "  sample_throughput: 86.197\n",
      "  sample_time_ms: 46405.554\n",
      "  update_time_ms: 6.148\n",
      "timestamp: 1625579238\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 152000\n",
      "training_iteration: 38\n",
      "\n",
      "agent_timesteps_total: 156000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_10-49-46\n",
      "done: false\n",
      "episode_len_mean: 36425.333333333336\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.799390616673324\n",
      "episode_reward_mean: 0.9341094029073039\n",
      "episode_reward_min: -0.6275862806873393\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 3\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.3051586151123047\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016283610835671425\n",
      "        model: {}\n",
      "        policy_loss: -0.1388373076915741\n",
      "        total_loss: -0.09899015724658966\n",
      "        vf_explained_var: -0.7911605834960938\n",
      "        vf_loss: 0.0027510658837854862\n",
      "  num_agent_steps_sampled: 156000\n",
      "  num_agent_steps_trained: 156000\n",
      "  num_steps_sampled: 156000\n",
      "  num_steps_trained: 156000\n",
      "iterations_since_restore: 39\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 70.89666666666666\n",
      "  ram_util_percent: 61.71\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12591144943638757\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 29.248921769787597\n",
      "  mean_inference_ms: 2.4886373317127473\n",
      "  mean_raw_obs_processing_ms: 0.16795648171458855\n",
      "time_since_restore: 5532.495454311371\n",
      "time_this_iter_s: 147.65168476104736\n",
      "time_total_s: 5532.495454311371\n",
      "timers:\n",
      "  learn_throughput: 44.899\n",
      "  learn_time_ms: 89089.513\n",
      "  load_throughput: 57100.573\n",
      "  load_time_ms: 70.052\n",
      "  sample_throughput: 78.575\n",
      "  sample_time_ms: 50906.621\n",
      "  update_time_ms: 6.28\n",
      "timestamp: 1625579386\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 156000\n",
      "training_iteration: 39\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 160000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_10-52-14\n",
      "done: false\n",
      "episode_len_mean: 36425.333333333336\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.799390616673324\n",
      "episode_reward_mean: 0.9341094029073039\n",
      "episode_reward_min: -0.6275862806873393\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 3\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.312380790710449\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016216952353715897\n",
      "        model: {}\n",
      "        policy_loss: -0.14881592988967896\n",
      "        total_loss: -0.10934530198574066\n",
      "        vf_explained_var: -0.6474730372428894\n",
      "        vf_loss: 0.002526385709643364\n",
      "  num_agent_steps_sampled: 160000\n",
      "  num_agent_steps_trained: 160000\n",
      "  num_steps_sampled: 160000\n",
      "  num_steps_trained: 160000\n",
      "iterations_since_restore: 40\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 71.27203791469194\n",
      "  ram_util_percent: 62.37962085308056\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12591144943638757\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 29.248921769787597\n",
      "  mean_inference_ms: 2.4886373317127473\n",
      "  mean_raw_obs_processing_ms: 0.16795648171458855\n",
      "time_since_restore: 5680.577744483948\n",
      "time_this_iter_s: 148.0822901725769\n",
      "time_total_s: 5680.577744483948\n",
      "timers:\n",
      "  learn_throughput: 45.788\n",
      "  learn_time_ms: 87358.788\n",
      "  load_throughput: 57440.62\n",
      "  load_time_ms: 69.637\n",
      "  sample_throughput: 72.308\n",
      "  sample_time_ms: 55318.906\n",
      "  update_time_ms: 6.301\n",
      "timestamp: 1625579534\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 160000\n",
      "training_iteration: 40\n",
      "\n",
      "agent_timesteps_total: 164000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_10-54-47\n",
      "done: false\n",
      "episode_len_mean: 36425.333333333336\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.799390616673324\n",
      "episode_reward_mean: 0.9341094029073039\n",
      "episode_reward_min: -0.6275862806873393\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 3\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.2986512184143066\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016613252460956573\n",
      "        model: {}\n",
      "        policy_loss: -0.1565398871898651\n",
      "        total_loss: -0.11552233248949051\n",
      "        vf_explained_var: -0.41517749428749084\n",
      "        vf_loss: 0.0031704972498118877\n",
      "  num_agent_steps_sampled: 164000\n",
      "  num_agent_steps_trained: 164000\n",
      "  num_steps_sampled: 164000\n",
      "  num_steps_trained: 164000\n",
      "iterations_since_restore: 41\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 71.64449541284404\n",
      "  ram_util_percent: 61.85229357798165\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12591144943638757\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 29.248921769787597\n",
      "  mean_inference_ms: 2.4886373317127473\n",
      "  mean_raw_obs_processing_ms: 0.16795648171458855\n",
      "time_since_restore: 5833.524008512497\n",
      "time_this_iter_s: 152.9462640285492\n",
      "time_total_s: 5833.524008512497\n",
      "timers:\n",
      "  learn_throughput: 46.708\n",
      "  learn_time_ms: 85637.994\n",
      "  load_throughput: 58228.121\n",
      "  load_time_ms: 68.695\n",
      "  sample_throughput: 68.284\n",
      "  sample_time_ms: 58578.513\n",
      "  update_time_ms: 6.387\n",
      "timestamp: 1625579687\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 164000\n",
      "training_iteration: 41\n",
      "\n",
      "agent_timesteps_total: 168000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_10-57-30\n",
      "done: false\n",
      "episode_len_mean: 36425.333333333336\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.799390616673324\n",
      "episode_reward_mean: 0.9341094029073039\n",
      "episode_reward_min: -0.6275862806873393\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 3\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.3565800189971924\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015500251203775406\n",
      "        model: {}\n",
      "        policy_loss: -0.1394869089126587\n",
      "        total_loss: -0.10135708004236221\n",
      "        vf_explained_var: -0.29574817419052124\n",
      "        vf_loss: 0.002818319946527481\n",
      "  num_agent_steps_sampled: 168000\n",
      "  num_agent_steps_trained: 168000\n",
      "  num_steps_sampled: 168000\n",
      "  num_steps_trained: 168000\n",
      "iterations_since_restore: 42\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 72.68706896551723\n",
      "  ram_util_percent: 61.88706896551724\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12591144943638757\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 29.248921769787597\n",
      "  mean_inference_ms: 2.4886373317127473\n",
      "  mean_raw_obs_processing_ms: 0.16795648171458855\n",
      "time_since_restore: 5996.756862640381\n",
      "time_this_iter_s: 163.2328541278839\n",
      "time_total_s: 5996.756862640381\n",
      "timers:\n",
      "  learn_throughput: 47.49\n",
      "  learn_time_ms: 84228.835\n",
      "  load_throughput: 58365.157\n",
      "  load_time_ms: 68.534\n",
      "  sample_throughput: 63.907\n",
      "  sample_time_ms: 62590.801\n",
      "  update_time_ms: 6.337\n",
      "timestamp: 1625579850\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 168000\n",
      "training_iteration: 42\n",
      "\n",
      "agent_timesteps_total: 172000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_11-00-25\n",
      "done: false\n",
      "episode_len_mean: 36425.333333333336\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.799390616673324\n",
      "episode_reward_mean: 0.9341094029073039\n",
      "episode_reward_min: -0.6275862806873393\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 3\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.293574094772339\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01787482015788555\n",
      "        model: {}\n",
      "        policy_loss: -0.1885036677122116\n",
      "        total_loss: -0.14503295719623566\n",
      "        vf_explained_var: -0.6962267756462097\n",
      "        vf_loss: 0.0027496314141899347\n",
      "  num_agent_steps_sampled: 172000\n",
      "  num_agent_steps_trained: 172000\n",
      "  num_steps_sampled: 172000\n",
      "  num_steps_trained: 172000\n",
      "iterations_since_restore: 43\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 71.8572\n",
      "  ram_util_percent: 62.294399999999996\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12591144943638757\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 29.248921769787597\n",
      "  mean_inference_ms: 2.4886373317127473\n",
      "  mean_raw_obs_processing_ms: 0.16795648171458855\n",
      "time_since_restore: 6172.046469449997\n",
      "time_this_iter_s: 175.2896068096161\n",
      "time_total_s: 6172.046469449997\n",
      "timers:\n",
      "  learn_throughput: 47.469\n",
      "  learn_time_ms: 84266.226\n",
      "  load_throughput: 58319.548\n",
      "  load_time_ms: 68.588\n",
      "  sample_throughput: 59.14\n",
      "  sample_time_ms: 67636.28\n",
      "  update_time_ms: 6.346\n",
      "timestamp: 1625580025\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 172000\n",
      "training_iteration: 43\n",
      "\n",
      "agent_timesteps_total: 176000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_11-03-12\n",
      "done: false\n",
      "episode_len_mean: 36425.333333333336\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.799390616673324\n",
      "episode_reward_mean: 0.9341094029073039\n",
      "episode_reward_min: -0.6275862806873393\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 3\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.293750762939453\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017131026834249496\n",
      "        model: {}\n",
      "        policy_loss: -0.16025099158287048\n",
      "        total_loss: -0.11912492662668228\n",
      "        vf_explained_var: -0.5288639068603516\n",
      "        vf_loss: 0.002099436242133379\n",
      "  num_agent_steps_sampled: 176000\n",
      "  num_agent_steps_trained: 176000\n",
      "  num_steps_sampled: 176000\n",
      "  num_steps_trained: 176000\n",
      "iterations_since_restore: 44\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 67.23781512605042\n",
      "  ram_util_percent: 62.56050420168065\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12591144943638757\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 29.248921769787597\n",
      "  mean_inference_ms: 2.4886373317127473\n",
      "  mean_raw_obs_processing_ms: 0.16795648171458855\n",
      "time_since_restore: 6338.882996559143\n",
      "time_this_iter_s: 166.83652710914612\n",
      "time_total_s: 6338.882996559143\n",
      "timers:\n",
      "  learn_throughput: 48.023\n",
      "  learn_time_ms: 83293.008\n",
      "  load_throughput: 58713.163\n",
      "  load_time_ms: 68.128\n",
      "  sample_throughput: 55.834\n",
      "  sample_time_ms: 71640.939\n",
      "  update_time_ms: 6.248\n",
      "timestamp: 1625580192\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 176000\n",
      "training_iteration: 44\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 180000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_11-06-06\n",
      "done: false\n",
      "episode_len_mean: 36425.333333333336\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.799390616673324\n",
      "episode_reward_mean: 0.9341094029073039\n",
      "episode_reward_min: -0.6275862806873393\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 3\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.324925184249878\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014740847051143646\n",
      "        model: {}\n",
      "        policy_loss: -0.15540161728858948\n",
      "        total_loss: -0.11969882249832153\n",
      "        vf_explained_var: -0.6431223750114441\n",
      "        vf_loss: 0.0021212706342339516\n",
      "  num_agent_steps_sampled: 180000\n",
      "  num_agent_steps_trained: 180000\n",
      "  num_steps_sampled: 180000\n",
      "  num_steps_trained: 180000\n",
      "iterations_since_restore: 45\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 66.45303643724695\n",
      "  ram_util_percent: 62.65344129554655\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12591144943638757\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 29.248921769787597\n",
      "  mean_inference_ms: 2.4886373317127473\n",
      "  mean_raw_obs_processing_ms: 0.16795648171458855\n",
      "time_since_restore: 6512.563641786575\n",
      "time_this_iter_s: 173.68064522743225\n",
      "time_total_s: 6512.563641786575\n",
      "timers:\n",
      "  learn_throughput: 48.555\n",
      "  learn_time_ms: 82381.295\n",
      "  load_throughput: 58848.593\n",
      "  load_time_ms: 67.971\n",
      "  sample_throughput: 52.468\n",
      "  sample_time_ms: 76236.364\n",
      "  update_time_ms: 5.985\n",
      "timestamp: 1625580366\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 180000\n",
      "training_iteration: 45\n",
      "\n",
      "agent_timesteps_total: 184000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_11-09-12\n",
      "done: false\n",
      "episode_len_mean: 36425.333333333336\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.799390616673324\n",
      "episode_reward_mean: 0.9341094029073039\n",
      "episode_reward_min: -0.6275862806873393\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 3\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.300640344619751\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015316932462155819\n",
      "        model: {}\n",
      "        policy_loss: -0.17327262461185455\n",
      "        total_loss: -0.13560007512569427\n",
      "        vf_explained_var: -0.31401973962783813\n",
      "        vf_loss: 0.0027786451391875744\n",
      "  num_agent_steps_sampled: 184000\n",
      "  num_agent_steps_trained: 184000\n",
      "  num_steps_sampled: 184000\n",
      "  num_steps_trained: 184000\n",
      "iterations_since_restore: 46\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 72.79924528301886\n",
      "  ram_util_percent: 61.96415094339623\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12591144943638757\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 29.248921769787597\n",
      "  mean_inference_ms: 2.4886373317127473\n",
      "  mean_raw_obs_processing_ms: 0.16795648171458855\n",
      "time_since_restore: 6698.77109837532\n",
      "time_this_iter_s: 186.20745658874512\n",
      "time_total_s: 6698.77109837532\n",
      "timers:\n",
      "  learn_throughput: 49.062\n",
      "  learn_time_ms: 81530.067\n",
      "  load_throughput: 58857.367\n",
      "  load_time_ms: 67.961\n",
      "  sample_throughput: 49.135\n",
      "  sample_time_ms: 81407.865\n",
      "  update_time_ms: 5.905\n",
      "timestamp: 1625580552\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 184000\n",
      "training_iteration: 46\n",
      "\n",
      "agent_timesteps_total: 188000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_11-12-27\n",
      "done: false\n",
      "episode_len_mean: 36425.333333333336\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.799390616673324\n",
      "episode_reward_mean: 0.9341094029073039\n",
      "episode_reward_min: -0.6275862806873393\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 3\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.300628900527954\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01300502847880125\n",
      "        model: {}\n",
      "        policy_loss: -0.13533185422420502\n",
      "        total_loss: -0.10391885042190552\n",
      "        vf_explained_var: -0.5194477438926697\n",
      "        vf_loss: 0.0017858812352642417\n",
      "  num_agent_steps_sampled: 188000\n",
      "  num_agent_steps_trained: 188000\n",
      "  num_steps_sampled: 188000\n",
      "  num_steps_trained: 188000\n",
      "iterations_since_restore: 47\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 73.87725631768953\n",
      "  ram_util_percent: 62.01516245487363\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12591144943638757\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 29.248921769787597\n",
      "  mean_inference_ms: 2.4886373317127473\n",
      "  mean_raw_obs_processing_ms: 0.16795648171458855\n",
      "time_since_restore: 6893.236273288727\n",
      "time_this_iter_s: 194.46517491340637\n",
      "time_total_s: 6893.236273288727\n",
      "timers:\n",
      "  learn_throughput: 49.369\n",
      "  learn_time_ms: 81021.742\n",
      "  load_throughput: 59395.313\n",
      "  load_time_ms: 67.345\n",
      "  sample_throughput: 46.894\n",
      "  sample_time_ms: 85298.395\n",
      "  update_time_ms: 5.381\n",
      "timestamp: 1625580747\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 188000\n",
      "training_iteration: 47\n",
      "\n",
      "agent_timesteps_total: 192000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_11-15-50\n",
      "done: false\n",
      "episode_len_mean: 36425.333333333336\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.799390616673324\n",
      "episode_reward_mean: 0.9341094029073039\n",
      "episode_reward_min: -0.6275862806873393\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 3\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.3035717010498047\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014683805406093597\n",
      "        model: {}\n",
      "        policy_loss: -0.14102105796337128\n",
      "        total_loss: -0.10579510778188705\n",
      "        vf_explained_var: -0.4154200553894043\n",
      "        vf_loss: 0.0017744239885360003\n",
      "  num_agent_steps_sampled: 192000\n",
      "  num_agent_steps_trained: 192000\n",
      "  num_steps_sampled: 192000\n",
      "  num_steps_trained: 192000\n",
      "iterations_since_restore: 48\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 80.96574394463667\n",
      "  ram_util_percent: 62.29307958477508\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12591144943638757\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 29.248921769787597\n",
      "  mean_inference_ms: 2.4886373317127473\n",
      "  mean_raw_obs_processing_ms: 0.16795648171458855\n",
      "time_since_restore: 7096.667812585831\n",
      "time_this_iter_s: 203.43153929710388\n",
      "time_total_s: 7096.667812585831\n",
      "timers:\n",
      "  learn_throughput: 49.574\n",
      "  learn_time_ms: 80687.054\n",
      "  load_throughput: 60034.538\n",
      "  load_time_ms: 66.628\n",
      "  sample_throughput: 44.24\n",
      "  sample_time_ms: 90415.394\n",
      "  update_time_ms: 5.392\n",
      "timestamp: 1625580950\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 192000\n",
      "training_iteration: 48\n",
      "\n",
      "agent_timesteps_total: 196000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_11-19-01\n",
      "done: false\n",
      "episode_len_mean: 36425.333333333336\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.799390616673324\n",
      "episode_reward_mean: 0.9341094029073039\n",
      "episode_reward_min: -0.6275862806873393\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 3\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.2622740268707275\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014487897045910358\n",
      "        model: {}\n",
      "        policy_loss: -0.1416720747947693\n",
      "        total_loss: -0.10668415576219559\n",
      "        vf_explained_var: -0.5181440114974976\n",
      "        vf_loss: 0.0019826702773571014\n",
      "  num_agent_steps_sampled: 196000\n",
      "  num_agent_steps_trained: 196000\n",
      "  num_steps_sampled: 196000\n",
      "  num_steps_trained: 196000\n",
      "iterations_since_restore: 49\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 74.7783088235294\n",
      "  ram_util_percent: 62.937867647058816\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12591144943638757\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 29.248921769787597\n",
      "  mean_inference_ms: 2.4886373317127473\n",
      "  mean_raw_obs_processing_ms: 0.16795648171458855\n",
      "time_since_restore: 7287.325367689133\n",
      "time_this_iter_s: 190.657555103302\n",
      "time_total_s: 7287.325367689133\n",
      "timers:\n",
      "  learn_throughput: 49.898\n",
      "  learn_time_ms: 80163.291\n",
      "  load_throughput: 61475.383\n",
      "  load_time_ms: 65.067\n",
      "  sample_throughput: 41.998\n",
      "  sample_time_ms: 95241.514\n",
      "  update_time_ms: 5.24\n",
      "timestamp: 1625581141\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 196000\n",
      "training_iteration: 49\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agent_timesteps_total: 200000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-06_11-22-37\n",
      "done: false\n",
      "episode_len_mean: 36425.333333333336\n",
      "episode_media: {}\n",
      "episode_reward_max: 2.799390616673324\n",
      "episode_reward_mean: 0.9341094029073039\n",
      "episode_reward_min: -0.6275862806873393\n",
      "episodes_this_iter: 0\n",
      "episodes_total: 3\n",
      "experiment_id: 2ddf65f5311241efb6e7031807c64547\n",
      "hostname: Studios-iMac-3.local\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 2.278125047683716\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.262425661087036\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013906210660934448\n",
      "        model: {}\n",
      "        policy_loss: -0.15012650191783905\n",
      "        total_loss: -0.11650804430246353\n",
      "        vf_explained_var: -0.07037811726331711\n",
      "        vf_loss: 0.0019383587641641498\n",
      "  num_agent_steps_sampled: 200000\n",
      "  num_agent_steps_trained: 200000\n",
      "  num_steps_sampled: 200000\n",
      "  num_steps_trained: 200000\n",
      "iterations_since_restore: 50\n",
      "node_ip: 192.168.0.15\n",
      "num_healthy_workers: 2\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 80.71168831168832\n",
      "  ram_util_percent: 62.824350649350656\n",
      "pid: 21386\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.12591144943638757\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 29.248921769787597\n",
      "  mean_inference_ms: 2.4886373317127473\n",
      "  mean_raw_obs_processing_ms: 0.16795648171458855\n",
      "time_since_restore: 7503.471053123474\n",
      "time_this_iter_s: 216.14568543434143\n",
      "time_total_s: 7503.471053123474\n",
      "timers:\n",
      "  learn_throughput: 49.241\n",
      "  learn_time_ms: 81232.797\n",
      "  load_throughput: 60477.615\n",
      "  load_time_ms: 66.14\n",
      "  sample_throughput: 39.613\n",
      "  sample_time_ms: 100976.78\n",
      "  update_time_ms: 5.226\n",
      "timestamp: 1625581357\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 200000\n",
      "training_iteration: 50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    result = agent.train()\n",
    "    print(pretty_print(result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/leandronogueira/ray_results/PPO_TradingEnv_2021-07-06_09-17-10hu4m7enp/checkpoint_000050/checkpoint-50\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = agent.save()\n",
    "print(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "seq_in (InputLayer)             [(None,)]            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_default_policy/Sequ [()]                 0           seq_in[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_default_policy/Sequ [()]                 0           tf_op_layer_default_policy/Sequen\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_default_policy/Sequ [(None, 1)]          0           seq_in[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_default_policy/Sequ [(None,)]            0           tf_op_layer_default_policy/Sequen\n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_default_policy/Sequ [(None, 1)]          0           tf_op_layer_default_policy/Sequen\n",
      "__________________________________________________________________________________________________\n",
      "inputs (InputLayer)             [(None, None, 256)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "h (InputLayer)                  [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "c (InputLayer)                  [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_default_policy/Sequ [(None, None)]       0           tf_op_layer_default_policy/Sequen\n",
      "                                                                 tf_op_layer_default_policy/Sequen\n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, None, 256),  525312      inputs[0][0]                     \n",
      "                                                                 h[0][0]                          \n",
      "                                                                 c[0][0]                          \n",
      "                                                                 tf_op_layer_default_policy/Sequen\n",
      "__________________________________________________________________________________________________\n",
      "logits (Dense)                  (None, None, 41)     10537       lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "values (Dense)                  (None, None, 1)      257         lstm[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 536,106\n",
      "Trainable params: 536,106\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-06 11:23:00,497\tINFO trainable.py:104 -- Trainable.setup took 22.392 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2021-07-06 11:23:00,500\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "2021-07-06 11:23:00,811\tINFO trainable.py:378 -- Restored on 192.168.0.15 from checkpoint: /Users/leandronogueira/ray_results/PPO_TradingEnv_2021-07-06_09-17-10hu4m7enp/checkpoint_000050/checkpoint-50\n",
      "2021-07-06 11:23:00,812\tINFO trainable.py:385 -- Current state after restoring: {'_iteration': 50, '_timesteps_total': None, '_time_total': 7503.471053123474, '_episodes_total': 3}\n"
     ]
    }
   ],
   "source": [
    "trained_config = config.copy()\n",
    "\n",
    "test_agent = PPOTrainer(trained_config, 'TradingEnv')\n",
    "test_agent.restore(checkpoint_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-06 11:23:03,155\tWARNING worker.py:1114 -- The actor or task with ID ffffffffffffffff63a00fc9c820f96b9b9cb76901000000 cannot be scheduled right now. It requires {CPU: 1.000000} for placement, but this node only has remaining {0.000000/3.000000 CPU, 6.059580 GiB/6.059580 GiB memory, 3.029790 GiB/3.029790 GiB object_store_memory, 1.000000/1.000000 node:192.168.0.15}\n",
      ". In total there are 0 pending tasks and 1 pending actors on this node. This is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increase the resources available to this Ray cluster. You can ignore this message if this Ray cluster is expected to auto-scale or if you specified a runtime_env for this task or actor because it takes time to install.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.307146754513714\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make('TradingEnv')\n",
    "config_env = {}\n",
    "config_env[\"data\"] = norm_data\n",
    "config_env[\"window_size\"] = 10\n",
    "env = createEnv(config_env)\n",
    "obs = env.reset()\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "# https://github.com/ray-project/ray/issues/9220\n",
    "# ver cell_size\n",
    "policy = test_agent.get_policy()\n",
    "cellsize=256\n",
    "state=[np.zeros(cellsize, np.float32),\n",
    "       np.zeros(cellsize, np.float32)]\n",
    "# state = policy.get_initial_state()\n",
    "# , prev_action=0, prev_reward=0\n",
    "actions=np.zeros(2*16, np.float32).reshape(2,16)\n",
    "rewards=np.zeros(16, np.float32)\n",
    "episode_reward =0\n",
    "while not done:\n",
    "#     print(\"before\")\n",
    "# #     print(state)\n",
    "#     action, state, logits = policy.compute_single_action(obs, state)\n",
    "#     state, reward, done, _ = env.step(action)\n",
    "#     cumulative_reward += reward\n",
    "#     print(state)\n",
    "    action, state, logits = test_agent.compute_action(obs, state)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    actions[:,:-1] = actions[:,1:]\n",
    "    actions[:, -1] = action\n",
    "    rewards[:-1] = rewards[1:]\n",
    "    rewards[-1] = reward\n",
    "    episode_reward += reward\n",
    "\n",
    "\n",
    "print(episode_reward)\n",
    "#     https://discuss.ray.io/t/rnn-l2-weights-regularization/2582/38\n",
    "# state=policy.get_initial_state()\n",
    "# action, state = policy.compute_single_action(current_obs,state=state)\n",
    "#   def test(self):\n",
    "#         \"\"\"Test trained agent for a single episode. Return the episode reward\"\"\"\n",
    "#         # instantiate env class\n",
    "#         env = self.env\n",
    "\n",
    "#         # run until episode ends\n",
    "#         episode_reward = 0\n",
    "#         done = False\n",
    "#         obs = env.reset()\n",
    "#         cell_size=16\n",
    "#         state=[np.zeros(cell_size, np.float32),\n",
    "#                np.zeros(cell_size, np.float32)]\n",
    "#         while not done:\n",
    "#             action, state, logits = self.agent.compute_action(obs, state)\n",
    "#             obs, reward, done, info = env.step(action)\n",
    "#             episode_reward += reward\n",
    "\n",
    "#         return episode_reward\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = policy.get_initial_state()\n",
    "# state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAGdCAYAAADwjmIIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAA9hAAAPYQGoP6dpAABZ8UlEQVR4nO3deXgURd4H8G/15CAHSbhiAuGGiAcERDwAF9YDEFgRUWHRlUNQ5FxZXFnEC0FBxZNFEIKALAIiCCqIirqvCIqAnCIhBEi4NkGSQA5yTNf7x5BOJjOTzEy6M9f38zw+6a6urq4pRvKjqrpKSCkliIiIiPyM4ukKEBERERmBQQ4RERH5JQY5RERE5JcY5BAREZFfYpBDREREfolBDhEREfklBjlERETklxjkEBERkV9ikENERER+iUEOERER+aUgT1fAG2RnZ6O0tFTXMhs1aoSsrCxdy6RybF9jsX2NxfY1FtvXeJ5u46CgINSrV6/6fLVQF69XWlqKkpIS3coTQmjlcmsw/bF9jcX2NRbb11hsX+P5UhtzuIqIiIj8EoMcIiIi8ksMcoiIiMgvMcghIiIiv8SJx1WQUiIvL8+tiVWFhYUoLi42oFb6EkIgMjJSm0hGRETkLxjkVCEvLw+hoaEICQlx+d7g4GBd39gySnFxMfLy8lC3bl1PV4WIiEhXHK6qgpTSrQDHl4SEhHj9K4BERETuYJBDREREfolBDhEREfklBjlERETklxjkEBERkV9ikOOnli5diptvvhmtWrVC//798euvv3q6SkRERLWKQY4f2rBhA1588UVMnjwZX375Ja699lo89NBDOH/+vKerRkREFciDe6Du+M7T1fBbXCfHSVJKoLjI+fyqGVKvdXJCQl1arG/RokUYOnQoBg8eDACYPXs2tm7dilWrVmH8+PH61ImIiGpMffsFAICMqQ9xTZJnK+OHGOQ4q7gI6vgHnc7ufDhUPWXeGiC0jlN5i4uLsX//fqtgRlEUdO/eHbt379axVkREpBd18VyY5i73dDX8Doer/MyFCxdgNpvRsGFDq/RGjRohKyvLQ7UiIqIqFRZ4ugZ+iT05zgoJtfSoOEnXbR1CQvUph4iIvBO3DzQEgxwnCSGcHjICABEcDKGYDKyRffXr14fJZLKZZJyVlYVGjRrVen2IiMg+qaqeroLf43CVnwkJCUGHDh2wbds2LU1VVWzbtg2dO3f2YM2IiMhKaYXe/uJiyJPHPFcXP8Ugxw+NHj0aK1euxJo1a3D06FFMnToVhYWF2ttWRETkeXL7VqtzdeaTHqqJ/+JwlR8aMGAALly4gNdffx1ZWVm47rrrsGLFCg5XERF5EfmfBZ6ugt9jkOOnRowYgREjRni6GkRERB7D4SoiIiI/Is1mqMlvQv3hK09XxeMY5BAREfkR+csPkD99B7l8nqer4nEMcoiIiLyEVM01L6Qwv+Zl+AkGOURERF5CffJhHUopX1nQ/NbzkOdO61Cmb2KQQ0RE5C0KdOiFqbih86Ffob71fM3L9FEMcqohpfR0FQzl75+PiCjgXKi0T+EfmZ6phxdgkFOF0NBQFBYWeroahiooKEBoKPfGIiLyF/LLTzxdBa/BdXKqEBoaivz8fOTm5lr2rnJBSEgIiouLDaqZPqSUCAoKYpBDRORP2EOvYZBTjYiICJfvEUIgPj4eZ8+e5XAQERGRh3C4ioiIqBbJywW6/QNYHjkI80tPQl7K1aU8f8Mgh4iIqJbI1MNQJwyBXPaOLuWpr08D0o9Bnfw3XcrzNwxyiIiIaom64T8AAPnjVod5OM1BPwxyiIiIakva79XnOXLA+HoECJcnHv/222/YuHEjjh8/juzsbEyZMgU33XSTdl1KiTVr1mDr1q3Iz89Hu3btMGrUKMTHx2t58vLysGTJEuzevRtCCNx8880YMWIE6tSpo+U5efIkkpOTcezYMURFRaFPnz4YMGCAVV127NiB1atXIysrC3FxcXjooYdwww03uNMORERExnPirVuZeRaiXYdaqIz/c7knp6ioCC1atMCjjz5q9/qGDRuwefNmjB49Gi+//DJCQ0Mxa9Ysq9ep33nnHWRkZGD69OmYOnUqDh8+jIULF2rXCwoKMHPmTDRs2BCzZ8/Gww8/jI8//hjffPONlufIkSN4++23cfvtt2POnDno0qULXnvtNaSnp7v6kYiIiGpHm2tt02IbW5+bddi/igC4EeR06tQJQ4YMseq9KSOlxKZNm3DfffehS5cuaN68OcaPH4/s7Gz88ssvAIBTp05h7969GDNmDNq2bYt27dph5MiR2L59Oy5cuAAA2LZtG0pLSzF27Fg0bdoU3bp1w913343PP/9ce9amTZvQsWNH3HPPPUhISMCQIUPQqlUrfPnll+62BRERkaFE66tt027u4YGaBAZd18nJzMxETk4OOnQo72YLDw9HmzZtkJKSgm7duiElJQURERFo3bq1lqd9+/YQQiA1NRU33XQTUlJScM011yAoqLx6SUlJ2LBhA/Ly8hAZGYmUlBT079/f6vlJSUlaMGVPSUkJSkpKtHMhBMLCwrRjvZSVpWeZVI7tayy2r7HYvsbyhvaVUkJ+9wVEs9YQba6xvlh02Sa/cns/mD/7SDsXQrhV/6ruCdTfcboGOTk5OQCA6Ohoq/To6GjtWk5ODqKioqyum0wmREZGWuWJjY21yhMTE6NdK8tb1XPsWb9+PdauXaudt2zZEnPmzEGjRo2c/ISuiYuLM6RcsmD7Govtayy2r7E82b6FO7fh/ErLFIymX+yyunaxZRvkfm+dP75lK8hPfsDpQbcBsPwui6wwj7UqGRXLuXJPhp188U6W5wpf+A4H1IrHAwcOtOr9KYtCs7KyUFpaqttzhBCIi4vDuXPn+CqgAdi+xmL7GovtayxvaF/1yCHt+NQ7s2B6YGT5tQLb/RDPZp0HVFU7z714EZfOnnX5uWcd3RNR1/E1N3hDGwcFBTnVQaFrkFPW25Kbm4t69epp6bm5uWjRooWW5+LFi1b3mc1m5OXlaffHxMTY9MiUnVfMk5trvcJjbm6udt2e4OBgBAcH271mxB+UlJJ/iRmI7Wsstq+x2L7G8mT7yuzz5cdb1kPePwJSVSEUBbJCMGN1T8WhnwaxbtXd4T2qOWB/x+m6Tk5sbCxiYmJw4ED5O/4FBQVITU1FYmIiACAxMRH5+flIS0vT8hw8eBBSSrRp00bLc/jwYavelf3796Nx48aIjIzU8lR8Tlmetm3b6vmRiIiInCbz8yC3rLdKM8+dDnXaY5BFRTabZyrzLTuGCyGAxs2uJLr3q1n9ZBnkgV22FwoLLM8OQC635OXLl3HixAmcOHECgGWy8YkTJ3D+/HkIIdC3b1+sW7cOu3btQnp6OubNm4d69eqhS5cuAICEhAR07NgRCxcuRGpqKn7//XcsWbIEXbt2Rf369QEA3bt3R1BQEBYsWICMjAxs374dmzdvthpq6tu3L/bt24fPPvsMp0+fxpo1a3Ds2DH06dNHh2YhIiJyw9GDtmm/7wf+yAQO7gKkdU+OqDi6YDJZfrrZOyK//ATqUgfbRZw75VaZvs7l4apjx47hxRdf1M6XL18OAOjRowfGjRuHAQMGoKioCAsXLkRBQQHatWuHadOmISQkRLtn4sSJSE5OxowZM7TFAEeOLB+zDA8Px/Tp05GcnIypU6eibt26GDRoEO68804tz9VXX42JEydi1apV+OijjxAfH4+nnnoKzZo1c6shiIiIakoeP+r4WsZxILSOw+sQV/odpP0hLadczLGf7mbvkK9zOci57rrrsGbNGofXhRAYPHgwBg8e7DBPZGQkJk2aVOVzmjdvjhkzZlSZ59Zbb8Wtt95adYWJiIhqy2XbicVl5BfWvztF74HWGcrm5TiYt1MjDHKIiIioRhpe5XzekFDr87JAxInhKnn8KJB/yfln+cCaNkZgkENERKQT0epqODujRnS9wzpBcX64Sn35H65VLEB7cgLzUxMRERnBhaEmUbnXRxuuMuC1bO9+09swDHKIiIj0UpP5NGVBjhFr2qTYeesrADDIISIi0ot6ZQfxxm686evk21VuLcBnZ8+sQMAgh4iISC9lAUo1c2CUl96zTbzSk+NoVWRNbrbr1Vq33OV7/AGDHCIiIr2UBShmc5XZRFwT20Rn364qLXG9Xmb99mf0JQxyiIiIdCJ//clycNbeXuDVuDJcJRfPhTx32nG+SxcdXyMrDHKIiIh0Iv9vS7V5xENj7F9QyteyUZ99wvHcm8sF7lQtIDHIISIiqkXy4B77F4T1r2S59gP7+UqKXX9ogC4GyCCHiIioFokWbRxcsA5E5Fef2s9X7M6O4gxyiIiIyFOc7G2RmWfdKDwwVwNkkENERGQA0bOv5ecDI60vOHpFPOO4U+XKzWtdr4wBCwz6AgY5REREBhB/fQzKS+9B3DXAOr1zN/s3XMiySZL2XkV3YlVl5enZELf3d6qe/oxBDhERkQGEokDENYGoNAwlmjR3ugz53822iQ1iq392m2uh/PUx67KOBN7WDgxyiIiIvNWpEzZJovtdbhWlvj6thpXxPQxyiIiIdODWnlLVEba/poUTPTlkwSCHiIhID1UEOeJPvd0r087Cf9XubUUaBjlEROQ3pKqi6OCvkJ7YdbuK4EMMHQPl7y9Cefsjl4qUO//PznOq2Bfr+s5Q3lrpuLyzp1x6vq8L8nQFiIiI9CClhPrYAGReOTct2li7z9/oOLgQJhNwXSf3yi0ugggJhSy6DLn+QyAuwWFe06TnqyxLfW4slDdXQERGuVUXX8Mgh4iI/IL8ar1nn+/O+jXOOPobcF0nqOMfrDKb6DPIufL+yAQY5BAREfkOuXapp6tQLrKubkXJ3OwqN2UQXe8AmjSH0uteJ0sMnC0eGOQQERHpzc5bUW4zmap+1C09Ia5J0u95foQTj4mIiPQWFKxfWYUFkKWldi+JAQ8B7Tro9yw/w54cIiIivSn69SGI2DigtMT2QlwTKP0Hu1FgzevkK9iTQ0REPs/r1o5xckdxp4RFAL/vs00/d1q/Z/gpBjlEROT7Lhd6ugZAVIxhRcuCfB1LC5yuHAY5RETkBwzYUsFVLRONKdfbeql8CIMcIiLyfV4Q44iKi/TpuY+VlMChvTqWFzhBE4McIiLyfSXFnq4B5KE92rG4vb/rBcQ2dlCwhLjhFtv0eg1dfwYQUD1DDHKIiMj3nThqkyT/yLSTsWpyzw6Y330J8tJF1+tw6oR2KO68x+XbxZ/7Wg5i6gMVdxqXKuT+XTb5lb8+5vIzADDIISIi8in2Nq28mON6Me+9Auz/BXL98hpVR7jxCrm4vR+UcdOgPPsWYKqwwouqQu741ia/PJHqXuUY5BAREfkQuysMu/8WkbyU635d3CQUE0THWyCiYqyHu6S0P8cn66x7D2KQQ0RE5EMaXmWbptTkVWnPvmatDV0BDicKy6LL7hVur9fLTzHIISIi32fvF3dNFuTz8BtIQlGApi0tJ6r9N7VEl+7uFa7nm19ejkEOERH5Prt7O9UgyNm30/179VI2BOcgKBFXJdhNr448csDdGvkcBjlEROT77M0z0XNrBU9IPwYAkI7m3oRHuFWs/GajuzXyOQxyiIjI95UFOXFNytNcHJaRlYa81K8+rWGl9CE/et/+BWeCHHsTssPcC458EXchJyIinyczz1gOKmxaKff+DNGslfOFVBrykh8vAXrdW/2zc7OhLnnT+efUkPjbWMCsQtSNrj6znblF4u77DaiVd2JPDhER+Ty5fJ5t2mcfuVZI6m/uPXvtB8Bve9261x3Kn/pAqfj2lavcHObyRQxyiIjIp0m7k45dp775vP3yCwsgUw5BOlhfRl6s/TV1aiSA3q7icBUREfk09Yn7jC1/4hAAgHhkPMRtvWwz+Nr85gAKctiTQ0RE5ISyITGZngZZcWjLm9/iqrjp53WdrhwETpDDnhwiIiIH5OVCq3P1p+8gky2TjJW5yyGiYoDT6bVWHzF8kmv5W7Ytn5T9R5bl55naq6+nsSeHiIjIkZISq9OyAAcAkJtt+Zl9vtaqI6JiXLuhUVz58blTAAD51adQN3+iX6W8GIMcIiLySTL/EmT2H/oVGN/UNq1G+1/VjOh+l51EVwuxf4Nct8z1CvkgBjlERORz1G82QP37Q5Bb1ulWpuh2h73UKm7Q7dH2hdaxTWvXwbUy7O7OHjgC+9MTEZFPkquTLT+3fmbsgxy8Nl4VZcxUfZ59ucAmSQQFu1aGN0+KrgUMcoiIiAC7r1bL3/e5XIzo3FWP2kD+uLXmhSiB/Ws+sD89ERFRGTtvVss1ybVfDx2JHncD9RtB3PEXT1fFIxjkEBGR31CefLEGd9uJcnIuOMytvjgJ6o7vavA844mISCizF0MZMtrTVfEIrpNDRER+QVn4KRSTCa7PornCjZWAZS1uzOkuEcDzctiTQ0REvs8UBFHT+ScBtN1BoGCQQ0REvk+P3ooAC3KkavZ0FQzHIIeIiHyKvGRn129dFu2reZDjUxN8Vf8P6hjkEBGRT1Ffeco2UZhqXrAev/PrN9ShkFqy9ydP18Bwuk88VlUVa9aswQ8//ICcnBzUr18fPXr0wKBBg7TJT1JKrFmzBlu3bkV+fj7atWuHUaNGIT4+XisnLy8PS5Yswe7duyGEwM0334wRI0agTp3yFSBPnjyJ5ORkHDt2DFFRUejTpw8GDBig90ciIiIvof70PZB1zvaCHj05egxXubpYX1XadQB+369feZXInAuGL9rsabr35Hz66af4+uuv8eijj+LNN9/EQw89hI0bN2Lz5s1ang0bNmDz5s0YPXo0Xn75ZYSGhmLWrFkoLi7W8rzzzjvIyMjA9OnTMXXqVBw+fBgLFy7UrhcUFGDmzJlo2LAhZs+ejYcffhgff/wxvvnmG70/EhEReQmZ/Ib9C8VFepTu3m1XNdEORVd7W0O4KYDfitKL7kFOSkoKbrzxRtxwww2IjY3FLbfcgg4dOiA1NRWApRdn06ZNuO+++9ClSxc0b94c48ePR3Z2Nn755RcAwKlTp7B3716MGTMGbdu2Rbt27TBy5Ehs374dFy5Y1izYtm0bSktLMXbsWDRt2hTdunXD3Xffjc8//1zvj0RERN7OrMMkWnc7cv53GgCgPPUKRJ2wmtdDq4//z5kxmu7DVYmJidi6dSvOnDmDxo0b48SJEzhy5AgeeeQRAEBmZiZycnLQoUP5JmPh4eFo06YNUlJS0K1bN6SkpCAiIgKtW7fW8rRv3x5CCKSmpuKmm25CSkoKrrnmGgQFlX+EpKQkbNiwAXl5eYiMjLSpW0lJCUpKSrRzIQTCwsK0Y72UlRXIaxMYie1rLLavsdi+xhFC2LSra+1cw6Ci+LK+f6529s3S+3eVO+X50ndY9yDn3nvvRWFhIZ588kkoigJVVTFkyBDcdtttAICcnBwAQHR0tNV90dHR2rWcnBxERUVZXTeZTIiMjLTKExsba5UnJiZGu2YvyFm/fj3Wrl2rnbds2RJz5sxBo0aN3P24VYqLizOkXLJg+xqL7Wsstq97Mqq4VjavM8NOmjNyIyNx0b1qAQAaXBWPOi48rzrFk6Yj6+nHoOaV18qVz1NZ5baLio5G3RqU5wvfYd2DnB07dmDbtm2YOHEimjZtihMnTmDp0qWoV68eevbsqffjXDJw4ED0799fOy+LQrOyslBaWqrbc4QQiIuLw7lz5yDZ3ag7tq+x2L7GYvsa5+zZsza9C2fPnnX6fvOlSzV6/oWcHAgXnlet0AiINz6EWPIm5E/fA3Dt81TnYm4u8twozxu+w0FBQU51UOge5KxYsQIDBgxAt27dAADNmjVDVlYWPv30U/Ts2VPrbcnNzUW9evW0+3Jzc9GiRQsAlh6Zixet42mz2Yy8vDzt/piYGK1Xp0zZeVmeyoKDgxEcbH/muxF/UFJK/iVmILavsdi+xmL76s9ee7rUxjX881BPHYfS9toalWFDCKv1bHT9ztRrUKPyfOE7rPvE46KiIiiVltZWFEVriNjYWMTExODAgQPa9YKCAqSmpiIxMRGAZV5Pfn4+0tLStDwHDx6ElBJt2rTR8hw+fNiqB2b//v1o3Lix3aEqIiIKDErd6Ooz2VOQX3W5L86D8vZHjjP8keXec6tlUCAR7v+/K3UPcjp37ox169Zhz549yMzMxM6dO/H555+jS5cuACzdXH379sW6deuwa9cupKenY968eahXr56WJyEhAR07dsTChQuRmpqK33//HUuWLEHXrl1Rv359AED37t0RFBSEBQsWICMjA9u3b8fmzZuthqOIiCjwCDd+ecu8i5Dfb7KctLkG4s99bctt3AwiPMJxIaUljq/VhFG9JV7eC6MH3YerRo4cidWrV2Px4sXIzc1F/fr1cdddd+H+++/X8gwYMABFRUVYuHAhCgoK0K5dO0ybNg0hISFanokTJyI5ORkzZszQFgMcOXKkdj08PBzTp09HcnIypk6dirp162LQoEG488479f5IRETkQ+p07IL8LadduynlUPnx/84A9VxfuVgk3eTyPU5hkOM23YOcsLAwDB8+HMOHD3eYRwiBwYMHY/DgwQ7zREZGYtKkSVU+q3nz5pgxY4a7VSUiIj8k3Fl1uOKE5Uu57i3E10bn+ThlDAtG/D/I4d5VRETkX4Lc+Pd7pbmkiHA85KVMfN5uunDwYkuN6T2ZuYz/xzgMcoiIyL9E9Crfw1Ae+tXmutz/C+SpE9aJlYIccd8jDssX7TtD3PLnGtXRFaJnX4i/jYMyc4G+BUvbxQb9DYMcIiLyK0qFicfqW89Dnv+fdi7Tj0F99yWoL060ukempVidizrhVT5D9HtQh5o6R5hMUP7UG+KqxjUrKK6J9Tl7coiIiHyb+q/R2rE8nW43j/x8VdWFNLxKzyp5hmKyPg+AnhzdJx4TEREZQV4utJsu+j0I0a3Cm7VVzRl2c7slZeqr7t3oTSpPpmZPDhERkZfIs7+zlOjyJ4hGzu6jZBvlOLNqr4iuVymhUjkRdZ18vgeJSr/yA6Anh0EOERH5BkdvTVWOW6p6/TvtiE2S3PqZ63Upumx93vY618uobUrldvH/rhwGOURE5NtshmHs//KWpSWQ331hm756sevPzLfezFPp+4DrZdS2yj05KoMcIiIi73DymFPZShzlO7RXv7rEJ1if1wnTr2yj2PRwWQc5ct8vML/+DOQfmbVXJ4MxyCEiIp+gzptp/0LlnhvV/lwTebnA+jw9zW4+G4l2hqIio6zPC/KcK8uTqujxkkcOQJ33EnDkANRl79ZyxYzDIIeIiLxelZODM89anQY3bWk/X6XgR547ZTebMmO+5eczcyFu6wXl8X86U8Hq83hapSBH7t2pHatzny2/cDGnlipkPAY5RETk/UpLrc+btyk/rvTL29FO4SIk1Opcfr7afr4rQ1GiRVsoj4yHiKpnm6nyNhAmk20eLyPim1qdy5++q3BSIQD0hYDNSQxyiIjI+1XqdVGGTyg/aXV1pcwO3q4KtQ5ycDbD8tON179F5YX1rmpiP6MXEQ+MdC4jgxwiIqJaVKkXBqJCkFE5SHH0Bnm9htbnV3pfxA231qxucNx75E1ERCSUZ9+sPmNZ8OcHuOIxERF5PZn6m3VCXBNL70l4hO2EWjvr5EjVbFtoaB3LtZwLelXT64lmrT1dhVrFIIeIiLzfxVyrU2EyQZkxDxAKROU5Ofa6cvLzbIdhCvIhd20DDuzSu7Y+wWorDD/F4SoiIvJ6onXleTeWeTGVAxzLBTtpDva9UitPPg71gfVuaqpDF8vP1u0cZrHb8+WDGOQQEZH3qxPufF57QU5IKOxuY3D6ZKV8IS5VyydVte1FGbN/7GvFIIeIiHyAC2/82Psl7uRmlOKeoc4/x585Ewj5AAY5RETkX+z9glZVp+Ikpefd+tfHW1Xxqrj8ZFktVsQ4DHKIiMj75WY7n9dekGM2V7v+i+g10MVK+SgnemnkNxtqoSLGY5BDREReT31nhgu53Ryuio5x4Rk+LPsPy08Hk7H9CYMcIiLyL/Y6KlQVsrpF7nxhJ3E9nEwFAMiPl3i4IsZjkENERH7F7mvlqgq5eG7V90VGG1Qj8hQGOURE5F/sBDnqe69Uf5+j3cur46dr61S587uPYJBDRETer0GsC5nt9OScO139bfZ2G3dGtJv3eTsGOURERLWgpFg7VF56r+q8bq7xIirvUl5d/sf+CcQ1gfLEVLee5/UqtLmv4t5VRETk/UpKtEMR16TqvG7EOMqsha7f06U70KW76w/zFSZT9Xm8HHtyiIjI+5UUOZ9XuP6rTcTGu3yP3/P90SoGOURE5ANKS53P6ydbEnie70c5DHKIiMiv2H2FnFzHicdERETkl3w/xmGQQ0REAc4U2O/gSD94i8oRBjlEROR3lLHTIIZPdC7zVY2NrYy3Szvi4ILvd+UwyCEiIr+j3HArlG53Opk5wH8VOtqzi3NyiIiIasHV7QEAosttNS5K3PJnq3PlvmE1LtOnVN6GwuEr9wxyiIiIDCWlBI4csJw0b1PzAisNT4n2nWtepi8xVfrVrzh4G833YxwGOURE5OUu5WiH8ufva1yc6HhTjcsICByuIiIiMljFhQBzLtS8vOj65cdRMTUvz+dU6rlxGMswyCEiIjKW2Vx+fCm35uUpSvkcnx5317w8X1N5sURHPTa+H+Nwg04iIvJyFYMcPQgBZfx04HgKkHi9vmX7JD+IZhxgTw4REXk3Ve8gR4GoEwZxTRKEH+y07bLK84wdxThHDxldE8MxyCEiIu/myuaczuDeVpXYj3LUf8+q5Xroj0EOERF5N723HQjwbRxsJx6XBznK82/bvUPu2Q71/dcgLxcaWTHdBfqfNBERebvSkvLj9jfWvLxAHKKqKO+i9XlZjFO/IdCkhd1b1PdmWw6uagzc+7BhVdMbe3KIiMi7FZf35IgWbWtcnAj0bRxsgryyKEdAVDeUd1GHt9tqUYD/SRMRkdcrKdIORa97XbpVmfwS0PAqnSvk42IrbUhaNlzlzFwlHwsQfau2REQUcGRJheGq0Dou3SuuSYIyY77ONfJxzq6TYw+DHCIiIh0VV+jJcePNKBEcrGdtfF/RZe1QXsoFiq5MJv4js/p7fezNNAY5RETk3XzsjR6vV3FxxdJSqG8+X2V2WTEo2r7VqFoZgm9XERGRV5M7vvV0FQKWvFwI9e0XyxMKCzxXGTcwyCEiIu8WEurpGvivCr009qjPjQOyz9dSZfTH4SoiIvJuF3M8XQP/UmFejbp4btV5fTjAARjkEBGRlxOt23m6Cv7rZKqna2AoBjlEROTdrulo+RmX4NFqkO8xZE7OhQsXsGLFCuzduxdFRUWIi4vD2LFj0bp1awCAlBJr1qzB1q1bkZ+fj3bt2mHUqFGIj4/XysjLy8OSJUuwe/duCCFw8803Y8SIEahTp3yNhJMnTyI5ORnHjh1DVFQU+vTpgwEDBhjxkYiIyFOkavnJRf304VtvgdeI7kFOXl4enn32WVx33XWYNm0aoqKicPbsWURERGh5NmzYgM2bN2PcuHGIjY3F6tWrMWvWLLzxxhsICQkBALzzzjvIzs7G9OnTYTabMX/+fCxcuBCTJk0CABQUFGDmzJlo3749Ro8ejfT0dLz33nuIiIjAnXfeqffHIiIiT1GvBDk+thCd96pBlBMWUX0eL6L7N2bDhg1o0KABxo4dizZt2iA2NhZJSUmIi4sDYOnF2bRpE+677z506dIFzZs3x/jx45GdnY1ffvkFAHDq1Cns3bsXY8aMQdu2bdGuXTuMHDkS27dvx4ULFwAA27ZtQ2lpKcaOHYumTZuiW7duuPvuu/H555/r/ZGIiMiTtCCnBhtrdrwFACDuZG9/jRTmQ/2/LZ6uhdN078nZtWsXkpKS8MYbb+C3335D/fr10atXL613JTMzEzk5OejQoYN2T3h4ONq0aYOUlBR069YNKSkpiIiI0Ia3AKB9+/YQQiA1NRU33XQTUlJScM011yAoqPwjJCUlYcOGDcjLy0NkZKRN3UpKSlBSYXlwIQTCwsK0Y72UlaVnmVSO7Wsstq+x2L6uE1KFBCBMSrXt5qh9TWP+CZw8BrRsy7aH420cnGkbdfk8YPBwn2hH3YOczMxMfP311+jXrx8GDhyIY8eO4YMPPkBQUBB69uyJnJwcAEB0dLTVfdHR0dq1nJwcREVFWV03mUyIjIy0yhMbG2uVJyYmRrtmL8hZv3491q5dq523bNkSc+bMQaNGjWrwiR0r670iY7B9jcX2NRbb13kXg4OQCyCkpBixFeZuVsVu+zZtpm/FfNTZyCiUXrD/anh8fDwynCzHF77Dugc5qqqidevWGDp0KABLIJGeno6vv/4aPXv21PtxLhk4cCD69++vnZdFoVlZWSgtLdXtOUIIxMXF4dy5c5CubHxGTmH7Govtayy2r+tKl84DABTt34WzZ89WmZftWz1z3WiH16pr34o82cZBQUFOdVDoHuTUq1cPCQnWr/klJCTg559/BlDe25Kbm4t69eppeXJzc9GiRQstz8WLF63KMJvNyMvL0+6PiYnRenXKlJ2X5aksODgYwQ42ajPiD0pKyf/JDMT2NRbb11hsX/c422ZsX8eqahdX2swX2lj3icdXX301zpw5Y5V25swZLeKKjY1FTEwMDhw4oF0vKChAamoqEhMTAQCJiYnIz89HWlqalufgwYOQUqJNmzZansOHD1v1wOzfvx+NGze2O1RFRES+SdzWy3IQ39SzFSGfo3uQ069fPxw9ehTr1q3DuXPnsG3bNmzduhW9e/cGYOlK7Nu3L9atW4ddu3YhPT0d8+bNQ7169dClSxcAlp6fjh07YuHChUhNTcXvv/+OJUuWoGvXrqhfvz4AoHv37ggKCsKCBQuQkZGB7du3Y/PmzVbDUURE5AfqNwQAiDbXeLgigUE8Mt420UfXKNJ9uKpNmzaYMmUKVq5ciU8++QSxsbEYNmwYbrvtNi3PgAEDUFRUhIULF6KgoADt2rXDtGnTtDVyAGDixIlITk7GjBkztMUAR44cqV0PDw/H9OnTkZycjKlTp6Ju3boYNGgQ18ghIvI3ZSMiguvk1Abltl4wL59nnfbP2VD/OUI7z/jLzVD+/gLENUm1XT2XGLLicefOndG5c2eH14UQGDx4MAYPHuwwT2RkpLbwnyPNmzfHjBkz3K4nERH5gLJ5H97/xrL/qvy6uGqG+sazMC3a6Jn6OIlhMREReTctyGGUowt3Jgsrvtn2DHKIiMjLMcjxOB8dKvTNWhMRUeBQtUk5Hq1GYPPNtmeQQ0REXu5KkMMNOj2Hw1VEREQG8PIF53yNiGngxk0McoiIiPTHice6Eg+MtH/BwTpEovdAOBqukhU2vfZGDHKIiMi7McjRlagbZf9C6mH7+e8bBpgcrDijmnWqlTEMWSeHiIhIPwxyPEkoChAa6ulquIU9OURE5N3Yk+MVlInP2yZ6+XwpBjlEROTdJF8hr23KE1MBAGJ4+c4Don1nKO+tg+m5t8szenmQw+EqIiLyWrIgDyi9MrmVPTm1RtzQFcp76yCCrMMEERRkvRu8VGu5Zq5hkENERF5JXsqFOvlv5QkMcmpV5QBHU3G9ItU6yJFSAsXFEF4yh4fDVURE5JXk7wesExjkGEr0fdC5jBWDnErDVepTw6GOfwDSwZtatY1BDhEReadfd1RKYJBjqPgmTmUTFYPNSj05yM22JM95Wq9a1QiDHCIi8krylx+sE3x0awHf4UL7KibLT3OpZYgK0H56EwY5RETkIxjkGMqV4cArQ1bq049C/fcsS1rFSchess+Yd9SCiIioEtHr3koJDHIM5UL7iopBzL6dlp+lpeVplYexPIRvVxERkVeSO76zTvjfGc9UJFC4EOTI4iLr87yLkPt36V2jGmNPDhEReadLuVancud/PVSRAFGxJ8ZFcvtWyG1fWadVCoQ8gUEOERERAZcL3L837xJw9DerJHXcAzWsUM0xyCEiIiJAuB8SyJSD9tOP/e52mXpgkENERN4pMsr6PKKuZ+oRKGryir6jYOZijvtl6oBBDhERuU1eLoDctQ3ycqH+heddtD6Pjdf/GVSubO0bHclK86pqG4McIiJymbxwHurapVBfewbqwlchl76jb/l/ZNmkibbX6fqMQKa8vRKIrNQzZsDaNnLjSt3LdAVfISciIpep814CMo5r53L3j7qWL3dstUkTdw3Q9RmBTIRHAg2uskwYNlLlIcdaxp4cIiJyXYUAxwhyg20PgIipb+gzA07ldXGM2FTzqsb6l+kCBjlERORVvHEPJH8ket5tnVBkwLo2eypvslq7OFxFREReRW772upc3D0Iomdfz1TGj4mud0AktIQ680lLgsn/+j387xMREZEhZGEB1M9XQ547Zf+6Dj0w8o9MyOXzrNKU+4ZB1G9U47LJmhACaNqyQoL/hQT+94mIiMgQcuUCyA3/gfrsWPsZLti+EeUqdfoTNS6DXFBxXk7daF2KVOYuszqXZrMu5bqDQQ4RETlF/vS98Q8pLTH+GaQRFYOcqBh9Cg2vC3HfsPLzrLP6lOsGBjlERAFKSgn1q08hU3+rPrMzzGbIvT/BPPoeyEO/ul6fE0dt0sT9w3WoGNUqRYHoc1/5uU49RO7gxGMiokC1Zwfkx0sgAZgWbaxxcXLPdshPLEMV6lvPu16mneEuccc9Na4X1S5xZVFB5YV5QGkxhAe342CQQ0QUoKTOwwhlAY7b7GwrIIL4a8orCQFUM9FcNGlWS5VxjMNVRESBSu8AoqZv5xiwrQAZI8gLAhhn8BtFRBSogoL1LU+qNbufvTY+Q1T87sQ39VxFqsEgh4goAEkpIb/e4FJ+o8nKq+OGhBr+THJThSBHNGvlwYpUjUEOEVEAktu/BTJdmJPjxqvd8ky6a/n/+6XVuTJllsvPpBqovJdVFYIaxWnH8uf/lhdR8dVxL8Agh4goAMmlb1ufV9dTcynX9YfUdM2b5m1qdj8Zpt7YpyE63Ahl4nNW6crdgzxUI/s4AEpEFGBUOzt8Q6qAsH27SePG5o3y7CmIZq1dvg8AxMC/aa8iUy1xYUjSVL8hTBOf9/rNVPkNIiIKIPL8/yA/X2V7wVzNpOFg20nKyqyFUN5c4fhZi+dC5l+CuuxdyKOuLTjI9XE8ILSOW7eJvz6mc0X0w54cIqIAIn/fb/+CudRuIKMpKbZJErHx1T5P/ftDludu+9qlxQFFKCcd1xbxwEjIQ79CdL3dvfv/3M8y+Tihhb4V0wGDHCKiACI/ddDzola9iaJMTzOgNuQNlF73Ar3udft+IQTQ5lrd6qMnDlcREQUS1cGwVDXDVXLxXAMqQ2QsBjlERIHE0YJ95lLHt+zeblBliIzFIIeIKJDkXbKf7mC4SpaWQl0w2ya98qvDRN6IQQ4REQFm2yBHHtgF9Yn77GYX7W90+RHe/rox+R8GOUREgcx0ZW0cO3N11Hdm2L1FmbvcvWflZrt3H5GbGOQQEQWyKz048sdvnL5FRMW49Sh10esOr8mUQ26VSVQVBjlERAFKmfFv7VhuXmv8A1MOOrykvvYv459PAYdBDhFRgBLxTa3O5a8/2T22useADRhlpaEy8eCjuj+DAhODHCIiAgCoHy8pP57/st08oo/tRGRxY/eaPbjSlg8irknNyiO6gkEOEVEgia4HABD9h9heyzpX5a2i7wOW1W0ri4xy+vHyjyzbtN3brM/37nS6PKKqMMghIgokjZtZfl7V2PIzNMyp25S5y6AM/Jv9i2F2yuh0i/28lwttkuTpdOsEc4lTdSKqDoMcIqJAcngfAEBc2YxTdLyp2luUVxZBRNVznKFOuE2SCI90UJj1rx1ZkG87Iblpq2rrROQMwzfo/PTTT7Fy5Ur07dsXw4cPBwAUFxdj+fLl2L59O0pKSpCUlIRRo0YhJiZGu+/8+fNYtGgRDh06hDp16qBHjx4YOnQoTGVrOgA4dOgQli9fjoyMDDRo0ACDBg1Cz549jf5IREQ+SZaWb92gfvsFTJ27Ac68Dh5Zt+rrdWx7csStf7a8lh7bGMg8U34hKAjy0kXI9cshut9ld30e4YW7WZNvMjTISU1Nxddff43mzZtbpS9btgx79uzB5MmTER4ejuTkZMydOxcvvfQSAEBVVbzyyiuIiYnBzJkzkZ2djXnz5sFkMmHo0KEAgMzMTMyePRt33XUXJkyYgIMHD2LBggWIiYlBx44djfxYRES+qbS4/PjUCctPRxt2VlTdkFalnhzx0BMQV7e3vKJevxEQFAx1zEDtujr5YQCA/OEr4Or2NsUJO2lE7jBsuOry5ct499138fjjjyMiIkJLLygowLfffothw4bh+uuvR6tWrTB27FgcOXIEKSkpAIB9+/bh1KlTmDBhAlq0aIFOnTph8ODB2LJlC0qv/Evkq6++QmxsLB555BEkJCSgT58+uOWWW/DFF18Y9ZGIiHxbhYBGmXZlYT4He1ZVZHeyccXrlXpyxJWeHxHfFCK0DoTJVN7bU3mD0CMHrE5NizZWWx8iZxnWk7N48WJ06tQJHTp0wLp167T0tLQ0mM1mtG9fHqk3adIEDRs2REpKChITE5GSkoJmzZpZDV917NgRixcvRkZGBlq2bImjR49alQEASUlJWLp0qcM6lZSUoKSkfEKbEAJhVybMVfc/sSvKytKzTCrH9jUW29dYHm3fCkGOiI23Wwdn0yqS4RHWCeGRtvdcmYsjSh3vdu7Ms6rD76/xfKmNDQlyfvzxRxw/fhyvvPKKzbWcnBwEBQVZ9e4AQHR0NHJycrQ8FQOcsutl18p+lqVVzFNYWIji4mKEhITYPHv9+vVYu7Z8Vc+WLVtizpw5aNSokasf0SlxcXGGlEsWbF9jsX2N5Yn2NYcE4QwAKAoaN7GsRVN871D877tNAIDg5q0RFx8P86VcVJhFg/j4+CrLLbr4BzIrnDeMi0dopXtOBwVDBVDn5++R76Cc8J590KCaZzmL31/j+UIb6x7knD9/HkuXLsX06dPtBhqeNHDgQPTv3187L4tCs7KytGEwPQghEBcXh3PnznHXXQOwfY3F9jWWnu0rS0uAvIsQMQ2cy//HlVBEMeHs2bOW44hoKA+MgPrxByiNqIuzZ89C/e+XVvdpeR2VeynP6vyPnByISveoF3MAAPmbHG8fUfTn/tU+qzr8/hrPG9o4KCjIqQ4K3YOctLQ05Obm4umnn9bSVFXF4cOH8eWXX+KZZ55BaWkp8vPzrXpzcnNztd6bmJgYpKamWpWbm5urXSv7WZZWMU9YWJjD4Co4OBjBV16brMyIPygpJf8nMxDb11hsX2PVtH3l8RSoL08BACjPvw2R0LL6e8r+MWcyWT1bXnk9XJbV62yGTV2rLDfI+u9VGRwCuPjZlOlvAo2b6fad4/fXeL7QxroHOe3bt8frr1vvNPvee++hcePGGDBgABo2bAiTyYQDBw7gllssi0WdOXMG58+fR2JiIgAgMTER69atQ25urjYktX//foSFhSEhIQEA0LZtW/z6669Wz9m/f79WBhGRPysLcABA7vwBiG9mmeBblYIrPS6O8pX9wsrN1pLEsAnVVya4wj8sr+8MuPEKuGje2uV7iKqj+9tVYWFhaNasmdV/oaGhqFu3Lpo1a4bw8HDcfvvtWL58OQ4ePIi0tDTMnz8fiYmJWoCSlJSEhIQEzJs3DydOnMDevXuxatUq9O7dW+uJ6dWrFzIzM7FixQqcPn0aW7ZswY4dO9CvXz+9PxIRkcfJrHOQly7av7Z5LdQxA6Fu3+rwX9cy9TDUWf+wnBRUmhVTaQKp/OWH8ku39Ky+ciGh2qEyYpJPTEilwGD4YoD2DBs2DEIIzJ07F6WlpdpigGUURcHUqVOxePFiTJ8+HaGhoejRowcGDx6s5YmNjcXUqVOxbNkybNq0CQ0aNMCYMWO4Rg4R+R3zc+OAK0NIytNzINpcYzef/OBtyA/etuQbOw3iytYK5qdHAhfOV/8gO8GRCLI/xG+VJyISousdQGkJhDOLCxLVkloJcl544QWr85CQEIwaNcoqsKmsUaNG+Ne//lVluddddx1effVVPapIROR1ZGmJJcioMEdGnfO0U2vJqPNfhrJwPYRiqj7AqdDzIouL3KqrMmKSW/cRGckjPTlERFQ19ef/Qi6eC9HtTvsZhKh+cm9+HlA32iZZeeFd+/mlhJr8pmsVJfJi3KCTiMgLycVzLT9//MbudXHLn6svY/2HkPb2hmrSvFLClZ6cy4XAnu2uVdRdMfXLHz/godp5JgUc9uQQEfkgbc2bqvL88BXEdZ3KE27oCtMTU+3kvBLknEy1c00nN3S1CqCUVz+wrMCcdRa4qolxz6WAxiCHiMiLyAO7gArbzziUctB+enhk+aviAGTq79qxMnyi3VuEsKyRU1uU+Wstb2CZTEBcQi0+mQINgxwiIi8hVTPUd2bUqAzl0SehbvgPkJ5mKfObDdo1ERZu/yYHr3yL4TpOJq44DObEG1tEemCQQ0TkLezMn7HRMhGywmJ9ZZQ3VwDn/wfRoi3E7u2QV4KcmhCNrqpxGRpF0T4f19Gh2sKJx0RE3qK4uPo8qgp1yjDrtNbtICKjIFq0BQDI7Vtdeqz67RcOrugYjFx3g35lETmJQQ4RkZeoOLRUmTL+WctB5cnBiddDGfdMtWWL+4c7vuhofk+U7evnRL6EQQ4RkZeQP/+fbWK9hhBDH4c88Ivde0xPvQxReS2cG261ySf+1Mfxg0Pr2OZ/eCyEnpOCVbN+ZRE5iUEOEZG3yDxjfd4oDqZXl0D5cz+g1Ik3rq5Q7viLTZrDSccAlInPW5/PXgylRxVBkRtEHcfPJzIKgxwiIi9ltUhea/v7Vdm9L/F6iCGjgbJF/0xVv2MiEq+zPm8Q6/SznK7Tg48CLdpCjHxS97KJHOHbVURE3iIoCCgt1U7F9Z3Lj6/tZLOWjfLSew6LUu74C3DHXyBPHgP0fEvKTaJ+Q5iemevpalCAYZBDROQtKgQ4ypsrICIiy6/Zee1axFW/UrBo3lqXqhH5Ig5XERHVApl3EeqH/4Y89nv1mQGIyCjrBKV2/roWg4ZVn4nIRzDIISKqBXLdcsj/2wLzK0+5WYDBGy80bgYAED3uNvY5RLWIw1VERLVAnjqhHauFBVXmFTf3sE104e0qdyjPvgWYzRChoYY+h6g2McghIqoNlwu1Q2lnZWNpLl9HRjw40vb+mAZWp2LIaP3qBkAEBVkmPhP5EQ5XERHVhvCI8mNzqc1lufnj8pNS24XzRLD1ppbiuk66VY3IXzHIISIymJQSqDDhWNoZepIbVpafRNStvtCrqn+ziijQMcghIjLavp1Wp5fWrbDNU/F18GrepFKmzOJO3kRO4AAsEZHB5J4dVud5n62GyM8HElpAua0XAED07Ae56n1LBgdBjrJwPZB3CSIqxsDaEvkPBjlERAaTO761Tfv2c8vP+KaQR3+z2sBSmEx2yxGKCWCAQ+Q0BjlERB6kznnaOqFOmGcqQuSHOCeHiMhAssT2dfEqVXjVnIhqhkEOEZGB1OlPeLoKRAGLQQ4Rkc7kgd1Qv99kWeDvQpanq0MUsDgnh4hIR1JKqO+8aDn+zwKX7xeP/VPvKhEFLAY5REQ6kWYz1IVzHF5X3loJ7NkOdfk8y/n7G4Ds80C9hlBn/xMiLgFKl+61VV0iv8cgh4hIJ3Lp28CvPzm8LiIiIf7UG41uuBnnzRIQAqjfCABg+tdrtVVNooDBOTlERDqRP33v8Joya6F2HNL2GojoerVQI6LAxiCHiEgHsuhylddFbHwt1YSIyjDIISJyg8y/BHlwN8wzJkFeLoA6/kGHeZU37exVRUSG45wcIiIXyRNHoc76h3auThhik0eZ/wmQkQY0jIWIjKrN6hHRFQxyiIhcVDHAsUfc+zBEcDDQ6upaqhER2cPhKiIiF0gpq80j+gyqhZoQUXUY5BARueJSTrVZHO0iTkS1i0EOEZEL5PebrRPCIjxTESKqFoMcIiJXRMVoh2L4RCgvvOOxqhBR1RjkEBG54tgR7VDpdicQGmZ1WfR1/Co5EdUuBjlERC6Qe360TggNLT++vjOUgQ/XboWIyCG+Qk5E5IriYqtTERQMtL8RuJgDZcJ0D1WKiOxhkENEXkGqZsiVC4GrmkC5a4Cnq2OXVFXtWNx6u3asTHjWkiZErdeJiBxjkENEXkHu3g753y8tx3fe45UBg/r4veUnMeUbbHpjXYmIc3KIyEtYrS1TzWaXRpO7t8P80t8h/3emPK3SIoCi3+DarhYRuYg9OUTkJSr0hlwuBOqEOc5qEFlUBCgC6oLZAAB1+hjtWtmQVBkRWqdW60ZErmOQQ0ReQRaX996oTw2HadHG2n2+qkId/4DD6+q7L2nHyvsbaqNKRFRDDHKIyBDm1/4FhNeFady0avOqn62C/O4LqzSpqhCK8yPqUsqazY0pLnI6K+fgEPkGBjlE5DSpmoF9vwDtb4QIsv3rQ17MgfrsE0BBfnnayVSI5m1s80oJnEyFTD8GuXGl7cMK84GIulXWR132LuS2ryFG/QNy8VwAcL8HKOdC9XnqRsP0xofulU9EtY5BDhE5TX18oHasvL/BpkdD/ccjtvfMnAwAEINHQbnzHi1d/vw9ZPKbjh9WVFRlkCMzjkNu+9pyfCXAAQDz6Hus6ialhPqY5ZV0Zf4nkJ+vhrj1zxBxTazr+ewT5Z9tzNNAUAhEUhfI/EvAyVSIazs5risReSUGOUTkFvWxARA9+kAMeBiiblS1+eXqxZC39wNOHgOata46wAEsPTlo6Pj5MyY5ftbP/wXaXAO5+RPI//uy/J6xgyzXN62BadFGyP+dsUwubnut1f2ic7fy44i6AAMcIp/EIIeI3Cb/+6W2to0y/5Pq87//OuTuH6vNBwDqS3+HacF6u9fMk/9W5b0iLBzqv0ZXmcf8j0eAizmWk6O/ld873HHwRES+hevkEFG15IUsmEffU2Uedf6s6supKsBpcw1E7/vKz81mqNu3lt97uRCyIN9Sj0u5VreKR8ZDeWVReV3mzay2LlqAU4nS7Y7q7yUin8CeHCKqkkw9DHXO09VnPLgHMAUB5lIoM/4NefQQ5IfznXpGxTk05i3ryp/9wdswf70BiG0M7Nlu/973PrHsH1UN8ZchkJs+Bsxmx3mGPOZUfYnINzDIIaIqORXglDGXWn5GRELc1tu5IEcoVb+SfeqE5T97t/bsW22AU/FtK/mXv2qTkAFAmf4G0LQlkJ8P1KkDOBEsEZHv4HAVEenPFAQhBJQZ84GY+lVmrbySsCvEX63n3SivLQVaJjrOLwRMizZCWfgpTIs2QjRvA6GYIOpGQQSHcP0bIj+je0/O+vXrsXPnTpw+fRohISFITEzEww8/jMaNG2t5iouLsXz5cmzfvh0lJSVISkrCqFGjEBMTo+U5f/48Fi1ahEOHDqFOnTro0aMHhg4dClOF/W0OHTqE5cuXIyMjAw0aNMCgQYPQs2dPvT8SUcAp223b/NRwm2vir49BxMZDXb0YyugpUF960rYA1bLPk4hPgOm1pTC/Ng1IOWiTTZmTDFG/kXX59wy1v25O2T0z5kP+ugPi5p4QisnqmoipD9O01yGPp0Ddsg7K6KfsluHKIoNE5Lt0D3J+++039O7dG61bt4bZbMZHH32EmTNn4o033kCdOpa9XpYtW4Y9e/Zg8uTJCA8PR3JyMubOnYuXXrIsm66qKl555RXExMRg5syZyM7Oxrx582AymTB06FAAQGZmJmbPno277roLEyZMwMGDB7FgwQLExMSgY8eOen8sooBROvNJnDqR6vC6cnt/AIDp+s4O81R+pVyZ+Jxlzk7idVArvBlVOcABAOUvQyBbt4P65nM218qGnkS84+0XAEC0TIRpzNQq8xCR/9P9nzPPPPMMevbsiaZNm6JFixYYN24czp8/j7S0NABAQUEBvv32WwwbNgzXX389WrVqhbFjx+LIkSNISUkBAOzbtw+nTp3ChAkT0KJFC3Tq1AmDBw/Gli1bUFpqGfP/6quvEBsbi0ceeQQJCQno06cPbrnlFnzxxRcO60ZETqgiwLFHmTbXOqFDF5s8IrQOROeuEHWjobz8viWtzyCHZYprO0L0fRDiz32BG7oCcU2gzFniUr2IiAyfeFxQUAAAiIyMBACkpaXBbDajffv2Wp4mTZqgYcOGSElJQWJiIlJSUtCsWTOr4auOHTti8eLFyMjIQMuWLXH06FGrMgAgKSkJS5cudViXkpISlJSUaOdCCISFhWnHeikri+P7xmD7eo4ydIxNu4tWiVC1EwWmkX+v8s9GxMZDWfxZtc8y3Vf1Wji+it9fY7F9jedLbWxokKOqKpYuXYqrr74azZo1AwDk5OQgKCgIERERVnmjo6ORk5Oj5akY4JRdL7tW9rMsrWKewsJCFBcXIyQkxKY+69evx9q1a7Xzli1bYs6cOWjUyLbLXA9xcXGGlEsWbF9jZFRx7aq7+iEoNt72whe7AADSbIYwmWyvkw1+f43F9jWeL7SxoUFOcnIyMjIyMGPGDCMf47SBAweif//+2nlZFJqVlaUNg+lBCIG4uDicO3fOsgkh6Yrta5yyCceOZJaoEGfP1lJt/BO/v8Zi+xrPG9o4KCjIqQ4Kw4Kc5ORk7NmzBy+++CIaNGigpcfExKC0tBT5+flWvTm5ubla701MTAxSU63nBeTm5mrXyn6WpVXMExYWZrcXBwCCg4MRHGx/HQwj/qCklPyfzEDe3L7y8D7I3Gwot/T0dFVcIiusAiz+3BdokQjR/kbL+jd1owEhvLbNfY03f3/9AdvXeL7QxrpPPJZSIjk5GTt37sRzzz2H2NhYq+utWrWCyWTCgQMHtLQzZ87g/PnzSEy0rG+RmJiI9PR0qyBm//79CAsLQ0JCAgCgbdu2VmWU5Skrg8iT1DeehUx+A+YqNpH0RuqUYdqxcsc9ULrebllDJqY+h6GIyOfoHuQkJyfjhx9+wKRJkxAWFoacnBzk5OSguLgYABAeHo7bb78dy5cvx8GDB5GWlob58+cjMTFRC1CSkpKQkJCAefPm4cSJE9i7dy9WrVqF3r17az0xvXr1QmZmJlasWIHTp09jy5Yt2LFjB/r166f3RyI/JqWEefQ9lv8WzLafp6QE8niKe/9iyThup7ximN98HvKI7box3kTENfF0FYiIakT34aqvvvoKAPDCCy9YpY8dO1ZbqG/YsGEQQmDu3LkoLS3VFgMsoygKpk6disWLF2P69OkIDQ1Fjx49MHjwYC1PbGwspk6dimXLlmHTpk1o0KABxowZwzVyyDUXzpcf794OWVgAERYOmZ4G9aW/Q9zcA/Ln/1rdosyYDxGf4LBIebnA6rxsY0vx2D8h339VS1d/+9VqywFPk0VFnq4CEZGuhPT2AbVakJWVZfVqeU0JIRAfH4+zZ896xXilPJkKdeZkiL+Ng/Kn3p6uTo3p2b5lbVORadHGanfcrhycSCmBo4egvjcbyLvo9POVVz+AqNfAMuG3MB8iom55mZcLoE4Y4vCZepGqGerjA63Swu/oh+K/jvGK76+/8ba/H/wN29d43tDGwcHBnp14TN5B/WYD5OpkAID88N8wf/hvKM++CdGsNWTeRSA8MqCXuJc7f3DvvqLLEKF1tHN1/svA3p9dLkf95wjbxGatoTw1C+qMv1s/U1Wd+rNSv/sCct1yKK8thagTVmVe+Ucm1KmjbNIbTH4RZ/kWFRH5OAY5fsr80t+B9DS71yrvNeRNQya1QZ7NAOo1BM6dgvxqvfXFFm2d+peJOv5BiP5DILreDtEozrUAp240cCnX8fX0Y1Y9ONozH78XyvxPIIKDLT0/p09a9nDqPQgiNBTqqkWQW8sX2VMnDLa6X3S7A+LhcRBBlv/tZc4fdgMc5cFHnf8sRERejEGOn5EpB6G+Ns21e0qKoY69H4Bl3ojSpbsRVfMK6s7/g1z0uuMMJ45C/rDF7iXlyRehvvm8di4/XwX5+SqI3vdV+Uxl7jIgMhpy/YcQLdpCdO5a7XCYw/qPtd0KQX62yql75Y9bIX/cWnVd//0xlAo9VEREvoxBjh+RJSUuBzgAtAAHAOT7r0J2vtVmd2d/IFXVYYBTcedr+eF8m+vKrAUQsY3tl7tlXXk5N3aHGDYeok44ZGkJRFD5ukxi0DB7t1tLaAGcOlF+Hl0PyM2u/r4aEv0ehHLvw4Y/h4ioNjHI8SPqM487vCYeGQ/RvjPUp+zMAalczuMD/W4Iq9qek/oN7SYr8z6GCA11+jnisafK93UJsr/wJAAoU2ZB7vwB4v7hUCdWmFz8/Ds2eeWFLKhPOz+EpMxfC7lpraWX6b5HIOITINNSIDevtZ9/7nKIqBinyyci8hUMcvxJ9nmrU2XWQi3wUW7rZfk5dznUfzxSbVFSNftNb47M/qPK68r7GyA3fWz3WuUAR/R7EPKLNfbz/qm30xvWiavbQ1xt2WBWefYty9waB7tyi/qNYFq0EVJKy1tQUoXy9kqI8EjIPTuAiEggLBzqS09CeeFdiOAQiAFDgQFDy8voeAtwX/V/7kRE/oRBjp9SXlkE0fAqmx4ZERUDcecAyG82VHm/Ou4BmN5bZ5Muiy4DigkICvKJHWjlxWyo/3nP4XVl4acQQkA9vM/22rurbdJEv8HAVU0gru1otTowAIiHx7pVR9GsFUSzVtXnEwKm9z+1TrvhVu3Y33rfiIhqikGOHxK9B0I0vMrhdWXwo5D3/Q0iOARSSi1YkaoK9fF7LZnsbFhq/scjQIW9jZS5yyCi6ulZdV3ItCNQ33gOKCqsMp/oPVB7JVs0vAryiPU2IfZevxbBwRC3/hmApQcIh/YACS2B6Ho+EfQREQWSwF0gxUtJPXZDd2JhQxFs2cS04i/mymuwlM1jKdv2oGKAAwByzZIaVlR/sqQE6itPVRvgKG+vhHJ/+fwk8bdx1tff+LDaZwkhIK7vbNnXiQEOEZHXYZDjReS+nVDHDoL64zfV55XSZvuAMqLrHW7XQRln/XZWVRN2K2934Cny3GnI4itbElzIqja/MuFZiPBIqzRhMkH89TGgeRvLKsR1o42oKhER1SIOV3kRdd5MAIBc+g7Q7U6H+Sou9Ce63gHRf7BlD6bQOkDRZctPd0VGuZTd2VV4jSDPpMP8nHUPjOhvvQCe8q/XgKsaA+GRkCvmQ9zQFeK6TnbLU27vD9ze37D6EhFR7WKQ42NkaYnVSsZy+1bI7ZUWeKtB0CHaXFt1hqSboIyeAnX8g5bzostAWLjbz3OX+Y8smwAHAOTn5ZOFldmLIRrEaueVh6SIiMi/McjxMfKXbdVniqxbfZ4qKC+8C/nfLyG/+6I8rcLO21JVtXR14hAo766udo8kPZWOuQ9nSp2Yd1QhwCEiosDDIMdLVN4vqfJquVr6kjerLavyfBNXiSbNIYY+Dgy1v7hg5eEpdcJgKP/+GCLE+UXzqiMvXYQ62bICrzJtLkTLtpb0oiLAiQBHeX2ZbnUhIiLfxInHXsD8zBiojw2wSlOfGASZcRwy47hLZSnTqtiXyUDqm8/VuAx5eB/UdcshzWYtwAEA9eV/wDz6HsjT6cDJozb3mRZtBJJuKk9o3gYi2vtebSciotrFnhwPk6oKZJ6xe02dMclycG0nmJ580TZD/UbAhSzvWJY/9bBT2eTlQiA3G+Iq232g1DeeteRxsP2A+sJ4h+Uq454Bss4BDWIhTP6xUjMREdUMgxxPc2LoBb/9ahnOqpTXNCfZoEpVT/ypN+T/We/WXXFhwTLq+hVAaQmUB0ZA/pEFdWr5HkxiyGgod/ylRvVQpsyylCUEEBtfo7KIiMi/MMjxtJJip7Kp/3gEosONBlfGeWLAQzZBDs6dBq5MTgYAmZsNucmyz5Psda9VgAMActUiyNbtAFOQzb5bzgha/JnNXCYiIqIynJPjaX+UL15X5d5Dl3Ihfyx/VVyZ8KyRtaqWiIqB8tZ/rHfvNldarbnCfKKyNYAqU2f9A+qMSVDffcn+c+4cYDediIioOgxyPEiqKtRl71ilKU/Prv7GG26F6NDFoFo5T0TUhfLQExUSKr119cNX5ScnbCcMOyx32AQo89fCtGgjlMGPQvR90CZP/X/McLm+REQUWBjk1BJ57hTM78yAuuE/AADzW89bNsOssLAfYFmMT3n8nxD9HoTyziq7ZZme+JfR1XWa6NAFKJvoa640v2jPdrv3KNOrfg1e6X6XtrcWAIh7/mp9/W/jEHF7X9crS0REAYVzcmqJ+uxYAIA8sAvmCqvyalq30w7Fjd0hbuwOwLLTdcXXy5X5nxhbUXfUawic/x9QUgJ5eB9k+jGIXgMdZhfNWzu+Zmd4SphMVkN53AyTiIicwSCnFlS1yWUZ5ek5dtOFEFXP1fEGQZavkTr7n1qSaNIconM3yN0/2r1Fmf4G1JmTy8+fnAGZkQZxF+fgEBGRPjhcZTB1+bxq8yhPz/bt3gmTbawscy5AFhU6vEU0bwOl4ivwLdpC6X0fhMI1boiISB/syTGQLC6CrDj5tklz4PRJ7VR59QOIeg08UDOdVfhMmuAQ4OAeAIC4fwTkt59bFi6cWL4ysqjfCMq/PwZUtVb3viIiosDAIMcApaP+ggw76aYX3oUsLgIyz1p+sftDgOOAXDy3/CQyyuHChXrud0VERFQRg5xaoox7BsCVX+oJLTxbmVomrknydBWIiCgAcU6OERrZbi8gOt7sgYp4iZj6nq4BEREFIAY5Bgh65X2rc2XBeg/VxDsIhV8zIiKqffztY5D4xZ9qx4G8K7Yye7Gnq0BERAGKc3IMEhSfEJgbSAoFkCoAy8KFIjjYwxUiIqJAxZ4c0teVAAcAAxwiIvIoBjmkK/GXIZ6uAhEREQAOV5HORN8HgPhmEFdf7+mqEBFRgGOQQ7oSQcEQXbp7uhpEREQcriIiIiL/xCCHai4swtM1ICIissEgh2pMdLvD01UgIiKywSCHaq5VO0/XgIiIyAYnHlONiRu7AUUTIFq09XRViIiINAxyqMaEEBDd7/J0NYiIiKxwuIqIiIj8EoMcIiIi8ksMcoiIiMgvMcghIiIiv8Qgh4iIiPwSgxwiIiLySwxyiIiIyC8xyCEiIiK/xCCHiIiI/BKDHCIiIvJLDHKIiIjILzHIISIiIr/EIIeIiIj8EnchBxAUZEwzGFUuWbB9jcX2NRbb11hsX+N5so2dfbaQUkqD60JERERU6zhcZYDCwkI8/fTTKCws9HRV/BLb11hsX2OxfY3F9jWeL7UxgxwDSClx/PhxsJPMGGxfY7F9jcX2NRbb13i+1MYMcoiIiMgvMcghIiIiv8QgxwDBwcG4//77ERwc7Omq+CW2r7HYvsZi+xqL7Ws8X2pjvl1FREREfok9OUREROSXGOQQERGRX2KQQ0RERH6JQQ4RERH5JW7uYYAvv/wSn332GXJyctC8eXOMHDkSbdq08XS1POq3337Dxo0bcfz4cWRnZ2PKlCm46aabtOtSSqxZswZbt25Ffn4+2rVrh1GjRiE+Pl7Lk5eXhyVLlmD37t0QQuDmm2/GiBEjUKdOHS3PyZMnkZycjGPHjiEqKgp9+vTBgAEDrOqyY8cOrF69GllZWYiLi8NDDz2EG264wfhGMMj69euxc+dOnD59GiEhIUhMTMTDDz+Mxo0ba3mKi4uxfPlybN++HSUlJUhKSsKoUaMQExOj5Tl//jwWLVqEQ4cOoU6dOujRoweGDh0Kk8mk5Tl06BCWL1+OjIwMNGjQAIMGDULPnj2t6uOP3/+vvvoKX331FbKysgAACQkJuP/++9GpUycAbF89ffrpp1i5ciX69u2L4cOHA2D71tSaNWuwdu1aq7TGjRvjrbfeAuDn7StJVz/++KP861//Kr/99luZkZEhFyxYIIcPHy5zcnI8XTWP2rNnj/zoo4/kzz//LB944AH5888/W11fv369HDZsmNy5c6c8ceKEnDNnjhw3bpwsKirS8syaNUtOmTJFpqSkyMOHD8sJEybIt956S7uen58vR40aJd9++22Znp4ut23bJh966CH59ddfa3l+//13OXjwYLlhwwaZkZEhP/roIzlkyBB58uRJ4xvBIDNnzpTfffedTE9Pl8ePH5cvv/yyfOKJJ2RhYaGW5/3335djxoyRBw4ckMeOHZPTpk2T06dP166bzWY5efJkOWPGDHn8+HG5Z88eOXLkSPmf//xHy/O///1PPvzww3LZsmUyIyNDbt68WQ4ePFj++uuvWh5//f7/8ssvcvfu3fLMmTPy9OnTcuXKlXLIkCEyPT1dSsn21cvRo0fl2LFj5ZQpU+QHH3ygpbN9a2b16tVy8uTJMjs7W/svNzdXu+7P7csgR2f/+te/5OLFi7Vzs9ksH3vsMbl+/XrPVcrLVA5yVFWVo0ePlhs2bNDS8vPz5dChQ+W2bduklFJmZGTIBx54QKampmp5fv31V/nggw/KP/74Q0op5ZYtW+Tw4cNlSUmJlmfFihVy0qRJ2vkbb7whX3nlFav6TJs2TS5cuFDXz+hJubm58oEHHpCHDh2SUlracsiQIXLHjh1anlOnTskHHnhAHjlyREppCUIffPBBmZ2dreXZsmWLfOSRR7T2/PDDD+XkyZOtnvXmm2/KmTNnaueB9P0fPny43Lp1K9tXJ4WFhXLixIly37598vnnn9eCHLZvza1evVpOmTLF7jV/b1/OydFRaWkp0tLS0L59ey1NURS0b98eKSkpHqyZd8vMzEROTg46dOigpYWHh6NNmzZau6WkpCAiIgKtW7fW8rRv3x5CCKSmpmp5rrnmGgQFlY/CJiUl4cyZM8jLy9PyVPzzKctz9OhRwz5fbSsoKAAAREZGAgDS0tJgNputPneTJk3QsGFDq/Zt1qyZVfd0x44dUVhYiIyMDADA0aNH7bZdWRmB8v1XVRU//vgjioqKkJiYyPbVyeLFi9GpUyervwcAfn/1cu7cOTz++OMYP3483nnnHZw/fx6A/7cv5+To6OLFi1BV1eqLAAAxMTE4c+aMZyrlA3JycgAA0dHRVunR0dHatZycHERFRVldN5lMiIyMtMoTGxtrlafszyInJ0fLW9VzfJ2qqli6dCmuvvpqNGvWDIDlswcFBSEiIsIqb+X2rfy9LWuninnstV1hYSGKi4uRl5fn19//9PR0PPPMMygpKUGdOnUwZcoUJCQk4MSJE2zfGvrxxx9x/PhxvPLKKzbX+P2tubZt22Ls2LFo3LgxsrOzsXbtWjz33HOYO3eu37cvgxwiP5KcnIyMjAzMmDHD01XxO40bN8Zrr72GgoIC/PTTT/j3v/+NF1980dPV8nnnz5/H0qVLMX36dISEhHi6On6pbII8ADRv3lwLenbs2OH3bc4gR0dRUVFQFMWmV8BeFEzlytomNzcX9erV09Jzc3PRokULLc/Fixet7jObzcjLy9Puj4mJsdv2FZ8RExOD3Nxcqzy5ubl+8eeTnJyMPXv24MUXX0SDBg209JiYGJSWliI/P9/qX2sVP3dMTIw27Ffxetm1sp/22i4sLAwhISF+//0PCgpCXFwcAKBVq1Y4duwYNm3ahK5du7J9ayAtLQ25ubl4+umntTRVVXH48GF8+eWXeOaZZ9i+OouIiEDjxo1x7tw5dOjQwa/bl3NydBQUFIRWrVrh4MGDWpqqqjh48CASExM9WDPvFhsbi5iYGBw4cEBLKygoQGpqqtZuiYmJyM/PR1pampbn4MGDkFJqrx8mJibi8OHDKC0t1fLs378fjRs31uanJCYmWj2nLE/btm0N+3xGk1IiOTkZO3fuxHPPPWczZNeqVSuYTCarz33mzBmcP3/eqn3T09Ot/pLav38/wsLCkJCQAMDS5W2v7crKCLTvv6qqKCkpYfvWUPv27fH666/j1Vdf1f5r3bo1unfvrh2zffV1+fJlnDt3DjExMX7//WWQo7P+/ftj69at+P7773Hq1CksXrwYRUVFNmsFBJrLly/jxIkTOHHiBADLZOMTJ07g/PnzEEKgb9++WLduHXbt2oX09HTMmzcP9erVQ5cuXQBY1iXp2LEjFi5ciNTUVPz+++9YsmQJunbtivr16wMAunfvjqCgICxYsAAZGRnYvn07Nm/ejP79+2v16Nu3L/bt24fPPvsMp0+fxpo1a3Ds2DH06dOn1ttEL8nJyfjhhx8wadIkhIWFIScnBzk5OSguLgZgmcR9++23Y/ny5Th48CDS0tIwf/58JCYman+5JCUlISEhAfPmzcOJEyewd+9erFq1Cr1799Z2Gu7VqxcyMzOxYsUKnD59Glu2bMGOHTvQr18/rS7++v1fuXIlfvvtN2RmZiI9PV07v+2229i+NRQWFoZmzZpZ/RcaGoq6deuiWbNmbF8dLF++XPv+HjlyBK+99hoURUH37t39vn25C7kBvvzyS2zcuBE5OTlo0aIFRowY4dM9BXo4dOiQ3fkLPXr0wLhx47TFAL/55hsUFBSgXbt2ePTRR60WtMvLy0NycrLVYoAjR450uBhg3bp10adPH9x7771Wz9yxYwdWrVqFrKwsxMfH+/xigA8++KDd9LFjx2p/eZQt9vXjjz+itLTU7mJfWVlZWLx4MQ4dOoTQ0FD06NEDDz30kM1iX8uWLcOpU6eqXOzL377/7733Hg4ePIjs7GyEh4ejefPmGDBggPYmENtXXy+88AJatGhhsxgg29c9b731Fg4fPoxLly4hKioK7dq1w5AhQ7ThV39uXwY5RERE5Jc4XEVERER+iUEOERER+SUGOUREROSXGOQQERGRX2KQQ0RERH6JQQ4RERH5JQY5RERE5JcY5BAREZFfYpBDREREfolBDhEREfklBjlERETklxjkEBERkV/6f6iWSFsCVXXnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "orDict = OrderedDict()\n",
    "for k in env.action_scheme.portfolio.performance.keys():\n",
    "    orDict[k] = env.action_scheme.portfolio.performance[k][\"net_worth\"]\n",
    "pd.DataFrame().from_dict(orDict, orient='index').plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20, {abb}'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"{aa}, {{abb}}\"\n",
    "a.format(aa=\"20\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "import ray\n",
    "# ray.init()\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"TradingEnv\", createEnv)\n",
    "from ray.tune.registry import register_env\n",
    "# register_env(\"TradingEnv\", createEnv)\n",
    "config = DEFAULT_CONFIG.copy()\n",
    "config[\"env_config\"][\"data\"]=norm_data\n",
    "config[\"env_config\"][\"window_size\"]=tune.grid_search([10, 15, 30])\n",
    "config[\"env\"] = \"TradingEnv\"\n",
    "\n",
    "# agent = PPOTrainer(config, 'TradingEnv')\n",
    "config['model'][\"use_lstm\"] = True\n",
    "config['num_workers'] = 1\n",
    "\n",
    "config[\"log_level\"] = \"DEBUG\"\n",
    "\n",
    "\n",
    "config[\"model\"][\"lstm_cell_size\"] = tune.grid_search([16, 32, 62, 128, 256])\n",
    "# config[\"model\"][\"max_seq_len\"] = tune.grid_search([5, 10, 16, 32, 62])\n",
    "config[\"model\"][\"lstm_use_prev_action\"] = tune.grid_search([True, False])\n",
    "config[\"model\"][\"lstm_use_prev_reward\"] = tune.grid_search([True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-06 21:43:27,019\tINFO tune.py:661 -- Initializing Ray automatically.For cluster usage or custom Ray initialization, call `ray.init(...)` before `tune.run`.\n",
      "2021-07-06 21:43:29,444\tINFO services.py:1274 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:43:54,076\tINFO trainer.py:671 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:44:45,480\tDEBUG worker_set.py:303 -- Creating TF session {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:44:45,481\tDEBUG rollout_worker.py:1160 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:44:45,524\tDEBUG catalog.py:710 -- Created preprocessor <ray.rllib.models.preprocessors.NoPreprocessor object at 0x237ee4490>: Box(-inf, inf, (10, 60), float32) -> (10, 60)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m Model: \"model_1\"\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m ==================================================================================================\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m seq_in (InputLayer)             [(None,)]            0                                            \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m tf_op_layer_default_policy/Sequ [()]                 0           seq_in[0][0]                     \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m tf_op_layer_default_policy/Sequ [()]                 0           tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m tf_op_layer_default_policy/Sequ [(None, 1)]          0           seq_in[0][0]                     \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m tf_op_layer_default_policy/Sequ [(None,)]            0           tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m tf_op_layer_default_policy/Sequ [(None, 1)]          0           tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m inputs (InputLayer)             [(None, None, 298)]  0                                            \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m h (InputLayer)                  [(None, 16)]         0                                            \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m c (InputLayer)                  [(None, 16)]         0                                            \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m tf_op_layer_default_policy/Sequ [(None, None)]       0           tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                                                  tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m lstm (LSTM)                     [(None, None, 16), ( 20160       inputs[0][0]                     \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                                                  h[0][0]                          \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                                                  c[0][0]                          \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                                                  tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m logits (Dense)                  (None, None, 41)     697         lstm[0][0]                       \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m values (Dense)                  (None, None, 1)      17          lstm[0][0]                       \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m ==================================================================================================\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m Total params: 20,874\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m Trainable params: 20,874\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m Non-trainable params: 0\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m __________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:44:46,598\tINFO tf_policy.py:154 -- TFPolicy (worker=1) running on CPU.\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:44:46,733\tINFO dynamic_tf_policy.py:472 -- Testing `compute_actions` w/ dummy batch.\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:44:46,736\tINFO tf_run_builder.py:87 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:44:47,063\tINFO dynamic_tf_policy.py:481 -- Adding extra-action-fetch `action_prob` to view-reqs.\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:44:47,064\tINFO dynamic_tf_policy.py:481 -- Adding extra-action-fetch `action_logp` to view-reqs.\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:44:47,065\tINFO dynamic_tf_policy.py:481 -- Adding extra-action-fetch `action_dist_inputs` to view-reqs.\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:44:47,066\tINFO dynamic_tf_policy.py:481 -- Adding extra-action-fetch `vf_preds` to view-reqs.\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:44:47,066\tINFO dynamic_tf_policy.py:488 -- Testing `postprocess_trajectory` w/ dummy batch.\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:44:47,357\tDEBUG dynamic_tf_policy.py:514 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'action_dist_inputs': <tf.Tensor 'default_policy/action_dist_inputs:0' shape=(?, 41) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'action_logp': <tf.Tensor 'default_policy/action_logp:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'actions': <tf.Tensor 'default_policy/action:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'agent_index': <tf.Tensor 'default_policy/agent_index:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'eps_id': <tf.Tensor 'default_policy/eps_id:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 10, 60) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'obs': <tf.Tensor 'default_policy/obs:0' shape=(?, 10, 60) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/prev_actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'seq_lens': <tf.Tensor 'default_policy/seq_lens:0' shape=(?,) dtype=int32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_in_0': <tf.Tensor 'default_policy/Placeholder:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_in_1': <tf.Tensor 'default_policy/Placeholder_1:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   't': <tf.Tensor 'default_policy/t:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'unroll_id': <tf.Tensor 'default_policy/unroll_id:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:44:48,780\tDEBUG tf_policy.py:325 -- These tensors were used in the loss_fn:\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'action_dist_inputs': <tf.Tensor 'default_policy/action_dist_inputs:0' shape=(?, 41) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'action_logp': <tf.Tensor 'default_policy/action_logp:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'actions': <tf.Tensor 'default_policy/action:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 10, 60) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'obs': <tf.Tensor 'default_policy/obs:0' shape=(?, 10, 60) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/prev_actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'seq_lens': <tf.Tensor 'default_policy/seq_lens:0' shape=(?,) dtype=int32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_in_0': <tf.Tensor 'default_policy/Placeholder:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_in_1': <tf.Tensor 'default_policy/Placeholder_1:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:44:49,303\tDEBUG rollout_worker.py:553 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:44:49,304\tDEBUG rollout_worker.py:700 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x237f4f610> (<TradingEnv instance>), policies {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x2365970d0>}\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:44:49,641\tDEBUG worker_set.py:303 -- Creating TF session {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:44:49,662\tDEBUG rollout_worker.py:1160 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:44:49,788\tDEBUG catalog.py:710 -- Created preprocessor <ray.rllib.models.preprocessors.NoPreprocessor object at 0x23ed09510>: Box(-inf, inf, (10, 60), float32) -> (10, 60)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m Model: \"model_1\"\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m ==================================================================================================\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m seq_in (InputLayer)             [(None,)]            0                                            \n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m tf_op_layer_default_policy/Sequ [()]                 0           seq_in[0][0]                     \n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m tf_op_layer_default_policy/Sequ [()]                 0           tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m tf_op_layer_default_policy/Sequ [(None, 1)]          0           seq_in[0][0]                     \n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m tf_op_layer_default_policy/Sequ [(None,)]            0           tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m tf_op_layer_default_policy/Sequ [(None, 1)]          0           tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m inputs (InputLayer)             [(None, None, 298)]  0                                            \n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m h (InputLayer)                  [(None, 16)]         0                                            \n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m c (InputLayer)                  [(None, 16)]         0                                            \n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m tf_op_layer_default_policy/Sequ [(None, None)]       0           tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                                                                  tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m lstm (LSTM)                     [(None, None, 16), ( 20160       inputs[0][0]                     \n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                                                                  h[0][0]                          \n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                                                                  c[0][0]                          \n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                                                                  tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m logits (Dense)                  (None, None, 41)     697         lstm[0][0]                       \n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m values (Dense)                  (None, None, 1)      17          lstm[0][0]                       \n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m ==================================================================================================\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m Total params: 20,874\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m Trainable params: 20,874\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m Non-trainable params: 0\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m __________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:44:50,958\tINFO tf_policy.py:154 -- TFPolicy (worker=local) running on CPU.\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:44:51,182\tINFO dynamic_tf_policy.py:472 -- Testing `compute_actions` w/ dummy batch.\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:44:51,182\tINFO tf_run_builder.py:87 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:44:51,392\tINFO dynamic_tf_policy.py:481 -- Adding extra-action-fetch `action_prob` to view-reqs.\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:44:51,393\tINFO dynamic_tf_policy.py:481 -- Adding extra-action-fetch `action_logp` to view-reqs.\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:44:51,393\tINFO dynamic_tf_policy.py:481 -- Adding extra-action-fetch `action_dist_inputs` to view-reqs.\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:44:51,394\tINFO dynamic_tf_policy.py:481 -- Adding extra-action-fetch `vf_preds` to view-reqs.\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:44:51,394\tINFO dynamic_tf_policy.py:488 -- Testing `postprocess_trajectory` w/ dummy batch.\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:44:51,697\tDEBUG dynamic_tf_policy.py:514 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m { 'action_dist_inputs': <tf.Tensor 'default_policy/action_dist_inputs:0' shape=(?, 41) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'action_logp': <tf.Tensor 'default_policy/action_logp:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'actions': <tf.Tensor 'default_policy/action:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'agent_index': <tf.Tensor 'default_policy/agent_index:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'eps_id': <tf.Tensor 'default_policy/eps_id:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 10, 60) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'obs': <tf.Tensor 'default_policy/obs:0' shape=(?, 10, 60) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/prev_actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'seq_lens': <tf.Tensor 'default_policy/seq_lens:0' shape=(?,) dtype=int32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'state_in_0': <tf.Tensor 'default_policy/Placeholder:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'state_in_1': <tf.Tensor 'default_policy/Placeholder_1:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   't': <tf.Tensor 'default_policy/t:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'unroll_id': <tf.Tensor 'default_policy/unroll_id:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:44:53,164\tDEBUG tf_policy.py:325 -- These tensors were used in the loss_fn:\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m { 'action_dist_inputs': <tf.Tensor 'default_policy/action_dist_inputs:0' shape=(?, 41) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'action_logp': <tf.Tensor 'default_policy/action_logp:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'actions': <tf.Tensor 'default_policy/action:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 10, 60) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'obs': <tf.Tensor 'default_policy/obs:0' shape=(?, 10, 60) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/prev_actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'seq_lens': <tf.Tensor 'default_policy/seq_lens:0' shape=(?,) dtype=int32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'state_in_0': <tf.Tensor 'default_policy/Placeholder:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'state_in_1': <tf.Tensor 'default_policy/Placeholder_1:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:44:53,684\tINFO rollout_worker.py:1199 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x23867b950>}\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:44:53,685\tINFO rollout_worker.py:1200 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x23ed09510>}\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:44:53,685\tDEBUG rollout_worker.py:553 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:44:53,685\tINFO rollout_worker.py:583 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x23bc4df90>}\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:44:53,686\tDEBUG rollout_worker.py:700 -- Created rollout worker with env None (None), policies {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x23867b950>}\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:44:54,030\tINFO tf_policy.py:154 -- TFPolicy (worker=local) running on CPU.\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:44:56,610\tINFO tf_policy.py:154 -- TFPolicy (worker=local) running on CPU.\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:45:01,621\tINFO trainable.py:104 -- Trainable.setup took 72.013 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:45:01,623\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:45:01,656\tINFO rollout_worker.py:724 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:45:01,656\tDEBUG sampler.py:540 -- No episode horizon specified, assuming inf.\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:45:01,661\tINFO sampler.py:591 -- Raw obs from env: { 0: { 'agent0': np.ndarray((10, 60), dtype=float32, min=0.0, max=17699.396, mean=179.501)}}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:45:01,661\tINFO sampler.py:592 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:45:01,662\tINFO sampler.py:813 -- Preprocessed obs: np.ndarray((10, 60), dtype=float32, min=0.0, max=17699.396, mean=179.501)\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:45:01,663\tINFO sampler.py:817 -- Filtered obs: np.ndarray((10, 60), dtype=float32, min=0.0, max=17699.396, mean=179.501)\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:45:01,668\tINFO sampler.py:1005 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'obs': np.ndarray((10, 60), dtype=float32, min=0.0, max=17699.396, mean=179.501),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:45:02,207\tINFO sampler.py:1023 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=15.0, max=15.0, mean=15.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                       [ np.ndarray((1, 16), dtype=float32, min=-0.328, max=0.424, mean=0.008),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         np.ndarray((1, 16), dtype=float32, min=-0.637, max=0.619, mean=-0.013)],\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 41), dtype=float32, min=-0.287, max=0.376, mean=-0.027),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-3.708, max=-3.708, mean=-3.708),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.025, max=0.025, mean=0.025),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.181, max=0.181, mean=0.181)})}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:45:02,943\tINFO simple_list_collector.py:661 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-0.659, max=0.922, mean=-0.017),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-4.202, max=-3.128, mean=-3.696),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=21.18),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-0.671, max=0.342, mean=-0.221),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=772733759.0, max=772733759.0, mean=772733759.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 1, 'net_worth': 997.349313899624}),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'new_obs': np.ndarray((200, 10, 60), dtype=float32, min=-3385.941, max=140120.094, mean=1744.029),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'obs': np.ndarray((200, 10, 60), dtype=float32, min=-3385.941, max=140120.094, mean=1733.951),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=21.045),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.01, max=0.007, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-0.01, max=0.007, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.608, max=0.796, mean=-0.045),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_in_1': np.ndarray((40, 16), dtype=float32, min=-4.939, max=2.265, mean=-0.311),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.734, max=0.796, mean=-0.047),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_out_1': np.ndarray((200, 16), dtype=float32, min=-4.939, max=2.325, mean=-0.312),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-0.112, max=-0.025, mean=-0.07),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-0.395, max=0.568, mean=0.151)}}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:45:02,948\tINFO rollout_worker.py:762 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-0.659, max=0.922, mean=-0.017),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-4.202, max=-3.128, mean=-3.696),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=21.18),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-0.671, max=0.342, mean=-0.221),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=772733759.0, max=772733759.0, mean=772733759.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'obs': np.ndarray((200, 10, 60), dtype=float32, min=-3385.941, max=140120.094, mean=1733.951),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=21.045),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.01, max=0.007, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-0.01, max=0.007, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'seq_lens': np.ndarray((40,), dtype=int32, min=5.0, max=5.0, mean=5.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.608, max=0.796, mean=-0.045),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_in_1': np.ndarray((40, 16), dtype=float32, min=-4.939, max=2.265, mean=-0.311),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.734, max=0.796, mean=-0.047),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_out_1': np.ndarray((200, 16), dtype=float32, min=-4.939, max=2.325, mean=-0.312),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-0.112, max=-0.025, mean=-0.07),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-0.395, max=0.568, mean=0.151)}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:45:26,007\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/fc_1/kernel:0' shape=(600, 256) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:45:26,008\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/fc_1/bias:0' shape=(256,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:45:26,008\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/fc_2/kernel:0' shape=(256, 256) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:45:26,008\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/fc_2/bias:0' shape=(256,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:45:26,008\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/lstm/lstm_cell/kernel:0' shape=(298, 64) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:45:26,008\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/lstm/lstm_cell/recurrent_kernel:0' shape=(16, 64) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:45:26,008\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/lstm/lstm_cell/bias:0' shape=(64,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:45:26,009\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/logits/kernel:0' shape=(16, 41) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:45:26,009\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/logits/bias:0' shape=(41,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:45:26,009\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/values/kernel:0' shape=(16, 1) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:45:26,009\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/values/bias:0' shape=(1,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:45:26,091\tINFO rnn_sequencing.py:139 -- Padded input for RNN/Attn.Nets/MA:\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m { 'features': [ np.ndarray((4000, 10, 60), dtype=float32, min=-3895.877, max=1345973.75, mean=3899.33),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000,), dtype=int64, min=0.0, max=40.0, mean=20.547),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000,), dtype=int64, min=0.0, max=40.0, mean=20.541),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-0.043, max=0.032, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-0.043, max=0.032, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-4.388, max=-2.849, mean=-3.692),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000, 41), dtype=float32, min=-0.853, max=1.015, mean=-0.018),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-0.468, max=0.914, mean=0.226),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-3.329, max=3.738, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-0.383, max=0.854, mean=0.103)],\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'initial_states': [ np.ndarray((800, 16), dtype=float32, min=-0.754, max=0.924, mean=-0.005),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                       np.ndarray((800, 16), dtype=float32, min=-4.939, max=3.86, mean=-0.062)],\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'max_seq_len': 5,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'seq_lens': np.ndarray((800,), dtype=int32, min=5.0, max=5.0, mean=5.0)}\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:45:26,098\tINFO multi_gpu_impl.py:143 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m { 'inputs': [ np.ndarray((4000, 10, 60), dtype=float32, min=-3895.877, max=1345973.75, mean=3899.33),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000,), dtype=int64, min=0.0, max=40.0, mean=20.547),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000,), dtype=int64, min=0.0, max=40.0, mean=20.541),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.043, max=0.032, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.043, max=0.032, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-4.388, max=-2.849, mean=-3.692),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000, 41), dtype=float32, min=-0.853, max=1.015, mean=-0.018),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.468, max=0.914, mean=0.226),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-3.329, max=3.738, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.383, max=0.854, mean=0.103)],\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/obs:0' shape=(?, 10, 60) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/action:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/prev_actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/prev_rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/action_logp:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/action_dist_inputs:0' shape=(?, 41) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/Placeholder:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/Placeholder_1:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/seq_lens:0' shape=(?,) dtype=int32>],\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'state_inputs': [ np.ndarray((800, 16), dtype=float32, min=-0.754, max=0.924, mean=-0.005),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     np.ndarray((800, 16), dtype=float32, min=-4.939, max=3.86, mean=-0.062),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     np.ndarray((800,), dtype=int32, min=5.0, max=5.0, mean=5.0)]}\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:45:26,098\tINFO multi_gpu_impl.py:188 -- Divided 800 rollout sequences, each of length 5, among 1 devices.\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:45:26,183\tDEBUG train_ops.py:218 -- == sgd epochs for default_policy ==\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 2.0/4 CPUs, 0/0 GPUs, 0.0/6.55 GiB heap, 0.0/3.28 GiB objects<br>Result logdir: /Users/leandronogueira/ray_results/PPO<br>Number of trials: 16/300 (15 PENDING, 1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  env_config/window_size</th><th style=\"text-align: right;\">  model/lstm_cell_size</th><th>model/lstm_use_prev_action  </th><th>model/lstm_use_prev_reward  </th><th style=\"text-align: right;\">  model/max_seq_len</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00003</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00004</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00005</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00006</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00007</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00008</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00009</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00010</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00011</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00012</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00013</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00014</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00015</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/6.55 GiB heap, 0.0/3.28 GiB objects<br>Result logdir: /Users/leandronogueira/ray_results/PPO<br>Number of trials: 18/300 (16 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  env_config/window_size</th><th style=\"text-align: right;\">  model/lstm_cell_size</th><th>model/lstm_use_prev_action  </th><th>model/lstm_use_prev_reward  </th><th style=\"text-align: right;\">  model/max_seq_len</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00001</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00003</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00004</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00005</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00006</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00007</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00008</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00009</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00010</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00011</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00012</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00013</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00014</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00015</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00016</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00017</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/6.55 GiB heap, 0.0/3.28 GiB objects<br>Result logdir: /Users/leandronogueira/ray_results/PPO<br>Number of trials: 18/300 (16 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">  env_config/window_size</th><th style=\"text-align: right;\">  model/lstm_cell_size</th><th>model/lstm_use_prev_action  </th><th>model/lstm_use_prev_reward  </th><th style=\"text-align: right;\">  model/max_seq_len</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00000</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00001</td><td>RUNNING </td><td>     </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00003</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00004</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00005</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00006</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00007</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00008</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00009</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00010</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00011</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00012</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00013</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00014</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00015</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00016</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00017</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TradingEnv_5aaa4_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-06_21-45-39\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_media: {}\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 0\n",
      "  experiment_id: 834d67c432884568b80056d0e12e90c1\n",
      "  hostname: Studios-iMac-3.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.700202465057373\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.025436019524931908\n",
      "          model: {}\n",
      "          policy_loss: -0.013990623876452446\n",
      "          total_loss: 0.012650690972805023\n",
      "          vf_explained_var: 0.00599696347489953\n",
      "          vf_loss: 0.02155410870909691\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.15\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.52909090909091\n",
      "    ram_util_percent: 80.05636363636363\n",
      "  pid: 25225\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf: {}\n",
      "  time_since_restore: 38.16521072387695\n",
      "  time_this_iter_s: 38.16521072387695\n",
      "  time_total_s: 38.16521072387695\n",
      "  timers:\n",
      "    learn_throughput: 296.197\n",
      "    learn_time_ms: 13504.526\n",
      "    load_throughput: 22387.382\n",
      "    load_time_ms: 178.672\n",
      "    sample_throughput: 164.24\n",
      "    sample_time_ms: 24354.621\n",
      "    update_time_ms: 10.967\n",
      "  timestamp: 1625618739\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 5aaa4_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:45:44,704\tINFO trainer.py:671 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:45:56,993\tDEBUG worker_set.py:303 -- Creating TF session {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:45:56,995\tDEBUG rollout_worker.py:1160 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:45:57,057\tDEBUG catalog.py:710 -- Created preprocessor <ray.rllib.models.preprocessors.NoPreprocessor object at 0x2347b8e10>: Box(-inf, inf, (15, 60), float32) -> (15, 60)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m Model: \"model_1\"\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m ==================================================================================================\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m seq_in (InputLayer)             [(None,)]            0                                            \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m tf_op_layer_default_policy/Sequ [()]                 0           seq_in[0][0]                     \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m tf_op_layer_default_policy/Sequ [()]                 0           tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m tf_op_layer_default_policy/Sequ [(None, 1)]          0           seq_in[0][0]                     \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m tf_op_layer_default_policy/Sequ [(None,)]            0           tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m tf_op_layer_default_policy/Sequ [(None, 1)]          0           tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m inputs (InputLayer)             [(None, None, 298)]  0                                            \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m h (InputLayer)                  [(None, 16)]         0                                            \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m c (InputLayer)                  [(None, 16)]         0                                            \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m tf_op_layer_default_policy/Sequ [(None, None)]       0           tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                                                  tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m lstm (LSTM)                     [(None, None, 16), ( 20160       inputs[0][0]                     \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                                                  h[0][0]                          \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                                                  c[0][0]                          \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                                                  tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m logits (Dense)                  (None, None, 41)     697         lstm[0][0]                       \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m values (Dense)                  (None, None, 1)      17          lstm[0][0]                       \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m ==================================================================================================\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m Total params: 20,874\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m Trainable params: 20,874\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m Non-trainable params: 0\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m __________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:45:58,250\tINFO tf_policy.py:154 -- TFPolicy (worker=1) running on CPU.\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:45:58,421\tINFO dynamic_tf_policy.py:472 -- Testing `compute_actions` w/ dummy batch.\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:45:58,421\tINFO tf_run_builder.py:87 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:45:58,631\tINFO dynamic_tf_policy.py:481 -- Adding extra-action-fetch `action_prob` to view-reqs.\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:45:58,632\tINFO dynamic_tf_policy.py:481 -- Adding extra-action-fetch `action_logp` to view-reqs.\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:45:58,633\tINFO dynamic_tf_policy.py:481 -- Adding extra-action-fetch `action_dist_inputs` to view-reqs.\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:45:58,634\tINFO dynamic_tf_policy.py:481 -- Adding extra-action-fetch `vf_preds` to view-reqs.\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:45:58,634\tINFO dynamic_tf_policy.py:488 -- Testing `postprocess_trajectory` w/ dummy batch.\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:45:59,013\tDEBUG dynamic_tf_policy.py:514 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'action_dist_inputs': <tf.Tensor 'default_policy/action_dist_inputs:0' shape=(?, 41) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'action_logp': <tf.Tensor 'default_policy/action_logp:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'actions': <tf.Tensor 'default_policy/action:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'agent_index': <tf.Tensor 'default_policy/agent_index:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'eps_id': <tf.Tensor 'default_policy/eps_id:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 15, 60) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'obs': <tf.Tensor 'default_policy/obs:0' shape=(?, 15, 60) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/prev_actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'seq_lens': <tf.Tensor 'default_policy/seq_lens:0' shape=(?,) dtype=int32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_in_0': <tf.Tensor 'default_policy/Placeholder:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_in_1': <tf.Tensor 'default_policy/Placeholder_1:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   't': <tf.Tensor 'default_policy/t:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'unroll_id': <tf.Tensor 'default_policy/unroll_id:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:46:00,704\tDEBUG tf_policy.py:325 -- These tensors were used in the loss_fn:\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'action_dist_inputs': <tf.Tensor 'default_policy/action_dist_inputs:0' shape=(?, 41) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'action_logp': <tf.Tensor 'default_policy/action_logp:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'actions': <tf.Tensor 'default_policy/action:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 15, 60) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'obs': <tf.Tensor 'default_policy/obs:0' shape=(?, 15, 60) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/prev_actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'seq_lens': <tf.Tensor 'default_policy/seq_lens:0' shape=(?,) dtype=int32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_in_0': <tf.Tensor 'default_policy/Placeholder:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_in_1': <tf.Tensor 'default_policy/Placeholder_1:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:01,366\tDEBUG worker_set.py:303 -- Creating TF session {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:01,376\tDEBUG rollout_worker.py:1160 -- Creating policy for default_policy\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:46:01,341\tDEBUG rollout_worker.py:553 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:46:01,341\tDEBUG rollout_worker.py:700 -- Created rollout worker with env <ray.rllib.env.base_env._VectorEnvToBaseEnv object at 0x23c7331d0> (<TradingEnv instance>), policies {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x23d1694d0>}\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:01,463\tDEBUG catalog.py:710 -- Created preprocessor <ray.rllib.models.preprocessors.NoPreprocessor object at 0x246839410>: Box(-inf, inf, (15, 60), float32) -> (15, 60)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m Model: \"model_1\"\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m ==================================================================================================\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m seq_in (InputLayer)             [(None,)]            0                                            \n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m tf_op_layer_default_policy/Sequ [()]                 0           seq_in[0][0]                     \n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m tf_op_layer_default_policy/Sequ [()]                 0           tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m tf_op_layer_default_policy/Sequ [(None, 1)]          0           seq_in[0][0]                     \n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m tf_op_layer_default_policy/Sequ [(None,)]            0           tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m tf_op_layer_default_policy/Sequ [(None, 1)]          0           tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m inputs (InputLayer)             [(None, None, 298)]  0                                            \n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m h (InputLayer)                  [(None, 16)]         0                                            \n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m c (InputLayer)                  [(None, 16)]         0                                            \n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m tf_op_layer_default_policy/Sequ [(None, None)]       0           tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                                                                  tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m lstm (LSTM)                     [(None, None, 16), ( 20160       inputs[0][0]                     \n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                                                                  h[0][0]                          \n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                                                                  c[0][0]                          \n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                                                                  tf_op_layer_default_policy/Sequen\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m logits (Dense)                  (None, None, 41)     697         lstm[0][0]                       \n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m __________________________________________________________________________________________________\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m values (Dense)                  (None, None, 1)      17          lstm[0][0]                       \n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m ==================================================================================================\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m Total params: 20,874\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m Trainable params: 20,874\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m Non-trainable params: 0\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m __________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:46:02,964\tINFO sampler.py:591 -- Raw obs from env: { 0: { 'agent0': np.ndarray((10, 60), dtype=float32, min=-1.402, max=111285.938, mean=4240.245)}}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:46:02,966\tINFO sampler.py:592 -- Info return from env: { 0: { 'agent0': { 'net_worth': 1467.0124812614,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                    'step': 5928}}}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:46:02,966\tINFO sampler.py:813 -- Preprocessed obs: np.ndarray((10, 60), dtype=float32, min=-1.402, max=111285.938, mean=4240.245)\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:46:02,967\tINFO sampler.py:817 -- Filtered obs: np.ndarray((10, 60), dtype=float32, min=-1.402, max=111285.938, mean=4240.245)\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:46:02,973\tINFO sampler.py:1005 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'info': { 'net_worth': 1467.0124812614,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                             'step': 5928},\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'obs': np.ndarray((10, 60), dtype=float32, min=-1.402, max=111285.938, mean=4240.245),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'prev_action': 31,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'prev_reward': 0.00022176511453686487,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'rnn_state': [ np.ndarray((16,), dtype=float32, min=-0.436, max=0.356, mean=-0.06),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                                  np.ndarray((16,), dtype=float32, min=-0.913, max=1.086, mean=-0.115)]},\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:46:02,974\tINFO tf_run_builder.py:87 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:46:02,979\tINFO sampler.py:1023 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=22.0, max=22.0, mean=22.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                       [ np.ndarray((1, 16), dtype=float32, min=-0.452, max=0.38, mean=-0.048),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         np.ndarray((1, 16), dtype=float32, min=-0.888, max=1.142, mean=-0.056)],\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 41), dtype=float32, min=-0.467, max=0.251, mean=-0.033),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-3.705, max=-3.705, mean=-3.705),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.025, max=0.025, mean=0.025),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.112, max=0.112, mean=0.112)})}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:03,038\tINFO tf_policy.py:154 -- TFPolicy (worker=local) running on CPU.\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:03,451\tINFO dynamic_tf_policy.py:472 -- Testing `compute_actions` w/ dummy batch.\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:03,452\tINFO tf_run_builder.py:87 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:03,728\tINFO dynamic_tf_policy.py:481 -- Adding extra-action-fetch `action_prob` to view-reqs.\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:03,730\tINFO dynamic_tf_policy.py:481 -- Adding extra-action-fetch `action_logp` to view-reqs.\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:03,731\tINFO dynamic_tf_policy.py:481 -- Adding extra-action-fetch `action_dist_inputs` to view-reqs.\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:03,731\tINFO dynamic_tf_policy.py:481 -- Adding extra-action-fetch `vf_preds` to view-reqs.\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:03,732\tINFO dynamic_tf_policy.py:488 -- Testing `postprocess_trajectory` w/ dummy batch.\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:04,119\tDEBUG dynamic_tf_policy.py:514 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m { 'action_dist_inputs': <tf.Tensor 'default_policy/action_dist_inputs:0' shape=(?, 41) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'action_logp': <tf.Tensor 'default_policy/action_logp:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'actions': <tf.Tensor 'default_policy/action:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'agent_index': <tf.Tensor 'default_policy/agent_index:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'eps_id': <tf.Tensor 'default_policy/eps_id:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 15, 60) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'obs': <tf.Tensor 'default_policy/obs:0' shape=(?, 15, 60) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/prev_actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'seq_lens': <tf.Tensor 'default_policy/seq_lens:0' shape=(?,) dtype=int32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'state_in_0': <tf.Tensor 'default_policy/Placeholder:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'state_in_1': <tf.Tensor 'default_policy/Placeholder_1:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   't': <tf.Tensor 'default_policy/t:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'unroll_id': <tf.Tensor 'default_policy/unroll_id:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:46:04,194\tINFO simple_list_collector.py:661 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-0.638, max=0.426, mean=-0.03),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-4.187, max=-3.305, mean=-3.707),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.81),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-0.241, max=0.114, mean=-0.097),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=772733759.0, max=772733759.0, mean=772733759.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 5801, 'net_worth': 1411.7514609314}),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'new_obs': np.ndarray((200, 10, 60), dtype=float32, min=-2.005, max=228257.5, mean=3863.252),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'obs': np.ndarray((200, 10, 60), dtype=float32, min=-2.005, max=228257.5, mean=3868.63),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.66),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.013, max=0.01, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-0.013, max=0.01, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.825, max=0.731, mean=-0.053),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_in_1': np.ndarray((40, 16), dtype=float32, min=-2.621, max=1.569, mean=-0.157),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.825, max=0.731, mean=-0.053),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_out_1': np.ndarray((200, 16), dtype=float32, min=-2.749, max=1.796, mean=-0.158),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=29.0, max=29.0, mean=29.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=0.001, max=0.05, mean=0.023),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-0.083, max=0.249, mean=0.12)}}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:46:04,211\tINFO rollout_worker.py:762 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-0.638, max=0.426, mean=-0.03),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-4.187, max=-3.305, mean=-3.707),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.81),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-0.241, max=0.114, mean=-0.097),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=772733759.0, max=772733759.0, mean=772733759.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'obs': np.ndarray((200, 10, 60), dtype=float32, min=-2.005, max=228257.5, mean=3868.63),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.66),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.013, max=0.01, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-0.013, max=0.01, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'seq_lens': np.ndarray((40,), dtype=int32, min=5.0, max=5.0, mean=5.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.825, max=0.731, mean=-0.053),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_in_1': np.ndarray((40, 16), dtype=float32, min=-2.621, max=1.569, mean=-0.157),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.825, max=0.731, mean=-0.053),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_out_1': np.ndarray((200, 16), dtype=float32, min=-2.749, max=1.796, mean=-0.158),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=29.0, max=29.0, mean=29.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=0.001, max=0.05, mean=0.023),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-0.083, max=0.249, mean=0.12)}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:46:04,232\tINFO rollout_worker.py:724 -- Generating sample batch of size 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:05,897\tDEBUG tf_policy.py:325 -- These tensors were used in the loss_fn:\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m { 'action_dist_inputs': <tf.Tensor 'default_policy/action_dist_inputs:0' shape=(?, 41) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'action_logp': <tf.Tensor 'default_policy/action_logp:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'actions': <tf.Tensor 'default_policy/action:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 15, 60) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'obs': <tf.Tensor 'default_policy/obs:0' shape=(?, 15, 60) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/prev_actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'seq_lens': <tf.Tensor 'default_policy/seq_lens:0' shape=(?,) dtype=int32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'state_in_0': <tf.Tensor 'default_policy/Placeholder:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'state_in_1': <tf.Tensor 'default_policy/Placeholder_1:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:06,531\tINFO rollout_worker.py:1199 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x246839910>}\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:06,531\tINFO rollout_worker.py:1200 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x246839410>}\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:06,532\tDEBUG rollout_worker.py:553 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:06,533\tINFO rollout_worker.py:583 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x243774150>}\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:06,533\tDEBUG rollout_worker.py:700 -- Created rollout worker with env None (None), policies {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x246839910>}\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:07,022\tINFO tf_policy.py:154 -- TFPolicy (worker=local) running on CPU.\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:09,817\tINFO tf_policy.py:154 -- TFPolicy (worker=local) running on CPU.\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:14,804\tINFO trainable.py:104 -- Trainable.setup took 33.056 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:14,805\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:46:14,824\tINFO rollout_worker.py:724 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:46:14,826\tDEBUG sampler.py:540 -- No episode horizon specified, assuming inf.\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:46:14,839\tINFO sampler.py:591 -- Raw obs from env: { 0: { 'agent0': np.ndarray((15, 60), dtype=float32, min=0.0, max=17699.396, mean=119.667)}}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:46:14,839\tINFO sampler.py:592 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:46:14,841\tINFO sampler.py:813 -- Preprocessed obs: np.ndarray((15, 60), dtype=float32, min=0.0, max=17699.396, mean=119.667)\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:46:14,842\tINFO sampler.py:817 -- Filtered obs: np.ndarray((15, 60), dtype=float32, min=0.0, max=17699.396, mean=119.667)\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:46:14,849\tINFO sampler.py:1005 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'obs': np.ndarray((15, 60), dtype=float32, min=0.0, max=17699.396, mean=119.667),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'prev_action': None,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'rnn_state': None},\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:46:15,028\tINFO sampler.py:1023 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=23.0, max=23.0, mean=23.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                       [ np.ndarray((1, 16), dtype=float32, min=-0.2, max=0.248, mean=-0.047),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         np.ndarray((1, 16), dtype=float32, min=-0.523, max=0.473, mean=-0.142)],\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 41), dtype=float32, min=-0.13, max=0.159, mean=0.003),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-3.638, max=-3.638, mean=-3.638),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.026, max=0.026, mean=0.026),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.25, max=-0.25, mean=-0.25)})}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:46:15,878\tINFO simple_list_collector.py:661 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-0.934, max=0.772, mean=0.026),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-4.231, max=-3.216, mean=-3.699),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=23.04),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-0.15, max=0.704, mean=0.344),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=267866790.0, max=267866790.0, mean=267866790.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 1, 'net_worth': 996.2228284951051}),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'new_obs': np.ndarray((200, 15, 60), dtype=float32, min=-3385.941, max=140120.094, mean=1718.418),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'obs': np.ndarray((200, 15, 60), dtype=float32, min=-3385.941, max=140120.094, mean=1708.941),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=22.91),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.01, max=0.007, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-0.01, max=0.007, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.792, max=0.823, mean=-0.037),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'state_in_1': np.ndarray((40, 16), dtype=float32, min=-3.795, max=3.05, mean=-0.182),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.842, max=0.825, mean=-0.038),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'state_out_1': np.ndarray((200, 16), dtype=float32, min=-3.875, max=3.065, mean=-0.188),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-0.484, max=-0.076, mean=-0.239),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-0.923, max=-0.221, mean=-0.583)}}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:46:15,884\tINFO rollout_worker.py:762 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-0.934, max=0.772, mean=0.026),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-4.231, max=-3.216, mean=-3.699),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=23.04),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-0.15, max=0.704, mean=0.344),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=267866790.0, max=267866790.0, mean=267866790.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'obs': np.ndarray((200, 15, 60), dtype=float32, min=-3385.941, max=140120.094, mean=1708.941),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=22.91),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.01, max=0.007, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-0.01, max=0.007, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'seq_lens': np.ndarray((40,), dtype=int32, min=5.0, max=5.0, mean=5.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.792, max=0.823, mean=-0.037),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_in_1': np.ndarray((40, 16), dtype=float32, min=-3.795, max=3.05, mean=-0.182),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.842, max=0.825, mean=-0.038),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_out_1': np.ndarray((200, 16), dtype=float32, min=-3.875, max=3.065, mean=-0.188),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-0.484, max=-0.076, mean=-0.239),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-0.923, max=-0.221, mean=-0.583)}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:46:29,705\tINFO rnn_sequencing.py:139 -- Padded input for RNN/Attn.Nets/MA:\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m { 'features': [ np.ndarray((4000, 10, 60), dtype=float32, min=-31483.492, max=1098872.5, mean=4252.23),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000,), dtype=int64, min=0.0, max=40.0, mean=19.573),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000,), dtype=int64, min=0.0, max=40.0, mean=19.575),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-0.119, max=0.198, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-0.119, max=0.198, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-4.292, max=-3.225, mean=-3.7),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000, 41), dtype=float32, min=-0.763, max=0.583, mean=-0.025),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-0.111, max=0.346, mean=0.115),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-3.967, max=3.989, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-0.151, max=0.416, mean=0.039)],\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'initial_states': [ np.ndarray((800, 16), dtype=float32, min=-0.862, max=0.765, mean=-0.06),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                       np.ndarray((800, 16), dtype=float32, min=-4.116, max=2.139, mean=-0.156)],\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'max_seq_len': 5,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'seq_lens': np.ndarray((800,), dtype=int32, min=5.0, max=5.0, mean=5.0)}\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:46:29,721\tINFO multi_gpu_impl.py:143 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m { 'inputs': [ np.ndarray((4000, 10, 60), dtype=float32, min=-31483.492, max=1098872.5, mean=4252.23),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000,), dtype=int64, min=0.0, max=40.0, mean=19.573),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000,), dtype=int64, min=0.0, max=40.0, mean=19.575),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.119, max=0.198, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.119, max=0.198, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-4.292, max=-3.225, mean=-3.7),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000, 41), dtype=float32, min=-0.763, max=0.583, mean=-0.025),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.111, max=0.346, mean=0.115),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-3.967, max=3.989, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.151, max=0.416, mean=0.039)],\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/obs:0' shape=(?, 10, 60) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/action:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/prev_actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/prev_rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/action_logp:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/action_dist_inputs:0' shape=(?, 41) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/Placeholder:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/Placeholder_1:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/seq_lens:0' shape=(?,) dtype=int32>],\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'state_inputs': [ np.ndarray((800, 16), dtype=float32, min=-0.862, max=0.765, mean=-0.06),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     np.ndarray((800, 16), dtype=float32, min=-4.116, max=2.139, mean=-0.156),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     np.ndarray((800,), dtype=int32, min=5.0, max=5.0, mean=5.0)]}\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:46:29,721\tINFO multi_gpu_impl.py:188 -- Divided 800 rollout sequences, each of length 5, among 1 devices.\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:46:29,730\tDEBUG train_ops.py:218 -- == sgd epochs for default_policy ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TradingEnv_5aaa4_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-06_21-46-41\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_media: {}\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 0\n",
      "  experiment_id: 834d67c432884568b80056d0e12e90c1\n",
      "  hostname: Studios-iMac-3.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.686875343322754\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012719170190393925\n",
      "          model: {}\n",
      "          policy_loss: -0.04779733717441559\n",
      "          total_loss: -0.040094662457704544\n",
      "          vf_explained_var: -0.8157675862312317\n",
      "          vf_loss: 0.0038869252894073725\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.15\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 86.17159090909091\n",
      "    ram_util_percent: 87.05454545454545\n",
      "  pid: 25225\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf: {}\n",
      "  time_since_restore: 99.88832879066467\n",
      "  time_this_iter_s: 61.72311806678772\n",
      "  time_total_s: 99.88832879066467\n",
      "  timers:\n",
      "    learn_throughput: 318.338\n",
      "    learn_time_ms: 12565.253\n",
      "    load_throughput: 26888.137\n",
      "    load_time_ms: 148.764\n",
      "    sample_throughput: 108.31\n",
      "    sample_time_ms: 36931.001\n",
      "    update_time_ms: 10.52\n",
      "  timestamp: 1625618801\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 5aaa4_00000\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-06 21:46:42,623\tWARNING util.py:162 -- The `process_trial_result` operation took 0.659 s, which may be a performance bottleneck.\n",
      "2021-07-06 21:46:42,626\tWARNING util.py:162 -- Processing trial results took 0.663 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2021-07-06 21:46:42,630\tWARNING util.py:162 -- The `process_trial` operation took 0.686 s, which may be a performance bottleneck.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.5/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/6.55 GiB heap, 0.0/3.28 GiB objects<br>Result logdir: /Users/leandronogueira/ray_results/PPO<br>Number of trials: 18/300 (16 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  env_config/window_size</th><th style=\"text-align: right;\">  model/lstm_cell_size</th><th>model/lstm_use_prev_action  </th><th>model/lstm_use_prev_reward  </th><th style=\"text-align: right;\">  model/max_seq_len</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00000</td><td>RUNNING </td><td>192.168.0.15:25225</td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         99.8883</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">               nan</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00001</td><td>RUNNING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00002</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00003</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00004</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00005</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00006</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00007</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00008</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00009</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00010</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00011</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00012</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00013</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00014</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00015</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00016</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00017</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:48,974\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/fc_1/kernel:0' shape=(900, 256) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:48,974\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/fc_1/bias:0' shape=(256,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:48,974\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/fc_2/kernel:0' shape=(256, 256) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:48,974\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/fc_2/bias:0' shape=(256,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:48,974\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/lstm/lstm_cell/kernel:0' shape=(298, 64) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:48,974\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/lstm/lstm_cell/recurrent_kernel:0' shape=(16, 64) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:48,974\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/lstm/lstm_cell/bias:0' shape=(64,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:48,975\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/logits/kernel:0' shape=(16, 41) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:48,975\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/logits/bias:0' shape=(41,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:48,975\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/values/kernel:0' shape=(16, 1) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:48,975\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/values/bias:0' shape=(1,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:49,083\tINFO rnn_sequencing.py:139 -- Padded input for RNN/Attn.Nets/MA:\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m { 'features': [ np.ndarray((4000, 15, 60), dtype=float32, min=-3895.877, max=1345973.75, mean=3895.822),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000,), dtype=int64, min=0.0, max=40.0, mean=20.968),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000,), dtype=int64, min=0.0, max=40.0, mean=20.966),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-0.043, max=0.041, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-0.043, max=0.041, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-4.66, max=-3.02, mean=-3.671),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000, 41), dtype=float32, min=-1.073, max=0.894, mean=0.038),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-1.135, max=-0.07, mean=-0.654),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-4.082, max=2.632, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-0.952, max=-0.026, mean=-0.255)],\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'initial_states': [ np.ndarray((800, 16), dtype=float32, min=-0.934, max=0.823, mean=-0.04),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                       np.ndarray((800, 16), dtype=float32, min=-3.799, max=3.638, mean=-0.188)],\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'max_seq_len': 5,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'seq_lens': np.ndarray((800,), dtype=int32, min=5.0, max=5.0, mean=5.0)}\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:49,092\tINFO multi_gpu_impl.py:143 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m { 'inputs': [ np.ndarray((4000, 15, 60), dtype=float32, min=-3895.877, max=1345973.75, mean=3895.822),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000,), dtype=int64, min=0.0, max=40.0, mean=20.968),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000,), dtype=int64, min=0.0, max=40.0, mean=20.966),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.043, max=0.041, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.043, max=0.041, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-4.66, max=-3.02, mean=-3.671),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000, 41), dtype=float32, min=-1.073, max=0.894, mean=0.038),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-1.135, max=-0.07, mean=-0.654),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-4.082, max=2.632, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.952, max=-0.026, mean=-0.255)],\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/obs:0' shape=(?, 15, 60) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/action:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/prev_actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/prev_rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/action_logp:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/action_dist_inputs:0' shape=(?, 41) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/Placeholder:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/Placeholder_1:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/seq_lens:0' shape=(?,) dtype=int32>],\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'state_inputs': [ np.ndarray((800, 16), dtype=float32, min=-0.934, max=0.823, mean=-0.04),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     np.ndarray((800, 16), dtype=float32, min=-3.799, max=3.638, mean=-0.188),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     np.ndarray((800,), dtype=int32, min=5.0, max=5.0, mean=5.0)]}\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:49,092\tINFO multi_gpu_impl.py:188 -- Divided 800 rollout sequences, each of length 5, among 1 devices.\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:46:49,200\tDEBUG train_ops.py:218 -- == sgd epochs for default_policy ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TradingEnv_5aaa4_00001:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-06_21-47-03\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_media: {}\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 0\n",
      "  experiment_id: ca8561777d5b4b408c18a0ec29d5696b\n",
      "  hostname: Studios-iMac-3.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.6973869800567627\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02782389707863331\n",
      "          model: {}\n",
      "          policy_loss: -0.004239772446453571\n",
      "          total_loss: 0.03100641630589962\n",
      "          vf_explained_var: 0.019269751384854317\n",
      "          vf_loss: 0.029681410640478134\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.0.15\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 75.00422535211268\n",
      "    ram_util_percent: 85.64788732394365\n",
      "  pid: 25226\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf: {}\n",
      "  time_since_restore: 49.17523717880249\n",
      "  time_this_iter_s: 49.17523717880249\n",
      "  time_total_s: 49.17523717880249\n",
      "  timers:\n",
      "    learn_throughput: 271.916\n",
      "    learn_time_ms: 14710.43\n",
      "    load_throughput: 17685.564\n",
      "    load_time_ms: 226.173\n",
      "    sample_throughput: 117.132\n",
      "    sample_time_ms: 34149.367\n",
      "    update_time_ms: 5.15\n",
      "  timestamp: 1625618823\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 5aaa4_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.6/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/6.55 GiB heap, 0.0/3.28 GiB objects<br>Result logdir: /Users/leandronogueira/ray_results/PPO<br>Number of trials: 18/300 (16 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  env_config/window_size</th><th style=\"text-align: right;\">  model/lstm_cell_size</th><th>model/lstm_use_prev_action  </th><th>model/lstm_use_prev_reward  </th><th style=\"text-align: right;\">  model/max_seq_len</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00000</td><td>RUNNING </td><td>192.168.0.15:25225</td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         99.8883</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">               nan</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00001</td><td>RUNNING </td><td>192.168.0.15:25226</td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         49.1752</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">               nan</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00002</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00003</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00004</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00005</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00006</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00007</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00008</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00009</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00010</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00011</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00012</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00013</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00014</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00015</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00016</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00017</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:47:04,241\tINFO sampler.py:813 -- Preprocessed obs: np.ndarray((10, 60), dtype=float32, min=-1.438, max=141938.453, mean=4806.675)\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:47:04,242\tINFO sampler.py:817 -- Filtered obs: np.ndarray((10, 60), dtype=float32, min=-1.438, max=141938.453, mean=4806.675)\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:47:04,243\tINFO sampler.py:1005 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'info': { 'net_worth': 1159.550858647738,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                             'step': 8792},\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'obs': np.ndarray((10, 60), dtype=float32, min=-1.438, max=141938.453, mean=4806.675),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'prev_action': 28,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'prev_reward': -0.0068913169406548125,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'rnn_state': [ np.ndarray((16,), dtype=float32, min=-0.792, max=0.456, mean=-0.155),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                                  np.ndarray((16,), dtype=float32, min=-1.834, max=1.244, mean=-0.25)]},\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:47:04,244\tINFO tf_run_builder.py:87 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:47:04,249\tINFO sampler.py:1023 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=2.0, max=2.0, mean=2.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                       [ np.ndarray((1, 16), dtype=float32, min=-0.816, max=0.424, mean=-0.17),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         np.ndarray((1, 16), dtype=float32, min=-1.804, max=1.16, mean=-0.27)],\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 41), dtype=float32, min=-0.673, max=0.682, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-3.644, max=-3.644, mean=-3.644),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.026, max=0.026, mean=0.026),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.141, max=0.141, mean=0.141)})}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:47:04,264\tINFO sampler.py:591 -- Raw obs from env: { 0: { 'agent0': np.ndarray((10, 60), dtype=float32, min=-1.438, max=141938.453, mean=4882.546)}}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:47:04,264\tINFO sampler.py:592 -- Info return from env: { 0: { 'agent0': { 'net_worth': 1163.002596993606,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                    'step': 8793}}}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:47:04,391\tINFO simple_list_collector.py:661 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-1.044, max=0.879, mean=0.012),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-4.422, max=-3.074, mean=-3.664),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=21.065),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-0.128, max=0.238, mean=0.034),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=772733759.0, max=772733759.0, mean=772733759.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 8601, 'net_worth': 1164.285337595111}),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'new_obs': np.ndarray((200, 10, 60), dtype=float32, min=-2.595, max=245207.656, mean=3385.428),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'obs': np.ndarray((200, 10, 60), dtype=float32, min=-2.595, max=245207.656, mean=3375.358),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=21.26),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.019, max=0.011, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-0.019, max=0.011, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.938, max=0.848, mean=-0.151),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_in_1': np.ndarray((40, 16), dtype=float32, min=-4.493, max=1.87, mean=-0.254),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.959, max=0.897, mean=-0.152),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_out_1': np.ndarray((200, 16), dtype=float32, min=-4.493, max=1.971, mean=-0.252),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=43.0, max=43.0, mean=43.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=0.004, max=0.123, mean=0.055),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-0.212, max=0.147, mean=0.021)}}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:47:04,397\tINFO rollout_worker.py:762 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-1.044, max=0.879, mean=0.012),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-4.422, max=-3.074, mean=-3.664),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=21.065),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-0.128, max=0.238, mean=0.034),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=772733759.0, max=772733759.0, mean=772733759.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'obs': np.ndarray((200, 10, 60), dtype=float32, min=-2.595, max=245207.656, mean=3375.358),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=21.26),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.019, max=0.011, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-0.019, max=0.011, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'seq_lens': np.ndarray((40,), dtype=int32, min=5.0, max=5.0, mean=5.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.938, max=0.848, mean=-0.151),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_in_1': np.ndarray((40, 16), dtype=float32, min=-4.493, max=1.87, mean=-0.254),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.959, max=0.897, mean=-0.152),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_out_1': np.ndarray((200, 16), dtype=float32, min=-4.493, max=1.971, mean=-0.252),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=43.0, max=43.0, mean=43.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=0.004, max=0.123, mean=0.055),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-0.212, max=0.147, mean=0.021)}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:47:04,403\tINFO rollout_worker.py:724 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:47:15,884\tINFO sampler.py:813 -- Preprocessed obs: np.ndarray((15, 60), dtype=float32, min=-1.506, max=62650.57, mean=2763.997)\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:47:15,884\tINFO sampler.py:817 -- Filtered obs: np.ndarray((15, 60), dtype=float32, min=-1.506, max=62650.57, mean=2763.997)\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:47:15,887\tINFO sampler.py:1005 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'info': { 'net_worth': 1559.7522924148998,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                             'step': 5307},\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'obs': np.ndarray((15, 60), dtype=float32, min=-1.506, max=62650.57, mean=2763.997),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'prev_action': 6,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'prev_reward': 0.0011811970998418797,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'rnn_state': [ np.ndarray((16,), dtype=float32, min=-0.313, max=0.5, mean=-0.03),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                                  np.ndarray((16,), dtype=float32, min=-1.294, max=1.124, mean=-0.17)]},\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:47:15,887\tINFO tf_run_builder.py:87 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:47:15,891\tINFO sampler.py:1023 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=18.0, max=18.0, mean=18.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                       [ np.ndarray((1, 16), dtype=float32, min=-0.341, max=0.263, mean=-0.048),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         np.ndarray((1, 16), dtype=float32, min=-1.221, max=0.939, mean=-0.177)],\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 41), dtype=float32, min=-0.186, max=0.21, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-3.814, max=-3.814, mean=-3.814),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.022, max=0.022, mean=0.022),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.184, max=-0.184, mean=-0.184)})}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:47:15,897\tINFO sampler.py:591 -- Raw obs from env: { 0: { 'agent0': np.ndarray((15, 60), dtype=float32, min=-1.506, max=62650.57, mean=2820.302)}}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:47:15,898\tINFO sampler.py:592 -- Info return from env: { 0: { 'agent0': { 'net_worth': 1559.4931320979997,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                    'step': 5308}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:47:16,792\tINFO simple_list_collector.py:661 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-0.611, max=0.719, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-4.252, max=-3.26, mean=-3.698),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=21.55),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-0.059, max=0.329, mean=0.116),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=267866790.0, max=267866790.0, mean=267866790.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 5201, 'net_worth': 1591.3176930254997}),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'new_obs': np.ndarray((200, 15, 60), dtype=float32, min=-2.021, max=411879.219, mean=3433.295),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'obs': np.ndarray((200, 15, 60), dtype=float32, min=-2.021, max=411879.219, mean=3426.065),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=21.56),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.008, max=0.008, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-0.008, max=0.008, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.665, max=0.655, mean=-0.052),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'state_in_1': np.ndarray((40, 16), dtype=float32, min=-2.678, max=1.691, mean=-0.218),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.8, max=0.68, mean=-0.053),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'state_out_1': np.ndarray((200, 16), dtype=float32, min=-2.856, max=2.051, mean=-0.219),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=26.0, max=26.0, mean=26.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-0.307, max=-0.063, mean=-0.147),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-0.438, max=-0.063, mean=-0.262)}}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:47:16,796\tINFO rollout_worker.py:762 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-0.611, max=0.719, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-4.252, max=-3.26, mean=-3.698),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=21.55),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-0.059, max=0.329, mean=0.116),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=267866790.0, max=267866790.0, mean=267866790.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'obs': np.ndarray((200, 15, 60), dtype=float32, min=-2.021, max=411879.219, mean=3426.065),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=21.56),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.008, max=0.008, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-0.008, max=0.008, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'seq_lens': np.ndarray((40,), dtype=int32, min=5.0, max=5.0, mean=5.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.665, max=0.655, mean=-0.052),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_in_1': np.ndarray((40, 16), dtype=float32, min=-2.678, max=1.691, mean=-0.218),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.8, max=0.68, mean=-0.053),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_out_1': np.ndarray((200, 16), dtype=float32, min=-2.856, max=2.051, mean=-0.219),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=26.0, max=26.0, mean=26.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-0.307, max=-0.063, mean=-0.147),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-0.438, max=-0.063, mean=-0.262)}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:47:16,800\tINFO rollout_worker.py:724 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:47:54,152\tINFO rnn_sequencing.py:139 -- Padded input for RNN/Attn.Nets/MA:\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m { 'features': [ np.ndarray((4000, 15, 60), dtype=float32, min=-31483.492, max=1098872.5, mean=4253.28),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000,), dtype=int64, min=0.0, max=40.0, mean=19.761),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000,), dtype=int64, min=0.0, max=40.0, mean=19.762),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-0.089, max=0.198, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-0.089, max=0.198, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-4.262, max=-3.088, mean=-3.697),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000, 41), dtype=float32, min=-0.625, max=0.719, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-0.537, max=0.038, mean=-0.263),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-3.778, max=5.871, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-0.377, max=0.367, mean=-0.114)],\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'initial_states': [ np.ndarray((800, 16), dtype=float32, min=-0.912, max=0.68, mean=-0.053),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                       np.ndarray((800, 16), dtype=float32, min=-4.123, max=2.401, mean=-0.218)],\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'max_seq_len': 5,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'seq_lens': np.ndarray((800,), dtype=int32, min=5.0, max=5.0, mean=5.0)}\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:47:54,160\tINFO multi_gpu_impl.py:143 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m { 'inputs': [ np.ndarray((4000, 15, 60), dtype=float32, min=-31483.492, max=1098872.5, mean=4253.28),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000,), dtype=int64, min=0.0, max=40.0, mean=19.761),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000,), dtype=int64, min=0.0, max=40.0, mean=19.762),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.089, max=0.198, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.089, max=0.198, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-4.262, max=-3.088, mean=-3.697),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000, 41), dtype=float32, min=-0.625, max=0.719, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.537, max=0.038, mean=-0.263),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-3.778, max=5.871, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.377, max=0.367, mean=-0.114)],\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/obs:0' shape=(?, 15, 60) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/action:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/prev_actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/prev_rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/action_logp:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/action_dist_inputs:0' shape=(?, 41) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/Placeholder:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/Placeholder_1:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/seq_lens:0' shape=(?,) dtype=int32>],\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'state_inputs': [ np.ndarray((800, 16), dtype=float32, min=-0.912, max=0.68, mean=-0.053),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     np.ndarray((800, 16), dtype=float32, min=-4.123, max=2.401, mean=-0.218),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     np.ndarray((800,), dtype=int32, min=5.0, max=5.0, mean=5.0)]}\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:47:54,160\tINFO multi_gpu_impl.py:188 -- Divided 800 rollout sequences, each of length 5, among 1 devices.\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:47:54,162\tDEBUG train_ops.py:218 -- == sgd epochs for default_policy ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:48:04,527\tINFO sampler.py:813 -- Preprocessed obs: np.ndarray((10, 60), dtype=float32, min=-1.203, max=82354.094, mean=2943.902)\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:48:04,528\tINFO sampler.py:817 -- Filtered obs: np.ndarray((10, 60), dtype=float32, min=-1.203, max=82354.094, mean=2943.902)\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:48:04,529\tINFO sampler.py:1005 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'info': { 'net_worth': 1676.5398130628,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                             'step': 11558},\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'obs': np.ndarray((10, 60), dtype=float32, min=-1.203, max=82354.094, mean=2943.902),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'prev_action': 9,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'prev_reward': 0.004805046381174627,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'rnn_state': [ np.ndarray((16,), dtype=float32, min=-0.872, max=0.192, mean=-0.171),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                                  np.ndarray((16,), dtype=float32, min=-1.936, max=0.702, mean=-0.309)]},\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:48:04,530\tINFO tf_run_builder.py:87 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:48:04,535\tINFO sampler.py:1023 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=21.0, max=21.0, mean=21.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                       [ np.ndarray((1, 16), dtype=float32, min=-0.848, max=0.246, mean=-0.161),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         np.ndarray((1, 16), dtype=float32, min=-2.026, max=0.648, mean=-0.271)],\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 41), dtype=float32, min=-0.742, max=0.513, mean=0.023),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-3.349, max=-3.349, mean=-3.349),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.035, max=0.035, mean=0.035),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.029, max=0.029, mean=0.029)})}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:48:04,626\tINFO sampler.py:591 -- Raw obs from env: { 0: { 'agent0': np.ndarray((10, 60), dtype=float32, min=-1.203, max=82354.094, mean=2890.465)}}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:48:04,626\tINFO sampler.py:592 -- Info return from env: { 0: { 'agent0': { 'net_worth': 1667.3589554461,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                    'step': 11559}}}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:48:06,794\tINFO simple_list_collector.py:661 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-0.839, max=0.857, mean=0.014),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-4.341, max=-2.955, mean=-3.652),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.77),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-0.157, max=0.14, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=772733759.0, max=772733759.0, mean=772733759.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 11401, 'net_worth': 1594.04795731962}),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'new_obs': np.ndarray((200, 10, 60), dtype=float32, min=-2.026, max=293981.031, mean=4026.732),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'obs': np.ndarray((200, 10, 60), dtype=float32, min=-2.026, max=293981.031, mean=4027.864),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.9),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.027, max=0.025, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-0.027, max=0.025, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.903, max=0.799, mean=-0.086),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_in_1': np.ndarray((40, 16), dtype=float32, min=-2.752, max=1.505, mean=-0.096),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.933, max=0.899, mean=-0.079),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_out_1': np.ndarray((200, 16), dtype=float32, min=-3.448, max=1.738, mean=-0.086),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=57.0, max=57.0, mean=57.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-0.039, max=0.066, mean=0.028),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-0.117, max=0.142, mean=0.025)}}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:48:06,815\tINFO rollout_worker.py:762 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-0.839, max=0.857, mean=0.014),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-4.341, max=-2.955, mean=-3.652),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.77),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-0.157, max=0.14, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=772733759.0, max=772733759.0, mean=772733759.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'obs': np.ndarray((200, 10, 60), dtype=float32, min=-2.026, max=293981.031, mean=4027.864),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.9),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.027, max=0.025, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-0.027, max=0.025, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'seq_lens': np.ndarray((40,), dtype=int32, min=5.0, max=5.0, mean=5.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.903, max=0.799, mean=-0.086),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_in_1': np.ndarray((40, 16), dtype=float32, min=-2.752, max=1.505, mean=-0.096),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.933, max=0.899, mean=-0.079),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_out_1': np.ndarray((200, 16), dtype=float32, min=-3.448, max=1.738, mean=-0.086),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=57.0, max=57.0, mean=57.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-0.039, max=0.066, mean=0.028),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-0.117, max=0.142, mean=0.025)}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:48:06,842\tINFO rollout_worker.py:724 -- Generating sample batch of size 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TradingEnv_5aaa4_00001:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-06_21-48-08\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_media: {}\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 0\n",
      "  experiment_id: ca8561777d5b4b408c18a0ec29d5696b\n",
      "  hostname: Studios-iMac-3.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.691011905670166\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009547153487801552\n",
      "          model: {}\n",
      "          policy_loss: -0.0267786867916584\n",
      "          total_loss: -0.01466041523963213\n",
      "          vf_explained_var: -0.23813757300376892\n",
      "          vf_loss: 0.009254123084247112\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.0.15\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 68.04285714285714\n",
      "    ram_util_percent: 83.72527472527473\n",
      "  pid: 25226\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf: {}\n",
      "  time_since_restore: 113.20607900619507\n",
      "  time_this_iter_s: 64.03084182739258\n",
      "  time_total_s: 113.20607900619507\n",
      "  timers:\n",
      "    learn_throughput: 278.588\n",
      "    learn_time_ms: 14358.129\n",
      "    load_throughput: 24247.493\n",
      "    load_time_ms: 164.966\n",
      "    sample_throughput: 95.177\n",
      "    sample_time_ms: 42027.022\n",
      "    update_time_ms: 6.092\n",
      "  timestamp: 1625618888\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 5aaa4_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.4/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/6.55 GiB heap, 0.0/3.28 GiB objects<br>Result logdir: /Users/leandronogueira/ray_results/PPO<br>Number of trials: 18/300 (16 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  env_config/window_size</th><th style=\"text-align: right;\">  model/lstm_cell_size</th><th>model/lstm_use_prev_action  </th><th>model/lstm_use_prev_reward  </th><th style=\"text-align: right;\">  model/max_seq_len</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00000</td><td>RUNNING </td><td>192.168.0.15:25225</td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         99.8883</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">               nan</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00001</td><td>RUNNING </td><td>192.168.0.15:25226</td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">        113.206 </td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">               nan</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00002</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00003</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00004</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00005</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00006</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00007</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00008</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00009</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00010</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00011</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00012</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00013</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00014</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00015</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00016</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00017</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:48:16,804\tINFO sampler.py:813 -- Preprocessed obs: np.ndarray((15, 60), dtype=float32, min=-1.825, max=103924.867, mean=2825.171)\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:48:16,805\tINFO sampler.py:817 -- Filtered obs: np.ndarray((15, 60), dtype=float32, min=-1.825, max=103924.867, mean=2825.171)\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:48:16,823\tINFO sampler.py:1005 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'info': { 'net_worth': 1458.767735060202,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                             'step': 8476},\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'obs': np.ndarray((15, 60), dtype=float32, min=-1.825, max=103924.867, mean=2825.171),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'prev_action': 14,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'prev_reward': -0.002949970101654431,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'rnn_state': [ np.ndarray((16,), dtype=float32, min=-0.709, max=0.18, mean=-0.13),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                                  np.ndarray((16,), dtype=float32, min=-1.221, max=0.741, mean=-0.355)]},\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:48:16,824\tINFO tf_run_builder.py:87 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:48:16,835\tINFO sampler.py:1023 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=3.0, max=3.0, mean=3.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                       [ np.ndarray((1, 16), dtype=float32, min=-0.751, max=0.305, mean=-0.14),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         np.ndarray((1, 16), dtype=float32, min=-1.657, max=0.858, mean=-0.415)],\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 41), dtype=float32, min=-0.377, max=0.49, mean=0.023),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-3.353, max=-3.353, mean=-3.353),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.035, max=0.035, mean=0.035),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.102, max=-0.102, mean=-0.102)})}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:48:16,858\tINFO sampler.py:591 -- Raw obs from env: { 0: { 'agent0': np.ndarray((15, 60), dtype=float32, min=-1.825, max=103924.867, mean=2825.077)}}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:48:16,858\tINFO sampler.py:592 -- Info return from env: { 0: { 'agent0': { 'net_worth': 1466.8749165073002,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                    'step': 8477}}}\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:48:17,694\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/fc_1/kernel:0' shape=(600, 256) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:48:17,695\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/fc_1/bias:0' shape=(256,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:48:17,695\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/fc_2/kernel:0' shape=(256, 256) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:48:17,695\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/fc_2/bias:0' shape=(256,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:48:17,695\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/lstm/lstm_cell/kernel:0' shape=(298, 64) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:48:17,696\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/lstm/lstm_cell/recurrent_kernel:0' shape=(16, 64) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:48:17,696\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/lstm/lstm_cell/bias:0' shape=(64,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:48:17,696\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/logits/kernel:0' shape=(16, 41) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:48:17,696\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/logits/bias:0' shape=(41,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:48:17,696\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/values/kernel:0' shape=(16, 1) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:48:17,696\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/values/bias:0' shape=(1,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:48:17,770\tDEBUG train_ops.py:218 -- == sgd epochs for default_policy ==\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:48:20,571\tINFO simple_list_collector.py:661 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-0.647, max=0.724, mean=0.012),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-4.309, max=-3.103, mean=-3.683),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.29),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-0.054, max=0.222, mean=0.107),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=267866790.0, max=267866790.0, mean=267866790.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 8401, 'net_worth': 1407.67499442957}),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'new_obs': np.ndarray((200, 15, 60), dtype=float32, min=-2.36, max=275087.75, mean=2735.437),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'obs': np.ndarray((200, 15, 60), dtype=float32, min=-2.36, max=275087.75, mean=2732.464),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.305),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.012, max=0.017, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-0.012, max=0.017, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.869, max=0.62, mean=-0.094),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'state_in_1': np.ndarray((40, 16), dtype=float32, min=-2.825, max=3.046, mean=-0.202),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.895, max=0.684, mean=-0.093),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'state_out_1': np.ndarray((200, 16), dtype=float32, min=-2.857, max=3.2, mean=-0.189),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=42.0, max=42.0, mean=42.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-0.013, max=0.034, mean=0.018),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-0.222, max=0.075, mean=-0.089)}}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:48:20,580\tINFO rollout_worker.py:762 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-0.647, max=0.724, mean=0.012),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-4.309, max=-3.103, mean=-3.683),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.29),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-0.054, max=0.222, mean=0.107),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=267866790.0, max=267866790.0, mean=267866790.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'obs': np.ndarray((200, 15, 60), dtype=float32, min=-2.36, max=275087.75, mean=2732.464),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.305),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.012, max=0.017, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-0.012, max=0.017, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'seq_lens': np.ndarray((40,), dtype=int32, min=5.0, max=5.0, mean=5.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.869, max=0.62, mean=-0.094),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_in_1': np.ndarray((40, 16), dtype=float32, min=-2.825, max=3.046, mean=-0.202),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.895, max=0.684, mean=-0.093),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_out_1': np.ndarray((200, 16), dtype=float32, min=-2.857, max=3.2, mean=-0.189),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=42.0, max=42.0, mean=42.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-0.013, max=0.034, mean=0.018),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-0.222, max=0.075, mean=-0.089)}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:48:20,588\tINFO rollout_worker.py:724 -- Generating sample batch of size 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TradingEnv_5aaa4_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-06_21-48-30\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_media: {}\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 0\n",
      "  experiment_id: 834d67c432884568b80056d0e12e90c1\n",
      "  hostname: Studios-iMac-3.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.668748378753662\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013344019651412964\n",
      "          model: {}\n",
      "          policy_loss: -0.038638681173324585\n",
      "          total_loss: -0.03145468607544899\n",
      "          vf_explained_var: -0.7831448912620544\n",
      "          vf_loss: 0.003180790226906538\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.15\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 90.21806451612903\n",
      "    ram_util_percent: 84.11741935483872\n",
      "  pid: 25225\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf: {}\n",
      "  time_since_restore: 208.12070965766907\n",
      "  time_this_iter_s: 108.2323808670044\n",
      "  time_total_s: 208.12070965766907\n",
      "  timers:\n",
      "    learn_throughput: 314.105\n",
      "    learn_time_ms: 12734.607\n",
      "    load_throughput: 32164.517\n",
      "    load_time_ms: 124.361\n",
      "    sample_throughput: 71.04\n",
      "    sample_time_ms: 56306.192\n",
      "    update_time_ms: 11.367\n",
      "  timestamp: 1625618910\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 5aaa4_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/6.55 GiB heap, 0.0/3.28 GiB objects<br>Result logdir: /Users/leandronogueira/ray_results/PPO<br>Number of trials: 18/300 (16 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  env_config/window_size</th><th style=\"text-align: right;\">  model/lstm_cell_size</th><th>model/lstm_use_prev_action  </th><th>model/lstm_use_prev_reward  </th><th style=\"text-align: right;\">  model/max_seq_len</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00000</td><td>RUNNING </td><td>192.168.0.15:25225</td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         208.121</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">               nan</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00001</td><td>RUNNING </td><td>192.168.0.15:25226</td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         113.206</td><td style=\"text-align: right;\"> 8000</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">               nan</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00002</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00003</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00004</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00005</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00006</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00007</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00008</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00009</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00010</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00011</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00012</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00013</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00014</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00015</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00016</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00017</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:49:06,853\tINFO sampler.py:813 -- Preprocessed obs: np.ndarray((10, 60), dtype=float32, min=-1.695, max=101381.25, mean=3350.02)\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:49:06,853\tINFO sampler.py:817 -- Filtered obs: np.ndarray((10, 60), dtype=float32, min=-1.695, max=101381.25, mean=3350.02)\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:49:06,855\tINFO sampler.py:1005 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'info': { 'net_worth': 1855.5617836077,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                             'step': 13396},\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'obs': np.ndarray((10, 60), dtype=float32, min=-1.695, max=101381.25, mean=3350.02),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'prev_action': 14,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'prev_reward': -0.00025198592566422207,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'rnn_state': [ np.ndarray((16,), dtype=float32, min=-0.896, max=0.601, mean=-0.148),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                                  np.ndarray((16,), dtype=float32, min=-1.849, max=1.24, mean=-0.301)]},\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:49:06,855\tINFO tf_run_builder.py:87 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:49:06,861\tINFO sampler.py:1023 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=16.0, max=16.0, mean=16.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                       [ np.ndarray((1, 16), dtype=float32, min=-0.914, max=0.554, mean=-0.146),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         np.ndarray((1, 16), dtype=float32, min=-2.103, max=1.259, mean=-0.305)],\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 41), dtype=float32, min=-0.851, max=0.639, mean=-0.068),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-4.034, max=-4.034, mean=-4.034),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.018, max=0.018, mean=0.018),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.032, max=-0.032, mean=-0.032)})}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:49:06,881\tINFO sampler.py:591 -- Raw obs from env: { 0: { 'agent0': np.ndarray((10, 60), dtype=float32, min=-1.689, max=101381.25, mean=3364.145)}}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:49:06,881\tINFO sampler.py:592 -- Info return from env: { 0: { 'agent0': { 'net_worth': 1855.4553168389,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                    'step': 13397}}}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:49:06,950\tINFO simple_list_collector.py:661 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-1.011, max=0.955, mean=-0.015),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-4.393, max=-3.054, mean=-3.7),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=20.02),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-0.091, max=0.174, mean=0.004),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=772733759.0, max=772733759.0, mean=772733759.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 13201, 'net_worth': 1841.3218532807}),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'new_obs': np.ndarray((200, 10, 60), dtype=float32, min=-2.267, max=102574.047, mean=3011.954),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'obs': np.ndarray((200, 10, 60), dtype=float32, min=-2.267, max=102574.047, mean=3010.383),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.865),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.012, max=0.013, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-0.012, max=0.013, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.899, max=0.973, mean=-0.058),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_in_1': np.ndarray((40, 16), dtype=float32, min=-2.317, max=2.865, mean=-0.061),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.948, max=0.978, mean=-0.057),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_out_1': np.ndarray((200, 16), dtype=float32, min=-2.317, max=3.557, mean=-0.055),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=66.0, max=66.0, mean=66.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-0.038, max=0.004, mean=-0.015),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-0.179, max=0.088, mean=-0.019)}}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:49:06,955\tINFO rollout_worker.py:762 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-1.011, max=0.955, mean=-0.015),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-4.393, max=-3.054, mean=-3.7),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=20.02),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-0.091, max=0.174, mean=0.004),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=772733759.0, max=772733759.0, mean=772733759.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'obs': np.ndarray((200, 10, 60), dtype=float32, min=-2.267, max=102574.047, mean=3010.383),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.865),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.012, max=0.013, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-0.012, max=0.013, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'seq_lens': np.ndarray((40,), dtype=int32, min=5.0, max=5.0, mean=5.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.899, max=0.973, mean=-0.058),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_in_1': np.ndarray((40, 16), dtype=float32, min=-2.317, max=2.865, mean=-0.061),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.948, max=0.978, mean=-0.057),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_out_1': np.ndarray((200, 16), dtype=float32, min=-2.317, max=3.557, mean=-0.055),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=66.0, max=66.0, mean=66.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-0.038, max=0.004, mean=-0.015),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-0.179, max=0.088, mean=-0.019)}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:49:06,965\tINFO rollout_worker.py:724 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:49:20,599\tINFO sampler.py:813 -- Preprocessed obs: np.ndarray((15, 60), dtype=float32, min=-1.347, max=66486.883, mean=2589.395)\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:49:20,601\tINFO sampler.py:817 -- Filtered obs: np.ndarray((15, 60), dtype=float32, min=-1.347, max=66486.883, mean=2589.395)\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:49:20,602\tINFO sampler.py:1005 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'info': { 'net_worth': 1799.942987719085,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                             'step': 11345},\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'obs': np.ndarray((15, 60), dtype=float32, min=-1.347, max=66486.883, mean=2589.395),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'prev_action': 8,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'prev_reward': -0.002409712617181259,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'rnn_state': [ np.ndarray((16,), dtype=float32, min=-0.58, max=0.383, mean=-0.069),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                                  np.ndarray((16,), dtype=float32, min=-0.958, max=1.192, mean=-0.166)]},\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:49:20,603\tINFO tf_run_builder.py:87 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:49:20,607\tINFO sampler.py:1023 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=34.0, max=34.0, mean=34.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                       [ np.ndarray((1, 16), dtype=float32, min=-0.36, max=0.502, mean=-0.079),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         np.ndarray((1, 16), dtype=float32, min=-1.016, max=1.398, mean=-0.19)],\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 41), dtype=float32, min=-0.51, max=0.394, mean=-0.002),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-3.713, max=-3.713, mean=-3.713),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.024, max=0.024, mean=0.024),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.148, max=-0.148, mean=-0.148)})}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:49:20,621\tINFO sampler.py:591 -- Raw obs from env: { 0: { 'agent0': np.ndarray((15, 60), dtype=float32, min=-1.347, max=66486.883, mean=2602.901)}}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:49:20,622\tINFO sampler.py:592 -- Info return from env: { 0: { 'agent0': { 'net_worth': 1802.7601991268148,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                    'step': 11346}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:49:21,513\tINFO simple_list_collector.py:661 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-0.681, max=0.627, mean=0.007),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-4.288, max=-3.155, mean=-3.695),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=18.995),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-0.059, max=0.2, mean=0.079),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=267866790.0, max=267866790.0, mean=267866790.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 11201, 'net_worth': 1821.8262873547199}),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'new_obs': np.ndarray((200, 15, 60), dtype=float32, min=-2.335, max=235405.422, mean=2858.783),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'obs': np.ndarray((200, 15, 60), dtype=float32, min=-2.335, max=235405.422, mean=2857.922),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=18.835),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.008, max=0.007, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-0.008, max=0.007, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.827, max=0.734, mean=-0.079),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'state_in_1': np.ndarray((40, 16), dtype=float32, min=-3.064, max=3.433, mean=-0.144),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.858, max=0.734, mean=-0.075),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'state_out_1': np.ndarray((200, 16), dtype=float32, min=-3.231, max=3.433, mean=-0.137),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=56.0, max=56.0, mean=56.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-0.087, max=-0.006, mean=-0.034),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-0.253, max=0.006, mean=-0.113)}}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:49:21,519\tINFO rollout_worker.py:762 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-0.681, max=0.627, mean=0.007),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-4.288, max=-3.155, mean=-3.695),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=18.995),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-0.059, max=0.2, mean=0.079),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=267866790.0, max=267866790.0, mean=267866790.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'obs': np.ndarray((200, 15, 60), dtype=float32, min=-2.335, max=235405.422, mean=2857.922),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=18.835),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.008, max=0.007, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-0.008, max=0.007, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'seq_lens': np.ndarray((40,), dtype=int32, min=5.0, max=5.0, mean=5.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.827, max=0.734, mean=-0.079),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_in_1': np.ndarray((40, 16), dtype=float32, min=-3.064, max=3.433, mean=-0.144),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.858, max=0.734, mean=-0.075),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_out_1': np.ndarray((200, 16), dtype=float32, min=-3.231, max=3.433, mean=-0.137),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=56.0, max=56.0, mean=56.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-0.087, max=-0.006, mean=-0.034),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-0.253, max=0.006, mean=-0.113)}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:49:21,528\tINFO rollout_worker.py:724 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:49:32,545\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/fc_1/kernel:0' shape=(900, 256) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:49:32,547\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/fc_1/bias:0' shape=(256,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:49:32,547\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/fc_2/kernel:0' shape=(256, 256) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:49:32,547\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/fc_2/bias:0' shape=(256,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:49:32,548\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/lstm/lstm_cell/kernel:0' shape=(298, 64) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:49:32,548\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/lstm/lstm_cell/recurrent_kernel:0' shape=(16, 64) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:49:32,548\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/lstm/lstm_cell/bias:0' shape=(64,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:49:32,548\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/logits/kernel:0' shape=(16, 41) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:49:32,548\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/logits/bias:0' shape=(41,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:49:32,548\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/values/kernel:0' shape=(16, 1) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:49:32,548\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/values/bias:0' shape=(1,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:49:32,641\tDEBUG train_ops.py:218 -- == sgd epochs for default_policy ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TradingEnv_5aaa4_00001:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-06_21-49-43\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_media: {}\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 0\n",
      "  experiment_id: ca8561777d5b4b408c18a0ec29d5696b\n",
      "  hostname: Studios-iMac-3.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.670205593109131\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010054918937385082\n",
      "          model: {}\n",
      "          policy_loss: -0.03477063030004501\n",
      "          total_loss: -0.027387380599975586\n",
      "          vf_explained_var: -0.9282627105712891\n",
      "          vf_loss: 0.004366767592728138\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.0.15\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.54632352941177\n",
      "    ram_util_percent: 84.64264705882353\n",
      "  pid: 25226\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf: {}\n",
      "  time_since_restore: 208.7360610961914\n",
      "  time_this_iter_s: 95.52998208999634\n",
      "  time_total_s: 208.7360610961914\n",
      "  timers:\n",
      "    learn_throughput: 300.852\n",
      "    learn_time_ms: 13295.587\n",
      "    load_throughput: 28012.246\n",
      "    load_time_ms: 142.795\n",
      "    sample_throughput: 71.303\n",
      "    sample_time_ms: 56098.648\n",
      "    update_time_ms: 7.108\n",
      "  timestamp: 1625618983\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 5aaa4_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/6.55 GiB heap, 0.0/3.28 GiB objects<br>Result logdir: /Users/leandronogueira/ray_results/PPO<br>Number of trials: 18/300 (16 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  env_config/window_size</th><th style=\"text-align: right;\">  model/lstm_cell_size</th><th>model/lstm_use_prev_action  </th><th>model/lstm_use_prev_reward  </th><th style=\"text-align: right;\">  model/max_seq_len</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00000</td><td>RUNNING </td><td>192.168.0.15:25225</td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         208.121</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">               nan</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00001</td><td>RUNNING </td><td>192.168.0.15:25226</td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         208.736</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">               nan</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00002</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00003</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00004</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00005</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00006</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00007</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00008</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00009</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00010</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00011</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00012</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00013</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00014</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00015</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00016</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00017</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:50:06,981\tINFO sampler.py:813 -- Preprocessed obs: np.ndarray((10, 60), dtype=float32, min=-1.398, max=46061.191, mean=2162.875)\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:50:06,982\tINFO sampler.py:817 -- Filtered obs: np.ndarray((10, 60), dtype=float32, min=-1.398, max=46061.191, mean=2162.875)\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:50:06,983\tINFO sampler.py:1005 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'info': { 'net_worth': 1768.0957851324947,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                             'step': 15758},\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'obs': np.ndarray((10, 60), dtype=float32, min=-1.398, max=46061.191, mean=2162.875),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'prev_action': 34,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'prev_reward': 9.622960383470769e-05,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'rnn_state': [ np.ndarray((16,), dtype=float32, min=-0.342, max=0.946, mean=0.134),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                                  np.ndarray((16,), dtype=float32, min=-1.031, max=4.059, mean=0.528)]},\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:50:06,984\tINFO tf_run_builder.py:87 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:50:06,988\tINFO sampler.py:1023 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=7.0, max=7.0, mean=7.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                       [ np.ndarray((1, 16), dtype=float32, min=-0.564, max=0.963, mean=0.087),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         np.ndarray((1, 16), dtype=float32, min=-0.882, max=4.21, mean=0.463)],\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 41), dtype=float32, min=-0.935, max=0.592, mean=-0.037),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-3.575, max=-3.575, mean=-3.575),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.028, max=0.028, mean=0.028),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.064, max=-0.064, mean=-0.064)})}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:50:07,013\tINFO sampler.py:591 -- Raw obs from env: { 0: { 'agent0': np.ndarray((10, 60), dtype=float32, min=-1.398, max=46061.191, mean=2209.593)}}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:50:07,014\tINFO sampler.py:592 -- Info return from env: { 0: { 'agent0': { 'net_worth': 1767.5960376994658,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                    'step': 15759}}}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:50:08,065\tINFO simple_list_collector.py:661 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-1.068, max=0.959, mean=-0.014),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-4.737, max=-2.878, mean=-3.597),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=20.005),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-0.069, max=0.153, mean=0.04),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=772733759.0, max=772733759.0, mean=772733759.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 15601, 'net_worth': 1726.0425702893099}),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'new_obs': np.ndarray((200, 10, 60), dtype=float32, min=-2.628, max=121263.781, mean=2265.291),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'obs': np.ndarray((200, 10, 60), dtype=float32, min=-8084.638, max=121263.781, mean=2260.479),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.94),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.008, max=0.007, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-0.008, max=0.007, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.806, max=0.977, mean=0.062),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_in_1': np.ndarray((40, 16), dtype=float32, min=-1.506, max=4.04, mean=0.263),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.838, max=0.989, mean=0.055),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_out_1': np.ndarray((200, 16), dtype=float32, min=-1.554, max=4.258, mean=0.248),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=78.0, max=78.0, mean=78.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-0.003, max=0.026, mean=0.014),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-0.142, max=0.085, mean=-0.026)}}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:50:08,070\tINFO rollout_worker.py:762 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-1.068, max=0.959, mean=-0.014),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-4.737, max=-2.878, mean=-3.597),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=20.005),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-0.069, max=0.153, mean=0.04),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=772733759.0, max=772733759.0, mean=772733759.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'obs': np.ndarray((200, 10, 60), dtype=float32, min=-8084.638, max=121263.781, mean=2260.479),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.94),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.008, max=0.007, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-0.008, max=0.007, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'seq_lens': np.ndarray((40,), dtype=int32, min=5.0, max=5.0, mean=5.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.806, max=0.977, mean=0.062),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_in_1': np.ndarray((40, 16), dtype=float32, min=-1.506, max=4.04, mean=0.263),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.838, max=0.989, mean=0.055),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_out_1': np.ndarray((200, 16), dtype=float32, min=-1.554, max=4.258, mean=0.248),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=78.0, max=78.0, mean=78.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-0.003, max=0.026, mean=0.014),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-0.142, max=0.085, mean=-0.026)}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:50:08,074\tINFO rollout_worker.py:724 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:50:12,863\tINFO rnn_sequencing.py:139 -- Padded input for RNN/Attn.Nets/MA:\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m { 'features': [ np.ndarray((4000, 10, 60), dtype=float32, min=-31133.604, max=825545.562, mean=2848.07),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000,), dtype=int64, min=0.0, max=40.0, mean=20.355),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000,), dtype=int64, min=0.0, max=40.0, mean=20.352),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-0.089, max=0.035, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-0.089, max=0.035, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-4.776, max=-2.687, mean=-3.665),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000, 41), dtype=float32, min=-1.338, max=1.18, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-0.217, max=0.265, mean=-0.019),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-6.401, max=3.656, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-0.112, max=0.112, mean=-0.015)],\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'initial_states': [ np.ndarray((800, 16), dtype=float32, min=-0.941, max=0.985, mean=0.017),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                       np.ndarray((800, 16), dtype=float32, min=-2.317, max=4.747, mean=0.12)],\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'max_seq_len': 5,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'seq_lens': np.ndarray((800,), dtype=int32, min=5.0, max=5.0, mean=5.0)}\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:50:12,873\tINFO multi_gpu_impl.py:143 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m { 'inputs': [ np.ndarray((4000, 10, 60), dtype=float32, min=-31133.604, max=825545.562, mean=2848.07),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000,), dtype=int64, min=0.0, max=40.0, mean=20.355),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000,), dtype=int64, min=0.0, max=40.0, mean=20.352),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.089, max=0.035, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.089, max=0.035, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-4.776, max=-2.687, mean=-3.665),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000, 41), dtype=float32, min=-1.338, max=1.18, mean=-0.001),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.217, max=0.265, mean=-0.019),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-6.401, max=3.656, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.112, max=0.112, mean=-0.015)],\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/obs:0' shape=(?, 10, 60) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/action:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/prev_actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/prev_rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/action_logp:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/action_dist_inputs:0' shape=(?, 41) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/Placeholder:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/Placeholder_1:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     <tf.Tensor 'default_policy/seq_lens:0' shape=(?,) dtype=int32>],\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m   'state_inputs': [ np.ndarray((800, 16), dtype=float32, min=-0.941, max=0.985, mean=0.017),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     np.ndarray((800, 16), dtype=float32, min=-2.317, max=4.747, mean=0.12),\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m                     np.ndarray((800,), dtype=int32, min=5.0, max=5.0, mean=5.0)]}\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:50:12,873\tINFO multi_gpu_impl.py:188 -- Divided 800 rollout sequences, each of length 5, among 1 devices.\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:50:12,881\tDEBUG train_ops.py:218 -- == sgd epochs for default_policy ==\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:50:21,529\tINFO sampler.py:813 -- Preprocessed obs: np.ndarray((15, 60), dtype=float32, min=-0.825, max=60952.379, mean=2824.426)\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:50:21,530\tINFO sampler.py:817 -- Filtered obs: np.ndarray((15, 60), dtype=float32, min=-0.825, max=60952.379, mean=2824.426)\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:50:21,531\tINFO sampler.py:1005 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'info': { 'net_worth': 1740.4357949946002,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                             'step': 13784},\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'obs': np.ndarray((15, 60), dtype=float32, min=-0.825, max=60952.379, mean=2824.426),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'prev_action': 14,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'prev_reward': 0.002466720537575018,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'rnn_state': [ np.ndarray((16,), dtype=float32, min=-0.794, max=0.685, mean=-0.046),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                                  np.ndarray((16,), dtype=float32, min=-1.496, max=3.744, mean=0.058)]},\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:50:21,533\tINFO tf_run_builder.py:87 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:50:21,539\tINFO sampler.py:1023 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=9.0, max=9.0, mean=9.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                       [ np.ndarray((1, 16), dtype=float32, min=-0.827, max=0.61, mean=-0.039),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         np.ndarray((1, 16), dtype=float32, min=-1.666, max=3.803, mean=0.074)],\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 41), dtype=float32, min=-0.675, max=0.631, mean=-0.011),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-3.802, max=-3.802, mean=-3.802),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.022, max=0.022, mean=0.022),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.095, max=0.095, mean=0.095)})}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:50:21,560\tINFO sampler.py:591 -- Raw obs from env: { 0: { 'agent0': np.ndarray((15, 60), dtype=float32, min=-0.825, max=60952.379, mean=2875.394)}}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:50:21,560\tINFO sampler.py:592 -- Info return from env: { 0: { 'agent0': { 'net_worth': 1742.6509353726,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                    'step': 13785}}}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:50:21,951\tINFO simple_list_collector.py:661 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-0.825, max=0.929, mean=-0.014),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-4.437, max=-3.02, mean=-3.677),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.13),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-0.227, max=0.25, mean=0.014),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=267866790.0, max=267866790.0, mean=267866790.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 13601, 'net_worth': 1668.369894697}),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'new_obs': np.ndarray((200, 15, 60), dtype=float32, min=-1.844, max=109697.43, mean=2728.296),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'obs': np.ndarray((200, 15, 60), dtype=float32, min=-1.844, max=109697.43, mean=2725.712),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.105),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.016, max=0.013, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-0.016, max=0.013, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.87, max=0.915, mean=-0.011),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'state_in_1': np.ndarray((40, 16), dtype=float32, min=-1.991, max=4.097, mean=0.11),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.87, max=0.941, mean=-0.009),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'state_out_1': np.ndarray((200, 16), dtype=float32, min=-2.153, max=4.216, mean=0.111),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=68.0, max=68.0, mean=68.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-0.021, max=0.035, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-0.247, max=0.232, mean=-0.013)}}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:50:21,956\tINFO rollout_worker.py:762 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-0.825, max=0.929, mean=-0.014),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-4.437, max=-3.02, mean=-3.677),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.13),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-0.227, max=0.25, mean=0.014),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=267866790.0, max=267866790.0, mean=267866790.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'obs': np.ndarray((200, 15, 60), dtype=float32, min=-1.844, max=109697.43, mean=2725.712),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.105),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.016, max=0.013, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-0.016, max=0.013, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'seq_lens': np.ndarray((40,), dtype=int32, min=5.0, max=5.0, mean=5.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.87, max=0.915, mean=-0.011),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_in_1': np.ndarray((40, 16), dtype=float32, min=-1.991, max=4.097, mean=0.11),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.87, max=0.941, mean=-0.009),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_out_1': np.ndarray((200, 16), dtype=float32, min=-2.153, max=4.216, mean=0.111),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=68.0, max=68.0, mean=68.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-0.021, max=0.035, mean=0.002),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-0.247, max=0.232, mean=-0.013)}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:50:21,961\tINFO rollout_worker.py:724 -- Generating sample batch of size 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TradingEnv_5aaa4_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-06_21-50-22\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_media: {}\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 0\n",
      "  experiment_id: 834d67c432884568b80056d0e12e90c1\n",
      "  hostname: Studios-iMac-3.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.652282953262329\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013564432971179485\n",
      "          model: {}\n",
      "          policy_loss: -0.03976481035351753\n",
      "          total_loss: -0.03343893587589264\n",
      "          vf_explained_var: -0.962289035320282\n",
      "          vf_loss: 0.0022565315011888742\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.15\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 85.88238993710691\n",
      "    ram_util_percent: 84.21194968553459\n",
      "  pid: 25225\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf: {}\n",
      "  time_since_restore: 319.7155816555023\n",
      "  time_this_iter_s: 111.59487199783325\n",
      "  time_total_s: 319.7155816555023\n",
      "  timers:\n",
      "    learn_throughput: 334.21\n",
      "    learn_time_ms: 11968.522\n",
      "    load_throughput: 33833.866\n",
      "    load_time_ms: 118.225\n",
      "    sample_throughput: 59.102\n",
      "    sample_time_ms: 67680.087\n",
      "    update_time_ms: 11.055\n",
      "  timestamp: 1625619022\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 5aaa4_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/6.55 GiB heap, 0.0/3.28 GiB objects<br>Result logdir: /Users/leandronogueira/ray_results/PPO<br>Number of trials: 18/300 (16 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  env_config/window_size</th><th style=\"text-align: right;\">  model/lstm_cell_size</th><th>model/lstm_use_prev_action  </th><th>model/lstm_use_prev_reward  </th><th style=\"text-align: right;\">  model/max_seq_len</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00000</td><td>RUNNING </td><td>192.168.0.15:25225</td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         319.716</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">               nan</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00001</td><td>RUNNING </td><td>192.168.0.15:25226</td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         208.736</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">               nan</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00002</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00003</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00004</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00005</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00006</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00007</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00008</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00009</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00010</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00011</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00012</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00013</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00014</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00015</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00016</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00017</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:51:08,105\tINFO sampler.py:813 -- Preprocessed obs: np.ndarray((10, 60), dtype=float32, min=-1.921, max=23965.301, mean=1763.399)\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:51:08,106\tINFO sampler.py:817 -- Filtered obs: np.ndarray((10, 60), dtype=float32, min=-1.921, max=23965.301, mean=1763.399)\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:51:08,107\tINFO sampler.py:1005 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'info': { 'net_worth': 1637.4384137056788,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                             'step': 17756},\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'obs': np.ndarray((10, 60), dtype=float32, min=-1.921, max=23965.301, mean=1763.399),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'prev_action': 9,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'prev_reward': 5.1951762788160494e-05,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'rnn_state': [ np.ndarray((16,), dtype=float32, min=-0.721, max=0.978, mean=0.148),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                                  np.ndarray((16,), dtype=float32, min=-0.963, max=6.365, mean=0.832)]},\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:51:08,108\tINFO tf_run_builder.py:87 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:51:08,111\tINFO sampler.py:1023 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=29.0, max=29.0, mean=29.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                       [ np.ndarray((1, 16), dtype=float32, min=-0.81, max=0.978, mean=0.111),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         np.ndarray((1, 16), dtype=float32, min=-1.236, max=6.035, mean=0.734)],\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 41), dtype=float32, min=-0.965, max=0.768, mean=-0.044),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-3.581, max=-3.581, mean=-3.581),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.028, max=0.028, mean=0.028),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.003, max=0.003, mean=0.003)})}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:51:08,134\tINFO sampler.py:591 -- Raw obs from env: { 0: { 'agent0': np.ndarray((10, 60), dtype=float32, min=-1.921, max=23965.301, mean=1791.963)}}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:51:08,134\tINFO sampler.py:592 -- Info return from env: { 0: { 'agent0': { 'net_worth': 1636.970565044971,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                    'step': 17757}}}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:51:10,939\tINFO simple_list_collector.py:661 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-1.214, max=1.014, mean=-0.033),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-4.563, max=-2.862, mean=-3.643),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.555),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-0.093, max=0.161, mean=0.016),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=772733759.0, max=772733759.0, mean=772733759.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 17601, 'net_worth': 1633.557396406624}),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'new_obs': np.ndarray((200, 10, 60), dtype=float32, min=-119.43, max=102601.742, mean=1984.293),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'obs': np.ndarray((200, 10, 60), dtype=float32, min=-119.43, max=102601.742, mean=1982.912),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.59),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.006, max=0.01, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-0.006, max=0.01, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.916, max=0.986, mean=0.103),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_in_1': np.ndarray((40, 16), dtype=float32, min=-1.801, max=6.215, mean=0.463),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.916, max=0.988, mean=0.101),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_out_1': np.ndarray((200, 16), dtype=float32, min=-1.801, max=6.365, mean=0.456),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=88.0, max=88.0, mean=88.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-0.004, max=0.016, mean=0.006),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-0.156, max=0.1, mean=-0.01)}}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:51:10,965\tINFO rollout_worker.py:762 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-1.214, max=1.014, mean=-0.033),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-4.563, max=-2.862, mean=-3.643),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.555),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-0.093, max=0.161, mean=0.016),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=772733759.0, max=772733759.0, mean=772733759.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'obs': np.ndarray((200, 10, 60), dtype=float32, min=-119.43, max=102601.742, mean=1982.912),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.59),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.006, max=0.01, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-0.006, max=0.01, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'seq_lens': np.ndarray((40,), dtype=int32, min=5.0, max=5.0, mean=5.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.916, max=0.986, mean=0.103),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_in_1': np.ndarray((40, 16), dtype=float32, min=-1.801, max=6.215, mean=0.463),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.916, max=0.988, mean=0.101),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_out_1': np.ndarray((200, 16), dtype=float32, min=-1.801, max=6.365, mean=0.456),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=88.0, max=88.0, mean=88.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-0.004, max=0.016, mean=0.006),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-0.156, max=0.1, mean=-0.01)}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:51:10,980\tINFO rollout_worker.py:724 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:51:12,225\tINFO rnn_sequencing.py:139 -- Padded input for RNN/Attn.Nets/MA:\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m { 'features': [ np.ndarray((4000, 15, 60), dtype=float32, min=-31133.604, max=825545.562, mean=2849.581),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000,), dtype=int64, min=0.0, max=40.0, mean=19.686),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000,), dtype=int64, min=0.0, max=40.0, mean=19.687),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-0.127, max=0.029, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-0.127, max=0.029, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-4.583, max=-2.834, mean=-3.668),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000, 41), dtype=float32, min=-0.941, max=1.046, mean=-0.015),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-0.335, max=0.429, mean=0.013),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-4.624, max=3.179, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                 np.ndarray((4000,), dtype=float32, min=-0.139, max=0.227, mean=0.004)],\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'initial_states': [ np.ndarray((800, 16), dtype=float32, min=-0.911, max=0.968, mean=0.006),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                       np.ndarray((800, 16), dtype=float32, min=-5.997, max=5.349, mean=0.119)],\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'max_seq_len': 5,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'seq_lens': np.ndarray((800,), dtype=int32, min=5.0, max=5.0, mean=5.0)}\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:51:12,288\tINFO multi_gpu_impl.py:143 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m { 'inputs': [ np.ndarray((4000, 15, 60), dtype=float32, min=-31133.604, max=825545.562, mean=2849.581),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000,), dtype=int64, min=0.0, max=40.0, mean=19.686),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000,), dtype=int64, min=0.0, max=40.0, mean=19.687),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.127, max=0.029, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.127, max=0.029, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-4.583, max=-2.834, mean=-3.668),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000, 41), dtype=float32, min=-0.941, max=1.046, mean=-0.015),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.335, max=0.429, mean=0.013),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-4.624, max=3.179, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m               np.ndarray((4000,), dtype=float32, min=-0.139, max=0.227, mean=0.004)],\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/obs:0' shape=(?, 15, 60) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/action:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/prev_actions:0' shape=(?,) dtype=int64>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/prev_rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/action_logp:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/action_dist_inputs:0' shape=(?, 41) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/Placeholder:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/Placeholder_1:0' shape=(?, 16) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     <tf.Tensor 'default_policy/seq_lens:0' shape=(?,) dtype=int32>],\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m   'state_inputs': [ np.ndarray((800, 16), dtype=float32, min=-0.911, max=0.968, mean=0.006),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     np.ndarray((800, 16), dtype=float32, min=-5.997, max=5.349, mean=0.119),\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m                     np.ndarray((800,), dtype=int32, min=5.0, max=5.0, mean=5.0)]}\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:51:12,289\tINFO multi_gpu_impl.py:188 -- Divided 800 rollout sequences, each of length 5, among 1 devices.\n",
      "\u001b[2m\u001b[36m(pid=25226)\u001b[0m 2021-07-06 21:51:12,290\tDEBUG train_ops.py:218 -- == sgd epochs for default_policy ==\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TradingEnv_5aaa4_00001:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-06_21-51-23\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_media: {}\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 0\n",
      "  experiment_id: ca8561777d5b4b408c18a0ec29d5696b\n",
      "  hostname: Studios-iMac-3.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.664738416671753\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010344280861318111\n",
      "          model: {}\n",
      "          policy_loss: -0.03738341107964516\n",
      "          total_loss: -0.030311385169625282\n",
      "          vf_explained_var: -0.8050626516342163\n",
      "          vf_loss: 0.003968733828514814\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.0.15\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.00633802816901\n",
      "    ram_util_percent: 82.99718309859153\n",
      "  pid: 25226\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf: {}\n",
      "  time_since_restore: 308.2392568588257\n",
      "  time_this_iter_s: 99.50319576263428\n",
      "  time_total_s: 308.2392568588257\n",
      "  timers:\n",
      "    learn_throughput: 313.26\n",
      "    learn_time_ms: 12768.937\n",
      "    load_throughput: 27518.971\n",
      "    load_time_ms: 145.354\n",
      "    sample_throughput: 62.398\n",
      "    sample_time_ms: 64104.585\n",
      "    update_time_ms: 6.92\n",
      "  timestamp: 1625619083\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 5aaa4_00001\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.1/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/6.55 GiB heap, 0.0/3.28 GiB objects<br>Result logdir: /Users/leandronogueira/ray_results/PPO<br>Number of trials: 18/300 (16 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  env_config/window_size</th><th style=\"text-align: right;\">  model/lstm_cell_size</th><th>model/lstm_use_prev_action  </th><th>model/lstm_use_prev_reward  </th><th style=\"text-align: right;\">  model/max_seq_len</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00000</td><td>RUNNING </td><td>192.168.0.15:25225</td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         319.716</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">               nan</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00001</td><td>RUNNING </td><td>192.168.0.15:25226</td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         308.239</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">               nan</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00002</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00003</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00004</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00005</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00006</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00007</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00008</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00009</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00010</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00011</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00012</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00013</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00014</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00015</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00016</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00017</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:51:23,690\tINFO sampler.py:1005 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'info': { 'net_worth': 1756.7539957792,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                             'step': 16000},\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'obs': np.ndarray((15, 60), dtype=float32, min=-1.333, max=87865.602, mean=2308.978),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'prev_action': 14,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'prev_reward': 0.00033636057854025125,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'rnn_state': [ np.ndarray((16,), dtype=float32, min=-0.262, max=0.8, mean=0.134),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                                  np.ndarray((16,), dtype=float32, min=-0.811, max=2.236, mean=0.258)]},\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:51:23,690\tINFO tf_run_builder.py:87 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:51:23,694\tINFO sampler.py:1023 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=21.0, max=21.0, mean=21.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                       [ np.ndarray((1, 16), dtype=float32, min=-0.429, max=0.682, mean=0.075),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         np.ndarray((1, 16), dtype=float32, min=-1.197, max=1.698, mean=0.059)],\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 41), dtype=float32, min=-0.618, max=0.713, mean=-0.054),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-3.436, max=-3.436, mean=-3.436),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.032, max=0.032, mean=0.032),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.121, max=0.121, mean=0.121)})}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:51:23,727\tINFO sampler.py:591 -- Raw obs from env: { 0: { 'agent0': np.ndarray((15, 60), dtype=float32, min=-1.333, max=87865.602, mean=2323.535)}}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:51:23,727\tINFO sampler.py:592 -- Info return from env: { 0: { 'agent0': { 'net_worth': 1760.9627624974,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                    'step': 16001}}}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:51:23,727\tINFO sampler.py:813 -- Preprocessed obs: np.ndarray((15, 60), dtype=float32, min=-1.333, max=87865.602, mean=2323.535)\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:51:23,728\tINFO sampler.py:817 -- Filtered obs: np.ndarray((15, 60), dtype=float32, min=-1.333, max=87865.602, mean=2323.535)\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:51:28,216\tINFO simple_list_collector.py:661 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-0.788, max=0.956, mean=-0.043),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-4.366, max=-2.884, mean=-3.633),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=20.38),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-0.276, max=0.223, mean=0.021),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=267866790.0, max=267866790.0, mean=267866790.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 16001, 'net_worth': 1760.9627624974}),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'new_obs': np.ndarray((200, 15, 60), dtype=float32, min=-149.327, max=99506.281, mean=2161.466),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'obs': np.ndarray((200, 15, 60), dtype=float32, min=-149.327, max=99506.281, mean=2163.422),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=20.345),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.009, max=0.011, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-0.009, max=0.011, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.663, max=0.892, mean=0.086),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'state_in_1': np.ndarray((40, 16), dtype=float32, min=-3.237, max=2.952, mean=0.09),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.764, max=0.898, mean=0.086),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'state_out_1': np.ndarray((200, 16), dtype=float32, min=-3.237, max=3.244, mean=0.088),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=80.0, max=80.0, mean=80.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=0.019, max=0.169, mean=0.075),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-0.196, max=0.315, mean=0.053)}}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:51:28,222\tINFO rollout_worker.py:762 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-0.788, max=0.956, mean=-0.043),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-4.366, max=-2.884, mean=-3.633),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=20.38),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-0.276, max=0.223, mean=0.021),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=267866790.0, max=267866790.0, mean=267866790.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'obs': np.ndarray((200, 15, 60), dtype=float32, min=-149.327, max=99506.281, mean=2163.422),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=20.345),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.009, max=0.011, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-0.009, max=0.011, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'seq_lens': np.ndarray((40,), dtype=int32, min=5.0, max=5.0, mean=5.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.663, max=0.892, mean=0.086),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_in_1': np.ndarray((40, 16), dtype=float32, min=-3.237, max=2.952, mean=0.09),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.764, max=0.898, mean=0.086),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_out_1': np.ndarray((200, 16), dtype=float32, min=-3.237, max=3.244, mean=0.088),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=80.0, max=80.0, mean=80.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=0.019, max=0.169, mean=0.075),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-0.196, max=0.315, mean=0.053)}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:51:28,226\tINFO rollout_worker.py:724 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:52:10,981\tINFO sampler.py:813 -- Preprocessed obs: np.ndarray((10, 60), dtype=float32, min=-1.981, max=65928.867, mean=2585.951)\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:52:10,981\tINFO sampler.py:817 -- Filtered obs: np.ndarray((10, 60), dtype=float32, min=-1.981, max=65928.867, mean=2585.951)\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:52:10,983\tINFO sampler.py:1005 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'info': { 'net_worth': 2037.9824920301978,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                             'step': 19712},\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'obs': np.ndarray((10, 60), dtype=float32, min=-1.981, max=65928.867, mean=2585.951),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'prev_action': 3,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'prev_reward': -0.006142301856201682,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                   'rnn_state': [ np.ndarray((16,), dtype=float32, min=-0.881, max=0.726, mean=-0.145),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                                                  np.ndarray((16,), dtype=float32, min=-1.857, max=0.952, mean=-0.266)]},\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:52:10,983\tINFO tf_run_builder.py:87 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:52:10,986\tINFO sampler.py:1023 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=13.0, max=13.0, mean=13.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                       [ np.ndarray((1, 16), dtype=float32, min=-0.831, max=0.848, mean=-0.089),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         np.ndarray((1, 16), dtype=float32, min=-1.407, max=1.308, mean=-0.209)],\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 41), dtype=float32, min=-0.899, max=0.856, mean=-0.056),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-3.59, max=-3.59, mean=-3.59),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.028, max=0.028, mean=0.028),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=-0.144, max=-0.144, mean=-0.144)})}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:52:11,012\tINFO sampler.py:591 -- Raw obs from env: { 0: { 'agent0': np.ndarray((10, 60), dtype=float32, min=-1.981, max=65928.867, mean=2576.032)}}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:52:11,012\tINFO sampler.py:592 -- Info return from env: { 0: { 'agent0': { 'net_worth': 2060.178720767369,\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m                    'step': 19713}}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:52:13,570\tINFO simple_list_collector.py:661 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-1.182, max=1.166, mean=-0.01),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-4.76, max=-2.847, mean=-3.625),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.4),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-0.087, max=0.166, mean=0.039),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=772733759.0, max=772733759.0, mean=772733759.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 19601, 'net_worth': 1968.613086037659}),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'new_obs': np.ndarray((200, 10, 60), dtype=float32, min=-3.239, max=165496.125, mean=2776.183),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'obs': np.ndarray((200, 10, 60), dtype=float32, min=-3.239, max=165496.125, mean=2770.491),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.4),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.012, max=0.015, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-0.012, max=0.015, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.939, max=0.98, mean=-0.025),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_in_1': np.ndarray((40, 16), dtype=float32, min=-3.071, max=4.566, mean=0.051),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.963, max=0.981, mean=-0.026),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'state_out_1': np.ndarray((200, 16), dtype=float32, min=-3.161, max=4.979, mean=0.05),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=98.0, max=98.0, mean=98.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-0.022, max=0.059, mean=0.014),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-0.144, max=0.078, mean=-0.024)}}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:52:13,576\tINFO rollout_worker.py:762 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-1.182, max=1.166, mean=-0.01),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-4.76, max=-2.847, mean=-3.625),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.4),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-0.087, max=0.166, mean=0.039),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=772733759.0, max=772733759.0, mean=772733759.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'obs': np.ndarray((200, 10, 60), dtype=float32, min=-3.239, max=165496.125, mean=2770.491),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.4),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.012, max=0.015, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-0.012, max=0.015, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'seq_lens': np.ndarray((40,), dtype=int32, min=5.0, max=5.0, mean=5.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.939, max=0.98, mean=-0.025),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_in_1': np.ndarray((40, 16), dtype=float32, min=-3.071, max=4.566, mean=0.051),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.963, max=0.981, mean=-0.026),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'state_out_1': np.ndarray((200, 16), dtype=float32, min=-3.161, max=4.979, mean=0.05),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=98.0, max=98.0, mean=98.0),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-0.022, max=0.059, mean=0.014),\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-0.144, max=0.078, mean=-0.024)}\n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25227)\u001b[0m 2021-07-06 21:52:13,581\tINFO rollout_worker.py:724 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:52:19,215\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/fc_1/kernel:0' shape=(600, 256) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:52:19,215\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/fc_1/bias:0' shape=(256,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:52:19,215\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/fc_2/kernel:0' shape=(256, 256) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:52:19,216\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/fc_2/bias:0' shape=(256,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:52:19,216\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/lstm/lstm_cell/kernel:0' shape=(298, 64) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:52:19,216\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/lstm/lstm_cell/recurrent_kernel:0' shape=(16, 64) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:52:19,216\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/lstm/lstm_cell/bias:0' shape=(64,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:52:19,217\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/logits/kernel:0' shape=(16, 41) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:52:19,217\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/logits/bias:0' shape=(41,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:52:19,217\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/values/kernel:0' shape=(16, 1) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:52:19,217\tINFO tf_policy.py:692 -- Optimizing variable <tf.Variable 'default_policy/values/bias:0' shape=(1,) dtype=float32>\n",
      "\u001b[2m\u001b[36m(pid=25225)\u001b[0m 2021-07-06 21:52:19,291\tDEBUG train_ops.py:218 -- == sgd epochs for default_policy ==\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:52:28,250\tINFO sampler.py:813 -- Preprocessed obs: np.ndarray((15, 60), dtype=float32, min=-2.488, max=72853.023, mean=2445.9)\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:52:28,252\tINFO sampler.py:817 -- Filtered obs: np.ndarray((15, 60), dtype=float32, min=-2.488, max=72853.023, mean=2445.9)\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:52:28,253\tINFO sampler.py:1005 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'info': { 'net_worth': 1591.8272379982,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                             'step': 18503},\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'obs': np.ndarray((15, 60), dtype=float32, min=-2.488, max=72853.023, mean=2445.9),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'prev_action': 8,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'prev_reward': -0.0006702904807956989,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                   'rnn_state': [ np.ndarray((16,), dtype=float32, min=-0.641, max=0.751, mean=0.006),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                                                  np.ndarray((16,), dtype=float32, min=-2.183, max=4.411, mean=0.047)]},\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:52:28,253\tINFO tf_run_builder.py:87 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:52:28,270\tINFO sampler.py:1023 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'default_policy': ( np.ndarray((1,), dtype=int64, min=33.0, max=33.0, mean=33.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                       [ np.ndarray((1, 16), dtype=float32, min=-0.624, max=0.886, mean=0.016),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         np.ndarray((1, 16), dtype=float32, min=-2.307, max=4.299, mean=0.031)],\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                       { 'action_dist_inputs': np.ndarray((1, 41), dtype=float32, min=-0.629, max=0.667, mean=-0.023),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         'action_logp': np.ndarray((1,), dtype=float32, min=-3.773, max=-3.773, mean=-3.773),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         'action_prob': np.ndarray((1,), dtype=float32, min=0.023, max=0.023, mean=0.023),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.059, max=0.059, mean=0.059)})}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:52:28,335\tINFO sampler.py:591 -- Raw obs from env: { 0: { 'agent0': np.ndarray((15, 60), dtype=float32, min=-2.488, max=72853.023, mean=2415.925)}}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:52:28,336\tINFO sampler.py:592 -- Info return from env: { 0: { 'agent0': { 'net_worth': 1591.1549809848,\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m                    'step': 18504}}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TradingEnv_5aaa4_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-06_21-52-30\n",
      "  done: false\n",
      "  episode_len_mean: .nan\n",
      "  episode_media: {}\n",
      "  episode_reward_max: .nan\n",
      "  episode_reward_mean: .nan\n",
      "  episode_reward_min: .nan\n",
      "  episodes_this_iter: 0\n",
      "  episodes_total: 0\n",
      "  experiment_id: 834d67c432884568b80056d0e12e90c1\n",
      "  hostname: Studios-iMac-3.local\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 3.6438608169555664\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014634430408477783\n",
      "          model: {}\n",
      "          policy_loss: -0.04408232122659683\n",
      "          total_loss: -0.03751102834939957\n",
      "          vf_explained_var: -1.0\n",
      "          vf_loss: 0.0021809665486216545\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.0.15\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 82.39010989010988\n",
      "    ram_util_percent: 83.13516483516483\n",
      "  pid: 25225\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf: {}\n",
      "  time_since_restore: 447.03832840919495\n",
      "  time_this_iter_s: 127.32274675369263\n",
      "  time_total_s: 447.03832840919495\n",
      "  timers:\n",
      "    learn_throughput: 341.648\n",
      "    learn_time_ms: 11707.97\n",
      "    load_throughput: 36375.939\n",
      "    load_time_ms: 109.963\n",
      "    sample_throughput: 51.648\n",
      "    sample_time_ms: 77447.919\n",
      "    update_time_ms: 18.245\n",
      "  timestamp: 1625619150\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 5aaa4_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/16.0 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/4 CPUs, 0/0 GPUs, 0.0/6.55 GiB heap, 0.0/3.28 GiB objects<br>Result logdir: /Users/leandronogueira/ray_results/PPO<br>Number of trials: 18/300 (16 PENDING, 2 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  env_config/window_size</th><th style=\"text-align: right;\">  model/lstm_cell_size</th><th>model/lstm_use_prev_action  </th><th>model/lstm_use_prev_reward  </th><th style=\"text-align: right;\">  model/max_seq_len</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00000</td><td>RUNNING </td><td>192.168.0.15:25225</td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         447.038</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">               nan</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00001</td><td>RUNNING </td><td>192.168.0.15:25226</td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         308.239</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">     nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">                 nan</td><td style=\"text-align: right;\">               nan</td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00002</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    16</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00003</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00004</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00005</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    32</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00006</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00007</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00008</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    62</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00009</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00010</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00011</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                   128</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00012</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00013</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00014</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                   256</td><td>True                        </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00015</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      10</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00016</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      15</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_TradingEnv_5aaa4_00017</td><td>PENDING </td><td>                  </td><td style=\"text-align: right;\">                      30</td><td style=\"text-align: right;\">                    16</td><td>False                       </td><td>True                        </td><td style=\"text-align: right;\">                  5</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">     </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:52:31,907\tINFO simple_list_collector.py:661 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'agent0': { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-0.88, max=0.881, mean=-0.038),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'action_logp': np.ndarray((200,), dtype=float32, min=-4.44, max=-2.899, mean=-3.656),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.55),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'advantages': np.ndarray((200,), dtype=float32, min=-0.195, max=0.148, mean=-0.035),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'eps_id': np.ndarray((200,), dtype=int64, min=267866790.0, max=267866790.0, mean=267866790.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'infos': np.ndarray((200,), dtype=object, head={'step': 18401, 'net_worth': 1593.30070158}),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'new_obs': np.ndarray((200, 15, 60), dtype=float32, min=-125.478, max=138222.547, mean=2555.633),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'obs': np.ndarray((200, 15, 60), dtype=float32, min=-125.478, max=138222.547, mean=2555.251),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.58),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.005, max=0.006, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'rewards': np.ndarray((200,), dtype=float32, min=-0.005, max=0.006, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.842, max=0.87, mean=0.047),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'state_in_1': np.ndarray((40, 16), dtype=float32, min=-2.668, max=4.538, mean=0.064),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.863, max=0.908, mean=0.045),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'state_out_1': np.ndarray((200, 16), dtype=float32, min=-2.968, max=4.784, mean=0.054),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'unroll_id': np.ndarray((200,), dtype=int64, min=92.0, max=92.0, mean=92.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'value_targets': np.ndarray((200,), dtype=float32, min=-0.005, max=0.008, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m               'vf_preds': np.ndarray((200,), dtype=float32, min=-0.144, max=0.195, mean=0.037)}}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:52:31,913\tINFO rollout_worker.py:762 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m { 'action_dist_inputs': np.ndarray((200, 41), dtype=float32, min=-0.88, max=0.881, mean=-0.038),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'action_logp': np.ndarray((200,), dtype=float32, min=-4.44, max=-2.899, mean=-3.656),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.55),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'advantages': np.ndarray((200,), dtype=float32, min=-0.195, max=0.148, mean=-0.035),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'dones': np.ndarray((200,), dtype=bool, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'eps_id': np.ndarray((200,), dtype=int64, min=267866790.0, max=267866790.0, mean=267866790.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'obs': np.ndarray((200, 15, 60), dtype=float32, min=-125.478, max=138222.547, mean=2555.251),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'prev_actions': np.ndarray((200,), dtype=int64, min=0.0, max=40.0, mean=19.58),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'prev_rewards': np.ndarray((200,), dtype=float32, min=-0.005, max=0.006, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'rewards': np.ndarray((200,), dtype=float32, min=-0.005, max=0.006, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'seq_lens': np.ndarray((40,), dtype=int32, min=5.0, max=5.0, mean=5.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_in_0': np.ndarray((40, 16), dtype=float32, min=-0.842, max=0.87, mean=0.047),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_in_1': np.ndarray((40, 16), dtype=float32, min=-2.668, max=4.538, mean=0.064),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_out_0': np.ndarray((200, 16), dtype=float32, min=-0.863, max=0.908, mean=0.045),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'state_out_1': np.ndarray((200, 16), dtype=float32, min=-2.968, max=4.784, mean=0.054),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'unroll_id': np.ndarray((200,), dtype=int64, min=92.0, max=92.0, mean=92.0),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'value_targets': np.ndarray((200,), dtype=float32, min=-0.005, max=0.008, mean=0.001),\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m   'vf_preds': np.ndarray((200,), dtype=float32, min=-0.144, max=0.195, mean=0.037)}\n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=25228)\u001b[0m 2021-07-06 21:52:31,917\tINFO rollout_worker.py:724 -- Generating sample batch of size 200\n"
     ]
    }
   ],
   "source": [
    "# config['model'][\"max_seq_len\"] = 32\n",
    "# config['model'][\"lstm_cell_size\"]= 256\n",
    "from ray.tune.registry import register_env\n",
    "register_env(\"TradingEnv\", createEnv)\n",
    "analysis = tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\n",
    "#           \"episode_reward_mean\": 500,\n",
    "        \"training_iteration\": 10\n",
    "\n",
    "    },\n",
    "    metric=\"episode_reward_mean\", \n",
    "    mode=\"max\",\n",
    "    config=config,\n",
    "    checkpoint_at_end=True, \n",
    "#     resume=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.rllib.agents.ppo as ppo\n",
    "\n",
    "# # Get checkpoint\n",
    "checkpoints = analysis.get_trial_checkpoints_paths(\n",
    "    trial=analysis.get_best_trial(\"episode_reward_mean\", mode=\"max\"),\n",
    "    metric=\"episode_reward_mean\"\n",
    ")\n",
    "checkpoint_path = checkpoints[0][0]\n",
    "# Restore agent\n",
    "agent = ppo.PPOTrainer(\n",
    "    env=\"TradingEnv\",\n",
    "    config=config\n",
    ")\n",
    "agent.restore(checkpoint_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = createEnv({\n",
    "    \"window_size\": 10, \n",
    "    \"data\": norm_data\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_env = {}\n",
    "config_env[\"data\"] = norm_data\n",
    "config_env[\"window_size\"] = 10\n",
    "env = createEnv(config_env)\n",
    "obs = env.reset()\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "# https://github.com/ray-project/ray/issues/9220\n",
    "# ver cell_size\n",
    "policy = agent.get_policy()\n",
    "cellsize=256\n",
    "state=[np.zeros(cellsize, np.float32),\n",
    "       np.zeros(cellsize, np.float32)]\n",
    "# state = policy.get_initial_state()\n",
    "# , prev_action=0, prev_reward=0\n",
    "actions=np.zeros(2*16, np.float32).reshape(2,16)\n",
    "rewards=np.zeros(16, np.float32)\n",
    "episode_reward =0\n",
    "while not done:\n",
    "#     print(\"before\")\n",
    "# #     print(state)\n",
    "#     action, state, logits = policy.compute_single_action(obs, state)\n",
    "#     state, reward, done, _ = env.step(action)\n",
    "#     cumulative_reward += reward\n",
    "#     print(state)\n",
    "    action, state, logits = agent.compute_action(obs, state)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    actions[:,:-1] = actions[:,1:]\n",
    "    actions[:, -1] = action\n",
    "    rewards[:-1] = rewards[1:]\n",
    "    rewards[-1] = reward\n",
    "    episode_reward += reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABNQAAAORCAYAAAA6ev/YAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzdd1yT1/4H8M+TgAooU4aAIqO4qAqoOKkIuOqoFoVWutx6RauiP+sGq62LamkVr3W09XpFqtYOUHBd16110NJarIKgFK0DFScreX5/cImEJJCwgvB5v16+as5zVo5KT745QxBFUQQRERERERERERFpRaLvDhAREREREREREb1IGFAjIiIiIiIiIiLSAQNqREREREREREREOmBAjYiIiIiIiIiISAcMqBEREREREREREemAATUiIiIiIiIiIiIdMKBGRERERERERESkAwbUiIiIiIiIiIiIdMCAGhERERERERERkQ4YUCMiIiKqIatXr4aLiwukUik6d+6s7+4QERERUTVhQI2IiIgalO3bt0MQBMWvJk2awN3dHdOmTcOtW7eqrZ3ExETMnTsXvXr1wrZt27BixYpqq5uIiIiI9MtA3x0gIiIi0ofIyEg4OzsjLy8PJ0+exMaNGxEfH4/ff/8dxsbGVa7/yJEjkEgk2LJlCxo1alQNPSYiIiKiuoIBNSIiImqQBg0ahC5dugAAxo8fDysrK0RFRWH//v144403Kl3v06dPYWxsjNu3b8PIyKjagmmiKCIvLw9GRkbVUh8RERERVR63fBIREREB6NevHwAgIyMDALBjxw54e3vDyMgIlpaWCAkJQVZWllKZvn37wsPDA+fPn4evry+MjY0xf/58CIKAbdu24cmTJ4qtpdu3bwcAFBUVYdmyZXB1dUXjxo3RunVrzJ8/H/n5+Up1t27dGkOGDMHBgwfRpUsXGBkZYdOmTTh27BgEQcDu3bsREREBBwcHNGvWDEFBQcjNzUV+fj7ef/992NjYoGnTpnjvvfdU6t62bRv69esHGxsbNG7cGO3bt8fGjRtVxqSkDydPnkS3bt3QpEkTuLi44KuvvlLJ++DBA8ycOROtW7dG48aN4ejoiLfffht3795V5MnPz8eSJUvg5uaGxo0bo2XLlpg7d65K/4iIiIjqOq5QIyIiIgKQnp4OALCyssLy5cuxaNEijB49GuPHj8edO3cQHR0NX19fJCcnw9zcXFEuJycHgwYNQkhICEJDQ2Fra4suXbrgn//8J37++Wd88cUXAICePXsCKF4N9+WXXyIoKAizZ8/GmTNn8NFHHyE1NRX79u1T6tOff/6JN954A5MmTcKECRPQpk0bxbOPPvoIRkZGmDdvHtLS0hAdHQ1DQ0NIJBLcv38fS5cuxU8//YTt27fD2dkZixcvVpTduHEjOnTogGHDhsHAwADff/89pk6dCrlcjn/84x9KfUhLS0NQUBDGjRuHd955B1u3bsW7774Lb29vdOjQAQDw+PFj9OnTB6mpqRg7diy8vLxw9+5dfPfdd/jrr7/QvHlzyOVyDBs2DCdPnsTEiRPRrl07/Pbbb/jkk09w+fJlfPvtt9X2Z0lERERU40QiIiKiBmTbtm0iAPHQoUPinTt3xKysLHHXrl2ilZWVaGRkJGZmZopSqVRcvny5UrnffvtNNDAwUEp/5ZVXRABiTEyMSjvvvPOOaGJiopT2yy+/iADE8ePHK6WHh4eLAMQjR44o0pycnEQA4oEDB5TyHj16VAQgenh4iAUFBYr0N954QxQEQRw0aJBS/h49eohOTk5KaU+fPlXp74ABA0QXFxeltJI+HD9+XJF2+/ZtsXHjxuLs2bMVaYsXLxYBiHv37lWpVy6Xi6Ioil9//bUokUjEEydOKD2PiYkRAYinTp1SKUtERERUV3HLJxERETVIAQEBsLa2RsuWLRESEoKmTZti37592Lt3L+RyOUaPHo27d+8qftnZ2eGll17C0aNHlepp3Lgx3nvvPa3ajI+PBwDMmjVLKX327NkAgB9//FEp3dnZGQMGDFBb19tvvw1DQ0PFax8fH4iiiLFjxyrl8/HxQVZWFoqKihRppc9hy83Nxd27d/HKK6/g6tWryM3NVSrfvn179OnTR/Ha2toabdq0wdWrVxVpe/bsQadOnTBixAiVfgqCAACIi4tDu3bt0LZtW6VxLdlqW3ZciYiIiOoybvkkIiKiBunzzz+Hu7s7DAwMYGtrizZt2kAikWD//v0QRREvvfSS2nKlg1gA4ODgoPXFA9euXYNEIoGbm5tSup2dHczNzXHt2jWldGdnZ411tWrVSum1mZkZAKBly5Yq6XK5HLm5ubCysgIAnDp1CkuWLMF///tfPH36VCl/bm6uoi517QCAhYUF7t+/r3idnp6O119/XWNfAeDKlStITU2FtbW12ue3b98utzwRERFRXcKAGhERETVI3bp1U9zyWZpcLocgCEhISIBUKlV53rRpU6XXlbl1s2TVVkXKq1td38pLF0URQHHwy9/fH23btkVUVBRatmyJRo0aIT4+Hp988gnkcrlO9WlLLpfj5ZdfRlRUlNrnZQOBRERERHUZA2pEREREpbi6ukIURTg7O8Pd3b1a63ZycoJcLseVK1fQrl07RfqtW7fw4MEDODk5VWt76nz//ffIz8/Hd999p7T6rCpbLl1dXfH7779XmOfXX3+Fv7+/1gFFIiIiorqKZ6gRERERlTJy5EhIpVJERESorMISRRE5OTmVrnvw4MEAgHXr1imll6zaevXVVytdt7ZKVpyVfm+5ubnYtm1bpet8/fXX8euvv6rcUlq6ndGjRyM7OxubN29WyfPs2TM8efKk0u0TERER1TauUCMiIiIqxdXVFR9++CE++OADZGZm4rXXXkOzZs2QkZGBffv2YeLEiQgPD69U3Z06dcI777yDf/7zn3jw4AFeeeUV/Pzzz/jyyy/x2muvwc/Pr5rfjar+/fujUaNGGDp0KCZNmoTHjx9j8+bNsLGxwc2bNytV55w5c/DNN99g1KhRGDt2LLy9vXHv3j189913iImJQadOnfDWW29h9+7dmDx5Mo4ePYpevXpBJpPh0qVL2L17Nw4ePKh2Cy4RERFRXcSAGhEREVEZ8+bNg7u7Oz755BNEREQAKD7jq3///hg2bFiV6v7iiy/g4uKC7du3Y9++fbCzs8MHH3yAJUuWVEfXK9SmTRt88803WLhwIcLDw2FnZ4cpU6bA2tpa5YZQbTVt2hQnTpzAkiVLsG/fPnz55ZewsbGBv78/HB0dAQASiQTffvstPvnkE3z11VfYt28fjI2N4eLighkzZlT79loiIiKimiSIup4oS0RERERERERE1IDxDDUiIiIiIiIiIiIdMKBGRERERERERESkAwbUiIiIiIiIiIiIdMCAGhERERERERERkQ4YUCMiIiIiIiIiItIBA2pEREREREREREQ6YECNiIiIiIiIiIhIBwyoERERERERERER6YABNSIiIiIiIiIiIh0woEZERERERERERKQDBtSIiIiIiIiIiIh0wIAaERERERERERGRDhhQIyIiIiIiIiIi0gEDakRERERERERERDpgQI2IiIiIiIiIiEgHDKgRERERERERERHpgAE1IiIiIiIiIiIiHTCgRkREREREREREpAMG1IiIiIiIiIiIiHTAgBoREREREREREZEOGFAjIiIiIiIiIiLSAQNqREREREREREREOmBAjYiIiIiIiIiISAcMqBEREREREREREemAATUiIiIiIiIiIiIdMKBGRERERERERESkAwbUiIiIiIiIiIiIdMCAGhERERERERERkQ4YUCMiIiIiIiIiItIBA2pEREREREREREQ6YECNiIiIiIiIiIhIBwyoERERERERERER6YABNSIiIiIiIiIiIh0woEZERERERERERKQDBtSIiIiIiIiIiIh0wIAaERERERERERGRDhhQIyIiIiIiIiIi0gEDakRERERERERERDpgQI2IiIiIiIiIiEgHDKgRERERERERERHpgAE1IiIiIiIiIiIiHTCgRkREREREREREpAMG1IiIiIiIiIiIiHTAgBoREREREREREZEOGFAjIiIiIiIiIiLSAQNqREREREREREREOmBAjYiIiIiIiIiISAcMqBEREREREREREemAATUiIiIiIiIiIiIdMKBGRERERERERESkAwbUiIiIiIiIiIiIdMCAGhERERERERERkQ4YUCMiIiIiIiIiItIBA2pEREREREREREQ6YECNiIiIiIiIiIhIBwyoERERERERERER6YABNSIiIiIiIiIiIh0woEZERERERERERKQDBtSIiIiIiIiIiIh0wIAaERERERERERGRDhhQIyIiIiIiIiIi0gEDakRERERERERERDpgQI2IXgh9+/aFIAhKaceOHYMgCFi6dKl+OlXNWrdujdatW+u7G0RERESkR9u3b4cgCNi+fbu+u0JE5WBAjYgUBEHQ6Rf/J1++nJwcLFu2DD179kTz5s1haGgIKysr9OnTBytWrMCtW7f03UUVnMARERFRRUrmgk5OTsjLy1Obp3Xr1hAEAUVFRVVqS92XqhUJCwuDIAjYuHGj2ucDBgyAIAh45ZVX1D7ftm0bBEHA2LFjde6vNurbl8JEDZWBvjtARHXHkiVLVNLWrVuH3NxczJgxA+bm5krPOnfuXDsd06Bbt25ITU1F8+bN9doPdX744QeEhoYiNzcXbm5uGDFiBGxsbJCbm4szZ85g4cKFWLFiBdLS0mBnZ6fv7hIRERHp7Pr161i3bh3mzZun764o8ff3x2effYYjR45gypQpSs8KCgpw8uRJCIKAn376Cc+ePYORkZFSnsOHDwMAAgICaq3PRPTiYUCNiBTUfUu2fft25Obm4v33369z2xGNjY3Rtm1bfXdDxX/+8x+MGDECBgYG2LZtG9555x2Vb1Z/++03zJgxQ+O3ukRERER1mYWFBQRBwMcff4zx48fXqS84+/btC4lEgmPHjkEURaV52E8//YSnT59i1KhRiIuLw8mTJxEYGKhU/ujRowCAfv361Wq/iejFwi2fRFQpJcvvCwoKEBkZiTZt2qBx48Z49913AQC5ublYvXo1+vXrB0dHRzRq1AjW1tYYNmwY/vvf/2qsd9euXfD29oaRkRFsbGzw1ltv4caNG2rzalouX9K3oqIirFixAi+99BIaN26Mli1b4v/+7/9QUFCgtr5//etf8PLyUmlbl60GcrkckyZNQlFREdavX493331XbdmXX34Zhw4dgoODg8qzJ0+eYM6cOWjVqhUaN24MNzc3rFy5EqIoquTdvn07Xn/9dbi4uMDIyAimpqbo1asXduzYobZ/5f259e3bF++99x4A4L333lPa3puZmanV+yciIqKGwdjYGIsWLUJubi4iIiJ0KnvmzBkEBQXBzs4OjRo1QsuWLTFp0iSlOV9mZiYEQcB//vMfAMpHk/Tt27fc+s3NzeHl5YW7d+8iJSVF6dmRI0cAFO/MkEgkitclLl26hBs3bqBDhw5KuwjOnz+P119/HTY2NmjcuDGcnJwwdepU3Lx5U6X9kvnf1atXER0djY4dO8LIyAh9+/bFu+++Cz8/PwBARESE0vs6duyYSl1Hjx5F37590axZM5iamuLVV19Fampque+fiGoHV6gRUZW8/vrrOHv2LAYNGoTXXnsNNjY2AIDU1FQsWLAAvr6+ePXVV2FhYYHr16/ju+++Q0JCAr7//nsMHDhQqa5PPvkEs2bNgrm5Od5++22Ym5vj4MGD6NmzJ8zMzHTu25tvvokTJ05g0KBBMDU1RXx8PFatWoXbt29j27ZtSnlXrVqF//u//4OFhQXeeecdmJmZISkpCb169dKp7f/85z/4888/4eDggHHjxpWbVyKRQCJR/l6jsLAQAwYMwI0bNzBo0CAYGBjg22+/xbx585CXl6eyLXfKlCno0KEDfH190aJFC+Tk5CA+Ph5vvfUW/vzzTyxbtkxt2+r+3Pr27Qtzc3Ps378fw4cPV9rSW3a7LxEREdE//vEPfPbZZ9i0aROmT5+Ol156qcIyW7duxcSJE9G4cWMMGzYMLVu2xJUrV/DFF1/g+++/x08//YRWrVrB3NwcS5Yswfbt23Ht2jWlOZA2uyb69euHc+fO4fDhw+jUqZMi/fDhw3B3d0eHDh3g6emp2N5Z+jlQvG20xA8//IDXX38doigiKCgITk5OOH/+PDZu3Ij9+/fj5MmTcHZ2VunDjBkzcOLECbz66qsYPHgwpFIpunbtCgD48ssv8corrygFB8u+rx9++AH79+/HoEGDMHnyZPzxxx+Ij4/H2bNn8ccff9SpVYFEDZJIRFQOJycnEYCYkZGhlP7KK6+IAMSXX35ZvHPnjkq5Bw8eqE3PysoSW7RoIbZt21YpPSMjQzQ0NBQtLCyU2pLJZOLIkSNFAGLZH1lHjx4VAYhLlixR2zcvLy8xJydHkf748WPR1dVVlEgk4s2bNxXp6enpooGBgdi8eXPx+vXrinS5XC6GhISobVuTyMhIEYA4ZswYrfKXVjLWgwYNEp8+fapIv3XrlmhmZiaamZmJBQUFSmXS0tJU6snPzxf79esnGhgYiH/99ZfSs4r+3LZt2yYCELdt26Zz/4mIiKhhACA6ODiIoiiKcXFxIgBxxIgRSnlK5jWFhYWKtD///FM0NDQUXV1dVeYohw4dEiUSifjaa68ppZfMXXR18OBBEYD46quvKtKePHkiGhoaipMmTRJFURTDw8NFqVQqPnjwQJGnZN65f/9+URRF8dGjR6KlpaUokUjE48ePK7Xx8ccfiwDEwMBApfR33nlHBCDa29uLV69eVembpjlsiZL5mFQqFQ8dOqT0bN68eSIAceXKldoPBhHVCG75JKIqWbZsmdpvx8zMzNSmOzo6IigoCJcuXcL169cV6f/6179QWFiIsLAwpW/nJBIJVq9erbKSSxsrV66EpaWl4rWJiQnGjBkDuVyOc+fOKdJ37tyJoqIihIWFoWXLlor0knNBpFKp1m2WLPt3dHTUub8lPv30U6XDcW1sbDB8+HDk5ubizz//VMrr6uqqUr5Ro0b4xz/+gaKiIpVvXUto+nMjIiIi0kVQUBB69OiBffv24eTJk+Xm3bhxIwoLC7F+/XqVYy/8/f0xbNgwfP/993j06FGV+9W7d280atQIJ06cgEwmAwCcOHEChYWFirPR/Pz8IJPJFNtKRVHEsWPHIJVKFTeA7t+/H/fu3UNwcDD69Omj1Mbs2bPRunVrJCUlKc1rS8ydO1ftyjVthYSEKK2UA4CJEycCAH7++edK10tE1YNbPomoSrp166bx2alTp7B+/Xr897//xe3bt1XOLsvOzkarVq0AABcuXAAAtdeXu7i4oGXLlrh27ZpOfevSpYtKWknA7P79+4q05ORkAMUTr7KcnJzQsmXLWjtDzMzMDG5ubirp6voNFN+utXLlShw+fBjXr1/Hs2fPlJ5nZ2erbae8PzciIiIiXaxduxY9e/ZEeHg4fvrpJ435Ss7R/c9//oOzZ8+qPL99+zZkMhkuX74Mb2/vKvXJ2NgY3bt3x/Hjx3H27Fl0794dR44cUTqDrU+fPjAwMMCRI0cwbNgw/PLLL7h37x58fHwUR36UzFHVXVBgYGAAX19fZGZmIjk5WTGvLVHV+Za2c1ki0g8G1IioSkof1lravn37EBQUhCZNmiAwMBCurq4wMTFR3Lj0n//8B/n5+Yr8ubm5AABbW1uN7egaUFN37peBQfGPvZJvKrVp29bWVuuAWosWLQBoDmRVRNNZZer6ffXqVXTr1g33799Hnz590L9/f5iZmUEqlSIzMxNffvml0hiXpunPjYiIiEhXPXr0QFBQEL755hvExsYiODhYbb6cnBwAwOrVq8ut7/Hjx9XSL39/fxw/fhyHDx9G9+7dcfjwYXTo0EFx5m+zZs3g5eWlWNFf8t+AgABFHSXzxJI5Xlkl6Q8ePFB5VtX5lrZzWSLSD275JKIq0XT75aJFi9CoUSOcO3cO3377LdauXYvIyEgsXboUbdq0Uclf8i3grVu31Nb3999/V1+nyzA1NS23bU3p6pSscjt27FiNT3SioqKQk5ODLVu24NixY/j000+xbNkyLF26FAMGDCi3rLa3lhIRERFp46OPPoKhoSE++OADjTeql8z3cnNzIYqixl/qdixURsmqsiNHjuDBgwdITk5W3LBZws/PDxcvXsTt27cVN36W3mZZ0mdNc9GS4z7UXWLF+RZR/caAGhHViLS0NLRv3x7t2rVTSpfL5WrP1/Dy8gIAxRkWpV29ehVZWVk101EAnp6eAKC2X9euXdOp7VdeeQVt2rTBX3/9pXKTaFlyuRyFhYW6dbaUtLQ0AMU3dpalbhy1UXJeHL/1JCIiIl24ublh6tSpyMjIQHR0tNo83bt3B1B8lpm2qjI38fHxgYmJCU6fPo0DBw5ALperbN308/ODKIpITEzEiRMnYGRkhJ49eyqel8wTjx07plJ/UVGR4r2UzGW1wfkWUf3AgBoR1YjWrVvjypUruHHjhiJNFEUsXboUf/zxh0r+MWPGwNDQENHR0UrbK+VyOebMmQO5XF5jfX3zzTdhYGCA6OhopeCZKIr44IMPdJrsSCQSbNq0CQYGBpg+fTp27NgBURRV8v3xxx/o379/pbeGAs+vVi87wTt48CC++OKLStVpZWUFAGoP1iUiIiIqz+LFi2Fubo7ly5er3bY5bdo0GBoaYubMmbh8+bLK84KCApVgW1XmJoaGhujTpw/y8vKwYsUKSCQSldVvvXv3hqGhIVatWoXHjx+jV69eaNy4seL5a6+9BktLS/z73/9WOR9u3bp1yMjIQEBAgMr5aeXhfIuofuAZakRUI2bOnInJkyfD09MTr7/+OgwNDXHq1Cn88ccfGDp0KL7//nul/K1bt8bHH3+M2bNnw9PTE8HBwTAzM8PBgwfx4MEDdOzYESkpKTXSV1dXV0RGRmL+/Pno1KmTou2kpCTcu3cPnTp10qntV155BXv37sVbb72Ft956C8uWLUPfvn1hbW2N3NxcnDt3DmfOnIGJiYnSbZ66mjp1KrZt24ZRo0YhKCgI9vb2+P3333HgwAGMHj0asbGxOtfZo0cPGBsbY926dcjJyVGc/REWFqZ2KwMRERFRCUtLS8yfPx9z585V+7xt27bYunUrxo4diw4dOmDgwIFwd3dHYWEhrl+/jhMnTsDa2hqXLl1SlPH390dcXBxGjhyJwYMHw8jICE5OTnjrrbe06pO/vz8OHDiA3377DV5eXrCwsFB6bmJigq5du+L06dOK/KU1bdoUW7duxahRo/DKK69g1KhRaNWqFc6fP4/ExETY2dlh06ZNugwT2rRpAwcHB+zatQuGhoZwcnKCIAh466234OTkpFNdRKQ/XKFGRDVi0qRJ2LZtG1q0aIEvv/wS//rXv9CyZUucOXNG45L4WbNmYefOnXB2dsb27duxdetWeHh44PTp0yqTn+r2wQcf4KuvvoKTkxO2bduGLVu2oF27djh16hSKiooU56xpa+jQoUhPT0dkZCSsrKzwzTffYOXKlfjXv/4FQRAQGRmJ9PR0jRchaKNjx444evQoevbsiR9//BEbN27Ew4cPsXfvXkyePLlSdVpYWGDPnj1o3749tm/fjkWLFmHRokW8SYqIiIi0Mn36dMUqenVCQ0Nx/vx5jBkzBikpKfjss8+wY8cOpKWlISgoCBs2bFDKP378eHzwwQfIzc3FqlWrsGjRImzZskXr/pQOkJU9P01detmAGgAMHz4cp06dwuDBg3Hw4EGsWbMGqampmDx5Ms6fPw8XFxet+wMUb/nct28fevfujbi4OCxZsgSLFi1CRkaGTvUQkX4Jorq9SEREBAB4+PAhbG1t0blzZ8VV70RERERERNSwcYUaERGAO3fuqFwQUFRUhNmzZyMvLw8jRozQU8+IiIiIiIioruEKNSIiADExMVi8eDECAgLQsmVL3Lt3D8ePH8fly5fRuXNnnD59ukrnnREREREREVH9wUsJiIhQfK167969cfz4ceTk5AAAnJ2dsWDBAvzf//0fg2lERERERESkwBVqREREREREREREOuAZakRERERERERERDpgQI2IiIiIiIiIiEgHDKgRERERERERERHpgJcSALh//z6KiopqpG5ra2vcuXOnRuom9Tjm+sFx1w+Ou35w3PWjroy7gYEBLCws9N0N0hLnefUPx732ccz1g+OuHxx3/agr467LPI8BNQBFRUUoLCys9noFQVDUz7sfagfHXD847vrBcdcPjrt+cNypsjjPq1847rWPY64fHHf94Ljrx4s67tzySUREREREREREpAMG1IiIiIiIiIiIiHTAgBoREREREREREZEOGFAjIiIiIiIiIiLSAS8lICIiIqIG6cmTJygqKlIchqyrZ8+eoaCgoJp7RRXhuNcsY2NjGBjwYyIRUUX4k5KIiIiIGpz8/HwIggAzM7NK12FoaFgjN4hS+TjuNUcul+PRo0cwMTFhUI2IqALc8klEREREDU5+fj6MjIz03Q2iOkUikaBZs2Z4+vSpvrtCRFTnMaBGRERERA1SZbd6EtVnEgk/IhIRaYM/LYmIiIiIiIiIiHTAgBoREREREREREZEOGFAjIiIiIiIiIiLSAQNqREREREQvgKCgICxevFjjcx8fH2zevLnCenbs2IEuXbrA0dERmzdvxtq1axEYGFidXSUiIqr3eBcyEREREVE9EB8fD2NjY8VrBwcHbNmyBQMHDlSkPXr0CAsXLsSSJUswePBgmJqaQi6X47333tNHl9WaOXMm7Ozs8Oabb6J79+44ePAgPDw8lPIEBQWhffv2iIyMBABcvHgRq1evxoULF/D48WNYW1vD09MTH374IZo3b46srCx0795dUd7ExAQODg7o0aMHxo8fDxcXF5V+lC2jjqOjI/766y+Nz0eNGoV169YhIyMDn376KY4fP4579+7B1tYWXl5emDRpEjp16gQAuH//PhYtWoSkpCRIJBIMHjwYkZGRMDEx0XrsiIio9jCgRkRERERUD1hZWVWYJzs7G4WFhfD394etra0iva4EbWQyGQ4dOoSvvvpK6zI5OTkIDg5GQEAAdu7cCVNTU2RlZSExMRFPnz5Vyrtr1y60adMGz549w6VLl/DFF18gMDAQ27dvR58+fZTy2tvbIzk5WfE6JiYGx44dw65duxRphYWFMDQ0BACcO3cOEyZMwPHjx9GsWTMAQJMmTfDrr78iODgYbdq0wcqVK+Hm5obHjx8jMTERkZGR2LNnDwAgLCwMt27dwr///W8UFRVh5syZmDt3Lj7//HPdBpGIiGoFA2pERERE1KCJoggU5OteTi6DWFhYtcYbNYYgCFpnl8lkWLBgAfbs2QMDAwO8/fbbmDNnDgRBgI+PD8aPH48JEybAx8cHADBu3DgAxSupZs2ahVmzZgEAevToAQD46aefsHv3bhw4cABJSUkAgPfffx8PHz5Et27dsGnTJhQUFGD48OGIiIhQBI/y8/OxcuVK7N+/H7m5uWjbti3mz5+Pnj17Ii8vD4MGDULXrl2xatUqAEBmZib69++PyMhIhISEaHx/586dg6GhITp37lzuyq/Szp49i0ePHmHNmjUwMCj+eNOqVSv06tVLJa+FhQVsbGwAAE5OTggMDMTo0aMRHh6O06dPQyqVKvJKpVJFXqA46Fg2rTRzc3MAQPPmzWFmZgag+O/WzJkz4ezsjH379kEieX7ijoeHh+LP58qVKzh69Cji4+MVK9Y+/PBDvPXWW1i0aBHs7Oy0GgsiIqo9DKgRERERUcNWkA/5tNE6F9M9BKdK8tluoHETrfPHxcUhJCQEP/zwA1JSUjB37lw4ODhgzJgxSvni4+PRsWNHREVFwc/PD1KpFMbGxrC3t0dISAh+/PFH2Nvba1zVdvr0adjY2CAuLg4ZGRmYMmUKOnTooGhn4cKFuHz5MjZs2ABbW1scOHAAoaGhOHToEFxcXBAdHY2hQ4fC398fAQEBCAsLg6+vr1IwzcHBAVFRUQgODlakJSYmIiAgQKcgo7W1NYqKipCQkIAhQ4boVFYikWD8+PEYN24cUlJS4OnpqXVZbVy8eBF//vknPv/8c6VgWomSwNv58+dhZmamCKYBQJ8+fSCRSJCcnIxBgwZVa7+IiKjqeCkBEREREdELwt7eHhEREXBzc8PIkSMxduxYtRcRlATKzMzMYGNjAysrKxgZGcHCwkLx3MbGRmlFVmlmZmZYvnw53NzcEBgYCH9/f5w8eRJA8bbR2NhYbNq0CT4+PmjdujUmT56Mrl27IjY2FkDx6qu5c+ciPDwcS5YsQXZ2tmK1WglXV1eYmpoqpR08eBD9+/fXaUy8vb0RFhaGadOmwcPDA6Ghodi4cSPu3LmjVXk3NzcAxWemVberV68qtaHJ7du3VYKbBgYGMDc3x+3bt6u9X0REVHVcoUZEREREDVujxsUrxXRkaGiIwmrY8qkLLy8vpRVY3t7e2LRpE2QyWdX6UYa7u7tSsM3W1hapqakAgNTUVMhkMpUzxwoKChQBOwCYNGkSDhw4gG3btmHHjh2wtLRUyn/8+HGl11euXMGtW7fQu3dvnfs7b948TJw4EadOnUJycjK+/vprREdHY8+ePWjXrl25ZUVRBAAIgoDs7Gz07dtX8SwsLAzTp0/XuT9l6yYiovqHATUiIiIiatAEQdBp26WinKEhBIn6FV4vupKz0korCQ49efIEUqkUCQkJKivcSl9ucPfuXVy9ehVSqRQZGRnw8/Mrt83ExET4+vqiSZPiP4umTZsCKL6ZtKzc3FyV1W2WlpYYOnQohg4dinnz5mHAgAGIiYnB+vXry233ypUrAIrPXbO1tUViYqLiWcm5aJXl6uoKAEhLS1O5qbQ0Gxsb5OTkKKUVFRXhwYMHGs9sIyIi/eKWTyIiIiKiF0TpWycB4MKFC3B2dla7ddPQ0LDaV64Bxds5ZTIZcnJy4OzsrPSrdPBn9uzZaNu2LdatW4fly5crAlealN3uaWFhAUtLS6SkpCjle/ToETIzM+Hi4qKxrkaNGsHJyUnlls+y5HI5tm7dilatWsHDwwMGBgZK76f0irvK6NChA9zd3bFp0ybI5XKV57m5uQCKVxrm5uYqvddTp05BLpdX+7luRERUPRhQIyIiIiJ6QWRnZ2Pp0qVIS0vDt99+i61btypuiizL0dERJ0+exO3bt/HgwYNq64OrqytGjhyJGTNmID4+HtevX0dycjKio6Nx6NAhAMD27dtx/vx5rFu3DiNHjsTAgQMxbdo0FBQUKOrx9fVFQkICgOLVbCkpKQgMDFRqa+LEiYiOjsbevXuRmZmJ5ORkTJkyBVZWVoqD+pOSkhAWFoakpCSkp6cjLS0NMTExOHLkCAYMGKBU3/3793H79m1cu3YNiYmJCA4ORnJyMtasWaPxPLmqEAQBUVFRuHr1KkaMGIHDhw/j2rVr+OOPP7B+/XqMHTsWAPDSSy/Bz88Pc+bMQXJyMs6ePYsFCxZg+PDhvOGTiKiO4pZPIiIiIqIXRFBQEPLy8jBkyBBIpVKMGzcOoaGhavMuXrwYERER2LlzJ+zs7HDmzJlq60dUVBTWr1+PyMhI/P3337C0tISXlxcCAgKQlpaGZcuWYe3atXBwcAAArFixAgEBAVi9ejUWLFgAAEhPT8fDhw8BFAfFOnfurHLO2tSpU2FiYoINGzYgMzMT5ubm8PHxQVxcHIyMjAAUn/dmZGSEyMhI3LhxA40bN4azszNWr16NoKAgpfpKbhk1MjKCo6MjevbsiVWrVsHZ2bnaxqYsT09PxMfH49NPP8XcuXNx79492NjYoEuXLoiIiFDki46OxsKFCxEcHAyJRILBgwdj2bJlNdYvIiKqGkHkSZm4c+dO1Q+UVUMQBLRo0QI3b97kgaS1hGOuHxx3/eC46wfHXT/q0rgbGhrC2tpar30g7Wma5z18+FDlDC5dVculBAQAePfdd9GtWzdMnTq1wrwc95pX+t9HXfr525Bw3PWD464fdWncdZnnccsnERERERHpVbdu3TB8+HB9d4OIiEhr3PJJRERERER6pc3KNCIiorqEK9SIiIiIiIiIiIh0wIAaERERERERERGRDhhQIyIiIiIiIiIi0gEDakRERERERERERDpgQI2IiIiIiIiIiEgHDKgRERERERERERHpgAE1IiKiBkDMuAx5zEqId2/puytEREREVI3Em38Vz/P+ytR3VxoUBtSIiIgaAPmKcIjnT0G+cp6+u0JElRQUFITFixdrfO7j44PNmzdXWM+OHTvQpUsXODo6YvPmzVi7di0CAwOrs6tERFSL5FELi+d5EdP13ZUGxUDfHSAiIqJa9CBH3z0gohoSHx8PY2NjxWsHBwds2bIFAwcOVKQ9evQICxcuxJIlSzB48GCYmppCLpfjvffe00eX1Zo5cybs7Ozw5ptvonv37jh48CA8PDyU8gQFBaF9+/aIjIwEAFy8eBGrV6/GhQsX8PjxY1hbW8PT0xMffvghmjdvjqysLHTv3l1R3sTEBA4ODujRowfGjx8PFxcXlX6ULaOOo6Mj/vrrL43PR40ahXXr1iEjIwOffvopjh8/jnv37sHW1hZeXl6YNGkSOnXqBABYv349Dh8+jIsXL6JRo0ZITU3VesyIqIF7cE/fPWiQGFAjIiIiIqoHrKysKsyTnZ2NwsJC+Pv7w9bWVpFuYmJSk13Tmkwmw6FDh/DVV19pXSYnJwfBwcEICAjAzp07YWpqiqysLCQmJuLp06dKeXft2oU2bdrg2bNnuHTpEr744gsEBgZi+/bt6NOnj1Jee3t7JCcnK17HxMTg2LFj2LVrlyKtsLAQhoaGAIBz585hwoQJOH78OJo1awYAaNKkCX799VcEBwejTZs2WLlyJdzc3PD48WMkJiYiMjISe/bsUdQ1ZMgQeHt7K7VBRER1EwNqRERERNSgiaKIfJmoczkZ5Cgsklep7cZSAYIgaN+mTIYFCxZgz549MDAwwNtvv405c+ZAEAT4+Phg/PjxmDBhAnx8fAAA48aNA1C8kmrWrFmYNWsWAKBHjx4AgJ9++gm7d+/GgQMHkJSUBAB4//338fDhQ3Tr1g2bNm1CQUEBhg8fjoiICEXwKD8/HytXrsT+/fuRm5uLtm3bYv78+ejZsyfy8vIwaNAgdO3aFatWrQIAZGZmon///oiMjERISIjG93fu3DkYGhqic+fO5a78Ku3s2bN49OgR1qxZAwOD4o83rVq1Qq9evVTyWlhYwMbGBgDg5OSEwMBAjB49GuHh4Th9+jSkUqkir1QqVeQFioOOZdNKMzc3BwA0b94cZmZmAIr/bs2cORPOzs7Yt28fJJLnJ+54eHgo/nwAIDw8HAAQGxur1fsmIiL9YkCNiIiIiBq0fJmI4NjLemk7NtgdTQy0D6jFxcUhJCQEP/zwA1JSUjB37lw4ODhgzJgxSvni4+PRsWNHREVFwc/PD1KpFMbGxrC3t0dISAh+/PFH2Nvba1zVdvr0adjY2CAuLg4ZGRmYMmUKOnTooGhn4cKFuHz5MjZs2ABbW1scOHAAoaGhOHToEFxcXBAdHY2hQ4fC398fAQEBCAsLg6+vr1IwzcHBAVFRUQgODlakJSYmIiAgQKcgo7W1NYqKipCQkIAhQ4boVFYikWD8+PEYN24cUlJS4OnpqXVZbVy8eBF//vknPv/8c6VgWomSwBsREb14eCkBEREREdELwt7eHhEREXBzc8PIkSMxduxYtRcRlATKzMzMYGNjAysrKxgZGcHCwkLx3MbGRmlFVmlmZmZYvnw53NzcEBgYCH9/f5w8eRJA8bbR2NhYbNq0CT4+PmjdujUmT56Mrl27KlZXeXh4YO7cuQgPD8eSJUuQnZ2tWK1WwtXVFaampkppBw8eRP/+/XUaE29vb4SFhWHatGnw8PBAaGgoNm7ciDt37mhV3s3NDUDxmWnV7erVq0ptEBFR/cEVakRERETUoDWWCogNdte5nKGBIQqLCqvcti68vLyUVmB5e3tj06ZNkMlkVepHWe7u7krBNltbW8Uh+ampqZDJZCpnjhUUFCgCdgAwadIkHDhwANu2bcOOHTtgaWmplP/48eNKr69cuYJbt26hd+/eOvd33rx5mDhxIk6dOoXk5GR8/fXXiI6Oxp49e9CuXbtyy4pi8XZfQRCQnZ2Nvn37Kp6FhYVh+vTK35pXUjcREdU/DKgRERERUYMmCIJO2y5LGBpKIK2nGz5KzkorrSQ49OTJE0ilUiQkJKiscCt9ucHdu3dx9epVSKVSZGRkwM/Pr9w2ExMT4evriyZNmgAAmjZtCqD4ZtKycnNzVVa3WVpaYujQoRg6dCjmzZuHAQMGICYmBuvXry+33StXrgAoPnfN1tYWiYmJimcl56JVlqurKwAgLS1N5aZSIiJ6sdXPGQARERERUT1U+tZJALhw4QKcnZ3Vbt00NDSs9pVrQPF2TplMhpycHDg7Oyv9Kn1g/+zZs9G2bVusW7cOy5cvVwSuNCm73dPCwgKWlpZISUlRyvfo0SNkZmbCxcVFY12NGjWCk5OTyi2fZcnlcmzduhWtWrWCh4cHDAwMlN5P6RV3ldGhQwe4u7tj06ZNkMtVL7DIzc2tUv1ERKQ/DKgREREREb0gsrOzsXTpUqSlpeHbb7/F1q1blW6KLM3R0REnT57E7du38eDBg2rrg6urK0aOHIkZM2YgPj4e169fR3JyMqKjo3Ho0CEAwPbt23H+/HmsW7cOI0eOxMCBAzFt2jQUFBQo6vH19UVCQgKA4tVsKSkpCAwMVGpr4sSJiI6Oxt69e5GZmYnk5GRMmTIFVlZWGDRoEAAgKSkJYWFhSEpKQnp6OtLS0hATE4MjR45gwIABSvXdv38ft2/fxrVr15CYmIjg4GAkJydjzZo1Gs+TqwpBEBAVFYWrV69ixIgROHz4MK5du4Y//vgD69evx9ixYxV5s7Oz8fvvv+PGjRuQyWT4/fff8fvvv+PJkyfV3i8iIqo6bvkkIiIiInpBBAUFIS8vD0OGDIFUKsW4ceMQGhqqNu/ixYsRERGBnTt3ws7ODmfOnKm2fkRFRWH9+vWIjIzE33//DUtLS3h5eSEgIABpaWlYtmwZ1q5dCwcHBwDAihUrEBAQgNWrV2PBggUAgPT0dDx8+BBAcVCsc+fOKuesTZ06FSYmJtiwYQMyMzNhbm4OHx8fxMXFwcjICEDxeW9GRkaIjIzEjRs30LhxYzg7O2P16tUICgpSqq/kllEjIyM4OjqiZ8+eWLVqFZydnattbMry9PREfHw8Pv30U8ydOxf37t2DjY0NunTpgoiICEW+1atXIy4uTvG6JBgYFxeHnj171lj/iKh+kZ/5DyQ+r+i7Gw2CIPKkTNy5cweFhVU7UFYdQRDQokUL3Lx5kweS1hKOuX5w3PWD464fL+q4yyYMU/xeGDACkqD39Ngb3dWlcTc0NIS1tbVe+0Da0zTPe/jwocoZXLoyNDSskTlkQ/Tuu++iW7dumDp1aoV5Oe41r/S/j7r087ch4bjrx4s67qXneQAgid4FoYmxnnqju7o07rrM87jlk4iIqIERD+7TdxeIiJR069YNw4cP13c3iIheOGoDUIVFtd+RBohbPomIiOo58Vn5h3ITEembNivTiIhIjTt/q6YV5td+PxogrlAjIiKq58SzJ1TTuF2KiIiI6MVXWKCSJF5MVpORqhsDakRERPXdk0eqaffv1H4/iIiIiKh6Fan5krSIWz5rAwNqRERE9Z1crpomU5NGRERERC+Wp09UksQLp/XQkYaHATUiIqL67vZN1bQb12u/H0RERERUrcSsq6qJmVdqvyMNEANqRERE9Zz4+3nVtJtZeugJEREREVUn8defVRPz82q/Iw0QA2pERET13cMHqmkFnGgRERERvfAuX1RN8+xR+/1ogBhQIyIiaoia2+m7B0RERERUA4QOnfXdhQaBATUiIqIGSLBsru8uEJGOgoKCsHjxYo3PfXx8sHnz5grr2bFjB7p06QJHR0ds3rwZa9euRWBgYHV2lYiI9Ekm03cPGgQDfXeAiIiI9MDUXN89IKJqFh8fD2NjY8VrBwcHbNmyBQMHDlSkPXr0CAsXLsSSJUswePBgmJqaQi6X47333tNHl9WaOXMm7Ozs8Oabb6J79+44ePAgPDw8lPIEBQWhffv2iIyMBABcvHgRq1evxoULF/D48WNYW1vD09MTH374IZo3b46srCx0795dUd7ExAQODg7o0aMHxo8fDxcXF5V+lC2jjqOjI/766y+Nz0eNGoV169YhIyMDn376KY4fP4579+7B1tYWXl5emDRpEjp16oSsrCysW7cOp06dwp07d2Bra4uRI0di+vTpaNSokS7DR0TEM9RqCQNqREREDZCYfR1CK1d9d4OIqpGVlVWFebKzs1FYWAh/f3/Y2toq0k1MTGqya1qTyWQ4dOgQvvrqK63L5OTkIDg4GAEBAdi5cydMTU2RlZWFxMREPH36VCnvrl270KZNGzx79gyXLl3CF198gcDAQGzfvh19+vRRymtvb4/k5GTF65iYGBw7dgy7du1SpBUWFsLQ0BAAcO7cOUyYMAHHjx9Hs2bNAABNmjTBr7/+iuDgYLRp0wYrV66Em5sbHj9+jMTERERGRmLPnj1IS0uDXC7HypUr0bp1a/z555+YM2cOnj59Wu6qRCIivNwF+O2cUpJ45jjEASMhCIKeOtUwMKBGRETUAAmW1vruAlGdIYpipXbHCIKIoiKxSm1LpdDpA49MJsOCBQuwZ88eGBgY4O2338acOXMgCAJ8fHwwfvx4TJgwAT4+PgCAcePGASheSTVr1izMmjULANCjR/GB1T/99BN2796NAwcOICkpCQDw/vvv4+HDh+jWrRs2bdqEgoICDB8+HBEREYrgUX5+PlauXIn9+/cjNzcXbdu2xfz589GzZ0/k5eVh0KBB6Nq1K1atWgUAyMzMRP/+/REZGYmQkBCN7+/cuXMwNDRE586dy135VdrZs2fx6NEjrFmzBgYGxR9vWrVqhV69eqnktbCwgI2NDQDAyckJgYGBGD16NMLDw3H69GlIpVJFXqlUqsgLFAcdy6aVZm5uDgBo3rw5zMzMABT/3Zo5cyacnZ2xb98+SCTPT9zx8PBQ/Pn4+fnBz89P8czJyQnp6en46quvGFAjonJJ+vSHvExADX9lQPz5OASfV/TTqQaCATUiIqJ6Tuj/GsTEb5XS5GvmQ7JhD4T/fTgmashkMiBhT65e2h70uhkMdJiRx8XFISQkBD/88ANSUlIwd+5cODg4YMyYMUr54uPj0bFjR0RFRcHPzw9SqRTGxsawt7dHSEgIfvzxR9jb22tc1Xb69GnY2NggLi4OGRkZmDJlCjp06KBoZ+HChbh8+TI2bNgAW1tbHDhwAKGhoTh06BBcXFwQHR2NoUOHwt/fHwEBAQgLC4Ovr69SMM3BwQFRUVEIDg5WpCUmJiIgIECnIKO1tTWKioqQkJCAIUOG6FRWIpFg/PjxGDduHFJSUuDp6al1WW1cvHgRf/75Jz7//HOlYFqJksCbOg8fPlQE6YiINBLVf7EjfrEWYECtRvFSAiIiovrOuoXaZPHn47XcESKqKnt7e0RERMDNzQ0jR47E2LFj1V5EUBIoMzMzg42NDaysrGBkZAQLCwvFcxsbG6UVWaWZmZlh+fLlcHNzQ2BgIPz9/XHy5EkAxdtGY2NjsWnTJvj4+KB169aYPHkyunbtitjYWADFq6/mzp2L8PBwLFmyBNnZ2YrVaiVcXV1hamqqlHbw4EH0799fpzHx9vZGWFgYpk2bBg8PD4SGhmLjxo24c+eOVuXd3NwAFJ+ZVt2uXr2q1Ia2MjIysG3bNoSGhlZ7n4iovqnaSmmqPK5QIyIiqvc0fHP583+AXv613BeiukcqLV4ppitDQ0MUFhZWuW1deHl5Ka3A8vb2xqZNmyCr5hvd3N3dlYJttra2SE1NBQCkpqZCJpOpnDlWUFCgCNgBwKRJk3DgwAFs27YNO3bsgKWlpVL+48eVg/pXrlzBrVu30Lt3b537O2/ePEycOBGnTp1CcnIyvv76a0RHR2PPnj1o165duWXF/63uEAQB2dnZ6Nu3r+JZWFgYpk+frnN/ytati5s3byI0NBRDhgxRWXlIRKSC8TS9YUCNiIiovtM00frjl9rsBVGdJQiCTtsuSxgYCBDF+nngs6Ga7eAlwaEnT55AKpUiISFBZYVb6csN7t69i6tXr0IqlSIjI0PpjDB1EhMT4evriyZNmgAAmjZtCqD4ZtKycnNzVVa3WVpaYujQoRg6dCjmzZuHAQMGICYmBuvXry+33StXrgAoPnfN1tYWiYmJimdV3XLp6lp8+UtaWprKTaXq/P333xg1ahS8vb1VVvQREaklyvXdgwaLWz6JiIjqPX51SVRflL51EgAuXLgAZ2dntVs3DQ0Nq33lGlC8nVMmkyEnJwfOzs5Kv0of2D979my0bdsW69atw/LlyxWBK03Kbve0sLCApaUlUlJSlPI9evQImZmZcHFx0VhXo0aN4OTkpHLLZ1lyuRxbt25Fq1at4OHhAQMDA6X3U3rFXWV06NAB7u7u2LRpE+Ry1Q+9ubnPz+67efMmgoKC0LFjR3zyySdqz1wjIiqrEgthqZrwpzQREVF9J+dMi6i+yM7OxtKlS5GWloZvv/0WW7duVdwUWZajoyNOnjyJ27dv48GDB9XWB1dXV4wcORIzZsxAfHw8rl+/juTkZERHR+PQoUMAgO3bt+P8+fNYt24dRo4ciYEDB2LatGkoKChQ1OPr64uEhAQAxavZUlJSEBgYqNTWxIkTER0djb179yIzMxPJycmYMmUKrKysMGjQIABAUlISwsLCkJSUhPT0dKSlpSEmJgZHjhzBgAEDlOq7f/8+bt++jWvXriExMRHBwcFITk7GmjVrNJ4nVxWCICAqKgpXr17FiBEjcPjwYVy7dg1//PEH1q9fj7FjxwJ4HkxzcHDAokWLkJOTg9u3b+P27dvV3iciqm84z9MXbvkkIiKq9zjRIqovgoKCkJeXhyFDhkAqlWLcuHEaD65fvHgxIiIisHPnTtjZ2eHMmTPV1o+oqCisX78ekZGR+Pvvv2FpaQkvLy8EBAQgLS0Ny5Ytw9q1a+Hg4AAAWLFiBQICArB69WosWLAAAJCeno6HDx8CKA6Kde7cWeWctalTp8LExAQbNmxAZmYmzM3N4ePjg7i4OBgZGQEoPu/NyMgIkZGRuHHjBho3bgxnZ2esXr0aQUFBSvWV3DJqZGQER0dH9OzZE6tWrYKzs3O1jU1Znp6eiI+Px6effoq5c+fi3r17sLGxQZcuXRAREQGg+Dy5zMxMZGZmokuXLkrls7Oza6xvRFQPqFn9SrVDECtzUmY9c+fOnSofKKuOIAho0aIFbt68WakDSUl3HHP94LjrB8ddP17EcZcf+g5i7BeqD5oYQRodW/sdqoS6NO6GhoawtrbWax9Ie5rmeQ8fPlQ5g0tX1XEpARV799130a1bN0ydOrXCvBz3mlf630dd+vnbkHDc9aO6xl2+5ROIPx2FZHkMBBv7auyhmrbO/AfiF2vVPpNu/q5G264udenvuy7zPG75JCIiqvc0TEw4PyeiOqJbt24YPny4vrtBRFQtxJ+OAgDkCybXQmP/m9C161TzbZESBtSIiIj0QL59PWQThkEsyK/5xjQFzmRFNd82EZEWpk6dqtgeSkREuvjfRE+on7dO12UMqBEREemBeOowAEC+6oNaaKx4oiX4vKKczoAaERER0YtNcfkUA2q1jQE1IiIifbqWVvNtiBq+ueSZLEREREQvuP/N5yQMqNU23vJJRERU7/GbSyIiIqL6QhRFiN/uAFq0LHW0B+d5tY0BNSIiovpOsUJNv90gIiIiomrw528Q4+MAAMKo94rTeIZareOWTyIionpKvH0TYn4ev7kkIiIiqkfE3PvPfx+3TY89adi4Qo2IiKgeEq+lQf7hLMDcEkK/IcWJPFuDiIiI6MUnylXTfjtX+/1o4LhCjYiIqB4SL/y3+DcP7kHc+9X/UhlQIyIiInrhyXmxVF3AgBoREVF9JFfzzWVBfu33g4iqTVBQEBYvXqzxuY+PDzZv3lxhPTt27ECXLl3g6OiIzZs3Y+3atQgMDKzOrhIRUU1St0KNah23fBIREdVHsiKVJPHsCT10hIhqS3x8PIyNjRWvHRwcsGXLFgwcOFCR9ujRIyxcuBBLlizB4MGDYWpqCrlcjvfee08fXVZr5syZsLOzw5tvvonu3bvj4MGD8PDwUMoTFBSE9u3bIzIyEgBw8eJFrF69GhcuXMDjx49hbW0NT09PfPjhh2jevDmysrLQvXt3RXkTExM4ODigR48eGD9+PFxcXFT6UbaMOo6Ojvjrr780Ph81ahTWrVuHjIwMfPrppzh+/Dju3bsHW1tbeHl5YdKkSejUqRMA4N1338XFixeRk5MDMzMz9O7dGwsWLICdnZ3WY0dEDcSDe/ruAYEBNSIiovqp1GG1RNQwWFlZVZgnOzsbhYWF8Pf3h62trSLdxMSkJrumNZlMhkOHDuGrr76qOPP/5OTkIDg4GAEBAdi5cydMTU2RlZWFxMREPH36VCnvrl270KZNGzx79gyXLl3CF198gcDAQGzfvh19+vRRymtvb4/k5GTF65iYGBw7dgy7du1SpBUWFsLQ0BAAcO7cOUyYMAHHjx9Hs2bNAABNmjTBr7/+iuDgYLRp0wYrV66Em5sbHj9+jMTERERGRmLPnj0AgJ49eyIsLAy2tra4efMmli1bhokTJ+K7777TbRCJqE4RL/8Owd2j4oy6uHureuujSmFAjYiIqB4Snz7RdxeIXhiiKKKoSHVVpzYKCwur1LaBgQEEQfvzDWUyGRYsWIA9e/bAwMAAb7/9NubMmQNBEODj44Px48djwoQJ8PHxAQCMGzcOQPFKqlmzZmHWrFkAgB49egAAfvrpJ+zevRsHDhxAUlISAOD999/Hw4cP0a1bN2zatAkFBQUYPnw4IiIiFMGj/Px8rFy5Evv370dubi7atm2L+fPno2fPnsjLy8OgQYPQtWtXrFq1CgCQmZmJ/v37IzIyEiEhIRrf37lz52BoaIjOnTuXu/KrtLNnz+LRo0dYs2YNDAyKP960atUKvXr1UslrYWEBGxsbAICTkxMCAwMxevRohIeH4/Tp05BKpYq8UqlUkRcoDjqWTSvN3NwcANC8eXOYmZkBKP67NXPmTDg7O2Pfvn2QSJ6fuOPh4aH48wGAiRMnKn7v6OiIadOmYezYsUpBOyJ68cgPfw9pdQfUeIxHncCAGhERUX30+3l994DohVFUVISNGzfqpe0pU6boFCyJi4tDSEgIfvjhB6SkpGDu3LlwcHDAmDFjlPLFx8ejY8eOiIqKgp+fH6RSKYyNjWFvb4+QkBD8+OOPsLe317iq7fTp07CxsUFcXBwyMjIwZcoUdOjQQdHOwoULcfnyZWzYsAG2trY4cOAAQkNDcejQIbi4uCA6OhpDhw6Fv78/AgICEBYWBl9fX6VgmoODA6KiohAcHKxIS0xMREBAgE5BRmtraxQVFSEhIQFDhgzRqaxEIsH48eMxbtw4pKSkwNPTU+uy2rh48SL+/PNPfP7550rBtBIlgbey7t+/j71796JLly4MphG96EouiqpOJk2rv07SGQNqREREeiZmXoHQ+iV9d4MI9+7dw44dO/DLL78gPz8fdnZ2mDp1KlxdXQEUr7bZvXs3Dh8+jCdPnqBt27YYP348WrRooajj8ePH2Lp1K86fP69YNfXee++hSZMmijzXrl3Dli1bkJ6eDlNTUwwcOBDDhw9X6st///tfxMbG4s6dO7Czs8OYMWPg5eVVOwNRh9nb2yMiIgKCIMDNzQ2XLl3C5s2bVQJqJYEyMzMzpRVVFhYWiueaVlqVlFu+fDmkUinc3Nzg7++PkydPYsyYMcjOzkZsbCx+/vlnxflekydPxtGjRxEbG4sPPvgAHh4emDt3LsLDwzF8+HBkZ2fjyy+/VGrD1dUVpqamSmkHDx7E0qVLdRoTb29vhIWFYdq0aZg3bx48PT3Rq1cvBAUFwdrausLybm5uAIrPTKvugNrVq1eV2qjI8uXLsW3bNjx79gxeXl4qY0ZEBAAwUBNob+UKXE+v/b40YAyoERER6ZmYmsKAGund48ePsWjRInTo0AHz58+Hqakpbt68qXS21v79+5GQkIB//OMfsLGxQWxsLJYvX46oqCg0atQIAPDpp5/i/v37WLhwIWQyGTZs2IBNmzZhxowZAICnT5/iww8/xMsvv4wJEybg+vXr2LhxI0xMTBAQEAAA+PPPP7F+/Xq8+eab8PLywsmTJ7F69WqsXLkSrVq1qvb3bmBggClTpuhcztDQsFq2fOrCy8tLaQWWt7c3Nm3aBJlMVqV+lOXu7q60/dHW1hapqakAgNTUVMhkMpUzxwoKChQBOwCYNGkSDhw4gG3btmHHjh2wtLRUyn/8+HGl11euXMGtW7fQu3dvnfs7b948TJw4EadOnUJycjK+/vprREdHY8+ePWjXrl25ZUVRBAAIgoDs7Gz07dtX8SwsLAzTp0/XuT9l69bWlClTEBISguzsbERFRWHGjBn46quvdFp1R0T6JT59XPONqPmZL3h4QzQ2AS6l1Hz7BIABNSIiIr0T934JDHq9eis1NQcePqjeOqle279/P6ysrDB16lRFWukVTKIoIj4+HiNHjkTXrl0BANOmTcOECRNw9uxZ9OrVC3/99Rd++eUXfPTRR4pVbWPHjsVHH32Et956C5aWljh58iSKioowdepUGBgYoGXLlsjMzMQPP/ygCKjFx8ejc+fOGDZsGAAgJCQEv/32Gw4cOKB0zlR1EQShUtvq6vNWPHXvrSQ49OTJE0ilUiQkJCgF3QDlyw3u3r2Lq1evQiqVIiMjA35+fuW2mZiYCF9fX8VqxqZNi7c0PXr0SCVvbm6uyuo2S0tLDB06FEOHDsW8efMwYMAAxMTEYP369eW2e+XKFQDF567Z2toiMTFR8azkXLTKKvl3kJaWpnJTqTqWlpawtLSEq6sr3Nzc0LVrV5w/fx5dunSpUj+IqBbduF7zbai7zf3yb5BMWwj5+2PUFKCawIAaERFRfdTMTCWgJvgOhHj8gH76Q3XeuXPn0KlTJ0RFReGPP/6ApaUl+vfvrwhy3b59Gw8ePEDHjh0VZYyNjeHm5obLly+jV69euHz5MkxMTBRBBAB4+eWXIQgC0tLS0K1bN1y+fBnt2rVTWpnVqVMn7N+/H48fP0bTpk1x+fJlDBkyRKl/nTp1wtmzZzX2v7CwUGm1mCAIMDIyUvy+vih96yQAXLhwAc7OziqBLaA4KFbdK9eA4sP0ZTIZcnJyFJcfqDN79my0bdsWb7zxBubMmYM+ffrgpZc0r8Y9ePCg0tZVCwsLWFpaIiUlRXGJAlAcYMvMzISLi4vGuho1agQnJyeVWz7Lksvl2Lp1K1q1agUPDw9IpVI4OzuXW0YXHTp0gLu7OzZt2oRhw4apnKOWm5ur8Ry1kgBmQUFBtfVHFyX/bsr+l2oHx10/qmfcVctWpj7x72zId8ZA8upoCG1eVn6o7md7WiokTU0hr2K7+vCi/n3XOaDGszWIiIiqRnxwr+YbUfPNJVzcgXMnAN4ASmrcvn0bSUlJePXVVzFixAikp6dj27ZtMDAwQN++ffHgwQMAqoeom5mZKZ49ePBAZdWQVCpF06ZNlfKUPburZBXQgwcPFHnLa0edffv24ZtvvlG8dnZ2xsqVKzWeofXs2bNqWWFWm6vUSrYkRkZG4p133kFKSgq2bt2quH1TEARIpVJFn1q2bInTp0+jR48eaNy4MczNzRWBTAMDA0U+iUSitEqv7Gug+M+xJK1t27Z4/fXX8f7772Pp0qV4+eWXkZOTgxMnTqB9+/YIDAzEli1bcP78eRw7dgwODg44evQowsLCkJCQoNge3LNnTyxYsACvvvoq7ty5g5SUFOzYsUOp3SlTpuCzzz6DnZ0dvL29cf/+fURFRaF58+YYNmwYDA0NkZiYiH379mHEiBFwdXWFKIo4ePAgjhw5gvXr18PQ0FDxvh89eoR79+7h2bNnuHTpEv75z3/il19+wb/+9S+lzyLqqBuX0kraMDQ0VMrz6aefIigoCCNHjsTMmTPh5uaGJ0+eIDExEceOHcP+/ftx/vx5/PLLL+jWrRvMzc2RmZmJjz/+GK1bt0b37t1rfTVko0aNlD6/AVCcl0e1i+OuH1UZ97xbWbhTJq3svydt/L1iNmRXL0P2xy9o+eM5pWc5jRtD3dcFLVq0QLapGeQPcyvdrj69aH/fdQqoNeSzNYiIiKqLmFwDtz2Vpe6by/w8SBZ+Avn86t8yRy8+uVwOV1dXvPnmmwCKA1LXr19HUlKS0plSddWIESOUVrWVfMt9584dFBWpBpgLCgqqfP5ZdZyhpgtRFBEUFISnT59iwIABkEqlGDduHN544w0UFhZCFEXIZDJFnxYtWoSIiAjs2LEDdnZ2OHPmjGIsioqKFPnkcjlEUdT4GgBkMplS2tq1a7F+/XosWbIEf//9NywtLeHl5QU/Pz+kpqYiIiICa9euhY2NDQoLC/Hhhx8iICAAK1aswIIFCwAUb4O8f/8+CgsLkZCQgM6dO8PU1FSp3UmTJqFJkyaIjo5GZmYmzM3N4ePjg927d8PAwACFhYVwcXFBkyZNsHjxYty4cQONGzeGs7MzVq9ejREjRqCwsFDxvoOCggAARkZGcHR0RM+ePfHxxx/D2dm5wj9LdeNSWkkbZVdLvvzyy4iPj8enn36KWbNm4d69e7CxsUGXLl2wdOlSFBYWwtDQEN9//z1WrlyJZ8+ewcbGBn379sXGjRshkUhq9e8ZUPzv4+bNmwCK/y3Z2dnh77//1vlMOKo8jrt+VMe4y2/eUEm7cekPyH/cDckrAyE4OGlVT9Hd24rfl/x7LCF79FBtmZs3b0Iukyu9fhHUpb/vBgYGWl1oA+gYUHvRz9ao7a0AL+qyxRcZx1w/OO76wXHXj+oYdwFA2alCpbYCPLgH8dQhCL0DIZhZKD9UE1AT//1PSP2HcisAqWVhYQFHR0elNEdHR5w5cwbA81Vkubm5SgfP5+bmonXr1oo8Dx8qT/JlMhkeP36sKG9ubq6y0qzkdek8ubm5Snlyc3PLPc+q7Kqg0vQ9Oa8upVfgffzxxyrPS/6sSvTv3x/9+/dXSvPw8EB2drZS2uzZszF79mzF63Xr1qnUHRkZqfTa0NAQ4eHhCA8PV9vX9HTlm+bMzMxUtuyW7sfBgwdV+goUr4wbO3Ysxo4dq9R26Tm9k5MTVq1apbYfJVq2bKnyvnVVdpzK6tmzp8Y2XF1dyz3LrV27doiLi6tS/6pb2X83oijWm39LLxKOu35UadzVlJNt+QT4IxmyIz8AnbpBMnYmBGMTNYX/V8UvPykd3SH/6xoEh+cLh0Q1OxEEv8Fq/92+SF60v+86BdRe9LM1dN0KUF1etGWL9QHHXD847vrBcdePqox7rkRA2e8VK7UV4KM5KEy/hEapv8J27ValZ9kQlQJnpdvJqmK7+sS/7zWnTZs2uHFD+Vv1GzduKOZJNjY2MDc3x2+//aYIoD19+hRpaWmKQIi7uzuePHmCq1evKs63+v333yGKItzc3BR5/v3vf6OoqEgx10tJSYG9vb3iEHp3d3f89ttvePXVVxV9SUlJKff8LXqxdevWTeV4FyKiF5K6QNn1Ul8y/PozxPg4CEHvqi0u5tyB/PMVSmnypdMg3fzd8wR1t3wOCSn+b7c+EI/G69xt0p1OAbUX/WwNXbcCVFVdWrbYUHDM9YPjrh8cd/2ojnGXpZxXSbuR/ReQfR1wcIJQ5tBqTYrSLwEACi6lqCzpl2vYHlQ2H7cC6E6XrQAvkldffRWLFi3C3r170bNnT6SlpeHw4cOKlf+CIGDw4MHYu3cvWrRoARsbG+zatQsWFhaKnQmOjo7o3LkzNm3ahAkTJqCoqAhbt25Fz549YWlpCQDo3bs34uLiEBMTg+HDhyMrKwsJCQl45513FH0ZPHgwli5diu+//x5eXl44deoU0tPTa+SGT6obSu+AISJ6oZlbqqaVXWH/RPX2YoUHORW3oS6gZmpe/N9XBhcH1ATt5pMVEeVyreemDY1OAbUX/WwNfW0FeNGWLdYHHHP94LjrB8ddP6oy7urKyWO3QDz8PYSufSD0GwLBrV35dciV15+p1KluK4D/UG4FII3c3NwQHh6OnTt3Ys+ePbCxscE777yDPn36KPIMHz4c+fn52LRpE54+fYq2bdti/vz5inNyAWD69OnYsmULIiMjFZdPld6uZ2xsjIULF2LLli2YN28emjVrhtdff12x4wEoXi03ffp07Nq1C//+97/RokULzJkzh+fkEhFR/VDVIyzKBtRaPd8BCMn/6jYyrlobAOQnkyDu3grJ9EUQ3NpXub76RqeA2ot+tgYREVGdkHNbJUk8/H3xf8+egHj2BCQL1kJorX57m5j3FPJF/1BKk/+4G5JXRz9PkKlu+BSGBFeh09QQeHt7w9vbW+NzQRAQHByM4GDNf5eaNm2quGhKEycnJ5Uzucrq0aMHevToUX6HiYiI6hp13/uVuWFdPJEIvD1Np2rlu7dAvH4VkpmRKl+cSoa/WepV9Z03K34ZXdz2ynmQTJ0PwbN7tdVdH+i0bk+XszVKlJyt4e7uDkD5bI0S6s7WSE1NVdqGqelsjdJ4tgYREb0IhE7dKswjpqVqfvbzCZXtAOK3O5QzqVuh1rT4yAUhlFuriIiIiGqEupX0auZlGotfvKA+PWk/8OdvwMULqivUHJ3VldC6TW3IN6yoOFMDo1NA7dVXX8WVK1ewd+9e/P333zh58iQOHz6MAQMGAFA+W+PcuXO4fv06PvvsM41na6SlpeHSpUtqz9YwMDBATEwMsrKycPr0aSQkJCidfzZ48GD8+uuv+P7775GdnY3du3cjPT0dAwcOrK6xISIiqhFCazflBDtHNZmqeFaFvMxES/p8Ubpg51D8mxYtq9YGAPHhfYi//ASxbHtEREREDVLVAlni97vKzyCXqQToBMvmpV5UqXnSgU5bPnm2BhERUTVoZq78unETHStQP1ETnzwG5HIIzUyBMmesSRZG6diGduSLpwFPHkEIHA6MGqu48IeIiIioQaqNo17VXEqgoor9EO/8XbUKGgCdAmoAz9YgIiKqsrJbAUTV887EXf8E/IeopJdH/n7x+RmSDd+oPjQqfYW7oL4flfG/W6rEpP1Ao8YQXgutep1EREREL6yajaiJDx9UsIW0mr7c1CZo18Dx7lMiIiJ9u3614jyliPt2lJ/hwT3VtNJbAWqI+OPuGm+DiIiIqE6r4dvIxa83lB/sUuwW0L0fYs6d5y8kDBdVhCNERERU66o40Xr8sOI8pTVuorwVU/HbKp7xUVhQpfJEpJugoCAsXrxY43MfHx9s3ry5wnp27NiBLl26wNHREZs3b8batWsRGBhYnV1tEM6ePQt/f384OTkpHV9DRKRE3a3tNvZVq/PvvyrOo2NgT7bsfcjnjYNs/sTiBKm0Eh1rWHTe8klERERVVBtna5SWn1cz9aacrZl6iahS4uPjYWxsrHjt4OCALVu2KF3a9ejRIyxcuBBLlizB4MGDYWpqCrlcjvfee08fXVZr5syZsLOzw5tvvonu3bvj4MGD8PDwUMoTFBSE9u3bK46IuXjxIlavXo0LFy7g8ePHsLa2hqenJz788EM0b94cWVlZ6N69u6K8iYkJHBwc0KNHD4wfPx4uLi5a9S0oKAgjR47Em2++iYiICLRv3x5ff/01TExMKi5MRA1D2XlekZrtmc1ta679yu74LNkxUXJ2WlUvyGoAOEJERES1roYjannPKshQcoZazXaDiGqXlZUVjIyMys2TnZ2NwsJC+Pv7w9bWFkZGRjAxMYGlpWUt9bJ8MpkMhw4dQv/+/bUuk5OTg+DgYJibm2Pnzp04duwYoqKiYGtri6dPnyrl3bVrF5KTk5GUlIR58+bhypUrCAwMxIkTJyps5/79+zh37pxiNV9mZiZ69+4Ne3t7mJmZqeQXRRFF6j5IE1E9V2aCVVRYcZ5a6IbOJLxoqiIMqBEREdW2Gj5bQ75+qXJCU9Nqq1ssvdqN31xSfSGKgLygEr/yK1mu1C9dt+TIZFiwYAHatm0LDw8PrFq1CuL/6ii95dPHxwcAMG7cODg4OMDHxwexsbHw9/cHUHy5l4ODA7KyslS2fL7//vsYO3YsYmJi4OnpiQ4dOmD+/PkoLHz+oTA/Px+RkZHw9vaGm5sbhgwZgtOnTwMA8vLy4Ofnh7lz5yryZ2Zmwt3dHbt27Sr3/Z07dw6Ghobo3Lmz1mNy9uxZPHr0CGvWrIGHhwdatWqFXr16ISIiAq1atVLKa2FhARsbGzg5OWHAgAGIjY2Fp6cnwsPDIavgAO7Dhw/Dw8MDeXl5cHBwwP379zFr1iw4ODggNjYWp0+fhoODA44cOYKBAwfC2dkZP//8M+RyOaKjo9G9e3e4uroiICAAP/zwg1Ldly5dQmhoKF566SV06tQJYWFhuHdPzXmYRFT3lf25ri6gdj+n2poTQqeWTdG5DjE/X01i5frTkHDLJxERUR0k9PKvfOHc+8qvm5VZOSFU7pZP2cIpwK1sCP5DIQmZwMNqqf4QC2FzdYlemr7tEgEIjbTOHxcXh5CQEPzwww9ISUnB3Llz4eDggDFjxijli4+PR8eOHREVFQU/Pz9IpVIYGxvD3t4eISEh+PHHH2Fvbw8rKyu17Zw+fRo2NjaIi4tDRkYGpkyZgg4dOijaWbhwIS5fvowNGzbA1tYWBw4cQGhoKA4dOgQXFxdER0dj6NCh8Pf3R0BAAMLCwuDr64uQkBBFGw4ODoiKikJwcLAiLTExEQEBAcrnPlbA2toaRUVFSEhIwJAhQ3QqK5FIMH78eIwbNw4pKSnw9PRUbA+Ni4tDz549lfo2YMAA2NvbIzk5Gb6+vggPD8ewYcPQrFkzJCcnAwBWrFiBxYsXo1WrVjAzM0N0dDT27t2Ljz/+GM7Ozvjpp58wffp0WFlZoUePHsjNzcXo0aPxxhtvYOnSpcjLy8Py5csxadIkxMXFaf1eiKiu0CKgdjOr2lqTvDJQOUHHSwnkP+6G+K3qhVfy2IrP5GzoGFAjIiKqbdoEskyqb1WZisqu4L+VDQAQD38PMKBGpBf29vaIiIiAIAhwc3PDpUuXsHnzZpWAWkmgzMzMDDY2Nop0CwsLxfPS6WWZmZlh+fLlkEqlcHNzg7+/P06ePIkxY8YgOzsbsbGx+Pnnn2FnZwcAmDx5Mo4ePYrY2Fh88MEH8PDwwNy5cxEeHo7hw4cjOzsbX375pVIbrq6uMDVV/ll38OBBLF26VKcx8fb2RlhYGKZNm4Z58+bB09MTvXr1QlBQEKytrSss7+bmBgDIysqCp6cnDAwM4OrqqrR9Nj8/H8eOHcPs2bMhlUphY2MDQRDQrFkzlXGcM2cOfH19FeWio6Oxa9cudOnSBQDg5OSEs2fPYseOHejRowe2bdsGDw8PfPDBB4o61q5di65duyI9PR2urq46jQcRVY1Y1a3aZad5heq2fNYd6oJpYlEhcP60HnrzYmFAjYiIqE6qxnX2GgN4VWyDWz6pvhAMi1eK6cjQ0ACFhVX84CUY6pTdy8tLaQWWt7c3Nm3aVOF2RV25u7tDWuqGN1tbW6SmpgIAUlNTIZPJ0KdPH6UyBQUFioAdAEyaNAkHDhzAtm3bsGPHDpVz2o4fP670+sqVK7h16xZ69+6tc3/nzZuHiRMn4tSpU0hOTsbXX3+N6Oho7NmzB+3atSu3bMmW2ZJxbdGihUrfTp06hebNm6NNmzYV9qVjx46K32dmZuLZs2d44403lPIUFhYqLlr4448/cPr0abz0kupNgNeuXWNAjagWybZ/ir9+Ogbpin8CFupX8Fao7LzLUPuf8+JfmZVrU5t+6EC+fLb6Kh89hNCsBr/0fcEwoEZERFTbtJngVOM5a0LPsttHq+mQWR22VRHVaYKg07ZLBYlhvV2paajmA2BJ4OnJkyeQSqVISEhQCroBULrt8u7du7h69SqkUikyMjLg5+dXbpuJiYnw9fVFkyZNAABNmzYFUHwzaVm5ubkqq9ssLS0xdOhQDB06FPPmzcOAAQMQExOD9evXl9vulStXAEDlvLWyfSt9zlx5St+0+uTJEwDAV199pVjNV6JRo+K/c0+fPkVgYCDmz5+vUpetbQ3eBEhEKsSTSQAA+ZEfIHn9neqptEVL4IHqmYji9asQWinfMCyPmK70Whg8CmJ8Zbd+V2EuqSmw9/ghwICaAgNqREREta7iCY6YtB8YPa5aWhMGvKahEe0mWuKvP0N+IlE1/WzFt+IRUfUqOaerxIULF+Ds7KwS2AKKg2LVvXINADw8PCCTyZCTk6O4/ECd2bNno23btnjjjTcwZ84c9OnTR+0qrBIHDx5U2rpqYWEBS0tLpKSkoEePHor0R48eITMzEy4uLuqqAVAcrHJyclK55bMsuVyOrVu3olWrVooVY2WJooikpCRER0eXW5c67u7uaNy4MbKzs5XeQ2keHh6Ij49Hy5YtYWDAj2dEdUJVvtgsKHPAv4a65Mveh+Sf+8s991Hw6ql7QK0mv/CU1s8vkSqLo0FERFTPCZIyH7R1nGjJP/sQ+PVnlXTx9OGqdIuIKiE7OxtLly5FWloavv32W2zduhXjxqkPvjs6OuLkyZO4ffs2Hjx4UG19cHV1xciRIzFjxgzEx8fj+vXrSE5ORnR0NA4dOgQA2L59O86fP49169Zh5MiRGDhwIKZNm4aCggJFPb6+vkhISABQvJotJSVFZRXYxIkTFYf6Z2ZmIjk5GVOmTIGVlRUGDRoEAEhKSkJYWBiSkpKQnp6OtLQ0xMTE4MiRIxgwYIBSfffv38ft27dx7do1JCYmIjg4GMnJyVizZo0iKHnz5k34+voqgpcpKSnIy8tDt27ddB6rpk2bYtKkSVi6dCl2796NzMxM/Pbbb9i6dSt2794NAHj33Xfx4MEDTJ06Fb/88gsyMzNx7NgxzJw5s0YCokSkBVFeqWLyn49DvvL/ytRVTnCuku2US3H5VPVXLf5+AaKcP5dK8CsQIiIiHch/OYOHp3Ih9tRu649aVZjgiGW/9dQT+dEf9d0FogYpKCgIeXl5GDJkCKRSKcaNG4fQ0FC1eRcvXoyIiAjs3LkTdnZ2OHPmTLX1IyoqCuvXr0dkZCT+/vtvWFpawsvLCwEBAUhLS8OyZcuwdu1aODg4ACi++TIgIACrV6/GggULAADp6el4+PAhgOKgWOfOnVXOWZs6dSpMTEywYcMGZGZmwtzcHD4+PoiLi1NcGuDu7g4jIyNERkbixo0baNy4MZydnbF69WoEBQUp1Vdyy6iRkREcHR3Rs2dPrFq1Cs7Ozoo8RUVFSE9Px7NnzwAUr5zr169fpVePzZ07F1ZWVvjss89w/fp1mJqa4uWXX0ZYWBgAwM7ODt9++y1WrFiBN998E/n5+XB0dETfvn0hqadbionqvErO1cTNa9QklhM0k4sVLHOqgahYFYj//ieQnw9h0Ov67kqdIIhiNR7S8oK6c+cOCmvg5g1BENCiRQvcvHkTHObawTHXD467fnDc9UM2YRgAQBK+AkIb9duDKiL+fgHy9UufJ9g5An//pZJPuvk7je0rtHIFrqeX217ZesT0S5B/PBdobgvpRxVfia7Spg5tVZe69Pfd0NBQq5sDqW7QNM97+PChyhlcujI0NKyROWRD9O6776Jbt26YOnVqhXlre9wDAgIwffp0DBum/c/CF13pfx916edvQ8Jx14+SOY8QMAyS4PGVLq/ErT2Q9ofa/JINeyCUOrOybHnJgrUaLwgAAMnU+RA8uyuliTl3IJ83DjAwhHTjnsr1uTzmVpCu3qZbmQrUpb/vuszz+JUHERFRZTzIqULhMhMFc0u1ueSxX1RYk2TG4ir0g4iobujWrRuGDx+u726oKCgowODBg9GvXz99d4WIapF4+fdqrKycFWoVbfmsKLZkaq5rb6qOAV4FBtSIiIgqo5KTCfHRQ8j/d4NURXWJh7RY7SVU4n/lirM1OCEiorph6tSpiu2hdUmjRo0wa9YsxY2jRFR/yb/Z/vzF9avVWHF5AbWqzcUE17ZqEhWVV6luzTh/LMGAGhERUaVUbjIhX78UOH+6TFVVmJjUsfN15N9sg5hfN855IyIiItKWeHDv8xcGhpoz6lxxVS4lqMrBu5UvWq7yAoQNTN2ahRMREb0o5JWcpVxLU02ryg1PFdzYKQwJ1rlMVYgH90H8/t81Vj8RERFRjZNKK86jrfICahUFpyysKtFgyTxPhMjgV41iQI2IiKhSqvFrvxpcoSb09K983ZUkZmfWeptERERE1UXw7qVzGbGwQMODcoJaRUUaH0lmfwjBXHNATRj6hoYH/wuoyWSQL/4HxKJqvsTlUW711vcCY0CNiIhIS6VvHRL/uladFVe+bEVnqJmoO/enhs9Q49lsRERE9AITrG11LiNuj1b/oLxdDeXc1C607Vh+g3lPK+7UrWzg6uWK81GlMKBGRESkJfmSaYrfi5d+rcaKq7Acv6IVasZqAmo1fVgt42lERET0IqvEpU/iz/9R/+CvDM2FpAY6t6No76dj6h+UPdmjnBVy3BJaNQyoERERaUF8cA+4maV4Lbi0qb7K69tkpipnwhERERHpmXjrRu20k3FZaQcEHFsDAIRur1RcWMutl/I1CyA+vK/+oUymVR2kHgNqRERE2pDX4ISjSmeolXNorqZvV0vO1uBKMiIiIiIV4n+P1E47+76GfN2SUgnFkzOhV1XOwFW9fEq+bb36rPXtS91axoAaERGRVpQnJ0Lrl3SuQeOy+kqu6JIsjIJQzi1UQuiU8iu4fxeyz1dUqu1y5dyp/jqJCEFBQVi8eLHG5z4+Pti8eXOF9ezYsQNdunSBo6MjNm/ejLVr1yIwMLA6u1rvnT59Gg4ODsjNrZ7Duau7PgDIysqCg4MDfv/992qrk4hqwB+/qKZpcyO7o7P6dHVlr2k4q03kCrWqYECNiIioMpo207mIqOnbwXJWipV3M5Pg5FZ+gxonY6XSf/kJ4tMn5dejq9u1s02CiJTFx8cjNDRU8drBwQEHDhxQyvPo0SMsXLgQ//jHP3D+/HmEhoZi8uTJiI2Nre3uajRz5kysXLmy3IBQ2eDixYsX8e6776Jjx45wcXGBj48PJk+ejLt37wJ4Hlwq+eXu7g4/Pz/Mnz8fV69e1bpvQUFB2LlzZ9XfJBFRVUm0CLpVhCvUqoQBNSIioloi/nRU/YMHOZoL5edXvsE7f1e+7P+I+XlVroOIaoeVlRWMjIzKzZOdnY3CwkL4+/vD1tYWRkZGMDExgaWlZS31snwymQyHDh1C//79tS6Tk5OD4OBgmJubY+fOnTh27BiioqJga2uLp0+Vb8HbtWsXkpOTkZSUhHnz5uHKlSsIDAzEiRMnKmzn/v37OHfuHFfzEVHNKe8YkM7dyyRoCKipS9Z0iRVvZq8SBtSIiIi0UXbCUah55ZjOHj/U/KzsIbJGxgAAoWufCqsVUzXcRFpmoiX+J0HzdtQbWerTieoRURRRJM/T/ZesEmXK/BJ1/DAjk8mwYMECtG3bFh4eHli1apWijtJbPn18fAAA48aNg4ODA3x8fBAbGwt//+JzeXr06AEHBwdkZWWpbPl8//33MXbsWMTExMDT0xMdOnTA/PnzUVjq515+fj4iIyPh7e0NNzc3DBkyBKdPnwYA5OXlwc/PD3PnzlXkz8zMhLu7O3bt2lXu+zt37hwMDQ3RuXNnrcfk7NmzePToEdasWQMPDw+0atUKvXr1QkREBFq1aqWU18LCAjY2NnBycsKAAQMQGxsLT09PhIeHQ1bB4dyHDx+Gh4cHrK2t1T7/8ccf4efnB2dnZ/j4+CAmJkbpeX5+PpYvX44uXbrA2dkZvXr1wr///W+1dT179gyhoaEYPny4Yhvozp078corr8DFxQW+vr7Yvn27Upnk5GT0798fLi4uGDRoELd6EulA15/FNaakH+p2GZRdkabNttCKcIValVT+jlYiIqIGRXmiJd/3NaRaBLWqSr74HxAmzoWka+/iBANDAIAweFTFhZuUv1KlhLj3K6CpKYQ+alaE8MZOagBkYj72pE7QS9uvt9sMA6GJ1vnj4uIQEhKCH374ASkpKZg7dy4cHBwwZswYpXzx8fHo2LEjoqKi4OfnB6lUCmNjY9jb2yMkJAQ//vgj7O3tYWVlpbad06dPw8bGBnFxccjIyMCUKVPQoUMHRTsLFy7E5cuXsWHDBtja2uLAgQMIDQ3FoUOH4OLigujoaAwdOhT+/v4ICAhAWFgYfH19ERISomjDwcEBUVFRCA4OVqQlJiYiICAAgg4fFK2trVFUVISEhAQMGTJEp7ISiQTjx4/HuHHjkJKSAk9PT2RlZaF79+6Ii4tDz549lfo2YMAAtfWkpKRg8uTJmDVrFoYNG4Zz585h/vz5sLCwULy/GTNm4Pz581i2bBnat2+P69ev4969eyp15ebm4u2334aJiQl27doFIyMj7N27F2vWrMGHH34IDw8P/P7775gzZw6MjY0xevRoPHnyBO+88w58fX0RHR2N69evY8mSJSp1E5EGtRRQk4Qtgjx6mfou3Lrx/ObOUj/HhDcnQzy0H5Kg9yC/8F8tWlH3M1DDz0UG1KqEATUiIiJtNCrzgbcatlNqS/znKqAkoFZCiw+MQitXTU9U2zh7AlAXUONEi6hOsbe3R0REBARBgJubGy5duoTNmzerBNRKAmVmZmawsbFRpFtYWCiel04vy8zMDMuXL4dUKoWbmxv8/f1x8uRJjBkzBtnZ2YiNjcXPP/8MOzs7AMDkyZNx9OhRxMbG4oMPPoCHhwfmzp2L8PBwDB8+HNnZ2fjyyy+V2nB1dYWpqalS2sGDB7F06VKdxsTb2xthYWGYNm0a5s2bB09PT/Tq1QtBQUEaV5OV5uZWfB5lVlYWPD09YWBgAFdXV6Xts/n5+Th27Bhmz56tto5//vOf6N27N2bOnKl4b1euXEFMTAyCg4ORnp6O77//Hv/+97/h6+sLAHByclKp586dO5g8eTKcnZ3x+eefo1GjRgCAtWvXYvHixRg8eDAAoFWrVrh8+TJ27NiB0aNHY9++fZDL5VizZg2aNGmCNm3a4ObNm/jggw90GEmihqyWVqh18FLf+oMcyBdOVvtM4jcY8Bus+sDYRPt2NU0bOc+rEgbUiIiItFLHtgJooxp2AvBsDWoIpEJjvN6u4tsxyzI0MERhOReHaNu2Lry8vJRWYHl7e2PTpk0VblfUlbu7O6SlbhG2tbVFamoqACA1NRUymQx9+iiv0i0oKFAE7ABg0qRJOHDgALZt24YdO3aonNN2/PhxpddXrlzBrVu30Lt3mS8QtDBv3jxMnDgRp06dQnJyMr7++mtER0djz549aNeuXbllS7Z6lYxrixYtVPp26tQpNG/eHG3atFFbx5UrV1RWr3Xt2hVffPEFZDIZLl68CKlUih49epTbl5CQEHh6eiImJkYx/k+fPkVmZiZmz56NOXPmKPLKZDI0a9ZM0X67du3QpMnzL3+8vb3LbYuISrn4S+20o2lulpWhZcbnJG9M1NCGmrIFBerzarMTwdkdyLhccb4GiAE1IiIibdS5wJLqZEnoPwJi4r5y8xQnq0nXtOKN31xSAyAIgk7bLksYSA0hyqUVZ3wBGRoaqqSVBJ6ePHkCqVSKhIQEpaAbAJiYPF8xcffuXVy9ehVSqRQZGRnw8/Mrt83ExET4+voqgkJNmzYFUHwzaVm5ubkqq9ssLS0xdOhQDB06FPPmzcOAAQMQExOD9es13LD8P1euXAEAlfPWyvatKpcRlA50lcff3x/x8fG4fPmyIhD45EnxTcyrV6+Gp6enUv6y409ElSM/8r2ee6A8DxOz0iG08Si/iLWdVnUBAJ4+Vp9Vi3me4NkDIgNqavFSAiIiIq3UlYBayWG1ah79b2uQSl6taAqoVe+qFyKqmuTkZKXXFy5cgLOzs9rAiqGhYbWvXAMADw8PyGQy5OTkwNnZWelX6W2ks2fPRtu2bbFu3TosX75cEbjS5ODBg0q3e1pYWMDS0hIpKSlK+R49eoTMzEy4uLhorKtRo0ZwcnJSueWzLLlcjq1bt6JVq1bw8FD/4VUURSQlJWk8Pw0AXnrpJZw9e1Yp7ezZs3BxcYFUKkW7du0gl8vx3/+Wf/7R/PnzMWrUKAQHB+Py5eIPsNbW1rCzs8O1a9dUxrskCPjSSy8hNTUVeXnPb2a+cOFCuW0RUSm/19a/F/XzLfHUIeWERhWvXhYMVL/4KI949U/VRG0Cak6af9Y2dAyoERERaaO24mnNbct/Xl4/bB3K5NWQWYezaiErKr8/RFSrsrOzsXTpUqSlpeHbb7/F1q1bMW7cOLV5HR0dcfLkSdy+fRsPHjyotj64urpi5MiRmDFjBuLj43H9+nUkJycjOjoahw4Vfyjcvn07zp8/j3Xr1mHkyJEYOHAgpk2bhoJS2458fX2RkJAAoHg1W0pKisoqsIkTJyI6Ohp79+5FZmYmkpOTMWXKFFhZWWHQoEEAgKSkJISFhSEpKQnp6elIS0tDTEwMjhw5ohIEu3//Pm7fvo1r164hMTERwcHBSE5Oxpo1axRByZs3b8LX11cRvExJSUFeXh66deumcUwmTZqEkydP4pNPPkF6ejp2796Nbdu2YdKkSQCAli1bYtSoUZg9ezYOHDiA69ev4/Tp0/juu+9U6lq8eDFGjBiB0aNHIy0tDUBxcPKzzz7Dli1bkJ6ejtTUVMTGxmLTpk0AgBEjRkAQBMyZMweXL1/G4cOHVW4ZJaI6QMOOAPH8KeVsDq2r0IaGdHW3ypcKqAmvhaovp0Vwr6Hilk8iIiJt1MaWT5sWkMz+EPL/U/1wLIoixKRvgSf/2/pUakImmb8WYnoqhG6+ELdEPS8k19RnHbZ81sDqFiKqvKCgIOTl5WHIkCGQSqUYN24cQkPVfwhavHgxIiIisHPnTtjZ2eHMmTPV1o+oqCisX78ekZGR+Pvvv2FpaQkvLy8EBAQgLS0Ny5Ytw9q1a+HgUBzoX7FiBQICArB69WosWLAAAJCeno6HD4s/4CUlJaFz584q56xNnToVJiYm2LBhAzIzM2Fubg4fHx/ExcUpLg1wd3eHkZERIiMjcePGDTRu3BjOzs5YvXo1goKClOoruWXUyMgIjo6O6NmzJ1atWgVnZ2dFnqKiIqSnp+PZs2cAilfO9evXDwYGmj86vfzyy4iJicGaNWuwfv162NjYYM6cOUo3mH700Uf4+OOPMX/+fNy/fx/29vaYPn262voiIiIgl8sxevRoxMXF4c0334SRkRE2btyIDz/8EMbGxmjbti3Gjx8PoHir7fbt2xVbXV966SUsWLAAEybo5/ZaIqoiHW4rrlKdpea3Qg8/iN/uUM0jZdhIE44MERGRNgo1HOZanZoYAYKGxeMX/gsxbluphFLXqTu/BMH5JdUytvY6NK4poMYVakR1xTfffKP4/ccff6zyvGzArH///kpbKIHi7ZrZ2dlKabNnz1a6vXLdunUqdUdGRiq9NjQ0RHh4OMLDw9X2NT09Xem1mZmZypbI0v0ou92zhFQqxdixYzF27FiltgsLn18G4eTkhFWrVqntR4mWLVuqvG9t8yYmJqoEvnr27KlS36uvvopXX31VY71NmjTB0qVL1d5iqq6+ZcuWYdmyZYrXI0aMwIgRIzTW7+3tjaSkJKU0bd8zEdUOQRAgBAyDeEh1dapyxipsJtQlGFd6y6dEw5mMEm5s1IQjQ0REpAW139hVt1s3NMa1xNs3dK5O6KP64bT4gZpGNAQMRS1WqAkhGm6ZIiLSUrdu3TB8+HB9d0NFQUEBBg8ejH79+um7K0RUT0iCxwPtPSvIVJUVajqULf3FqaZLTjQF2ogBNSIiIm2IPx+v+Uby86BxEvT0ifLrWxWvOhB0uf3tz9/Up2uxQk1o31n7doiI1Jg6dapie2hd0qhRI8yaNUtx4ygRkTri3Vu6Fagw5qU+g9Clt27taKhTlMsgXvgvcO/O88eaVqLVwO7T+oJbPomIiF4A4oE9ygmW1lWoTcOBuLn3IZhZKCfKKr79qWp9ISIiInqxiccSdCtQdktnYyMg/1mp5xoCaiPegvjnbxAChpVTt4b0Uje3iyeTIH69oUw5DQW5Qk0jrlAjIiKqS7Q9q83IuPrbVrcarVSaZPI89eX4zSURERE1ZLpeIlA2f+lgGgAUFUIdwaYFJGu/gmTwKN3aA5TOSxN/v6D6XNMKtRYtdW+rgWBAjYiIqC7R9lbNqhwQq3HOp+ZB6f64d9BQjNMJIiIiasB0/XKxogBc3tNyilbUmIbnpcupm7tpmM9V3F7DxRkwERFRXWJrD3j31CJjLU1uSm0P0Dj54zyLiIiIGjQ1kyErG83Ziyo6o7YKkysN8zXx7t+6lWvXCZLZH1a+Hw0AA2pERER1iCAIkE6eBzg6V5Sx8o2IOuQtvQ1UY0CN0wkiIiIiJTYtND+7ca38srpcLKUlMXbL8xcXTqtmKLP7QfDwgtC2Y7X3oz7hDJiIiKgWiA9ydCtQ4Wp+DRls7LXpjVZdEJ8+1m4LKrcCEBERUUNWZi4kjHwHkvGzNOd/cK8G+1LZchIIpc9my8+vlu7UZwyoERER1QIxbptuBSoKUmm6/WnAiOLfeJWzbVSsOKAm/nIG8hlvQtz7VYV5GVAjIiKihq1MQK1HXwimFhryakGLuVqlqs29r/mhIPBGTx0xoEZERFQLxKdPdCxRyYBan/6QRHwGycQ5OrYHpfPS5Ls2q3mufnLHw2qJakdQUBAWL16s8bmPjw82b1bzb7eMHTt2oEuXLnB0dMTmzZuxdu1aBAYGVmdXqZplZWXBwcEBv//+u767QtRgiFUJalX1OIyWFRz9UX7jmh+Vc5u8UJULrxoojhgREVGdVMEkTlNATRAg2LeCUJmzN0pfQCA1KP85EdU58fHxCA0NVbx2cHDAgQMHlPI8evQICxcuxD/+8Q+cP38eoaGhmDx5MmJjY2u7uxrNnDkTK1euLDeIVDa4ePHiRbz77rvo2LEjXFxc4OPjg8mTJ+Pu3bsAngekSn65u7vDz88P8+fPx9WrV7XuW1BQEHbu3InTp0/DwcEBubm5Or23ypYjoton/nxc+8z37ii/ruJ3jYJJs6pVoIkuQUJ+YVohBtSIiIhqg7pJSTMzzfkrmvBUaZKjoW6ZvPxictXnwpCQKvSDiKqTlZUVjIyMys2TnZ2NwsJC+Pv7w9bWFkZGRjAxMYGlpWUt9bJ8MpkMhw4dQv/+/bUuk5OTg+DgYJibm2Pnzp04duwYoqKiYGtri6dPnyrl3bVrF5KTk5GUlIR58+bhypUrCAwMxIkTJyps5/79+zh37hxX8xHVRwZqvkjM0j7YLv50VDmhsKJbPDWTTJ5X6bIAyp8jihXM9WSF2tVDABhQIyIi0h8jY83PsjLKL1sDy/LFoz8+f6HujA1DQ+UuTF8CyfA3q70fRLVOFCHI5Dr/gkxWqXJKdei4pUgmk2HBggVo27YtPDw8sGrVKsW2pNJbPn18fAAA48aNg4ODA3x8fBAbGwt/f38AQI8ePeDg4ICsrCyVLZ/vv/8+xo4di5iYGHh6eqJDhw6YP38+Cguff9DKz89HZGQkvL294ebmhiFDhuD06eJb4/Ly8uDn54e5c+cq8mdmZsLd3R27du0q9/2dO3cOhoaG6Ny5s9ZjcvbsWTx69Ahr1qyBh4cHWrVqhV69eiEiIgKtWrVSymthYQEbGxs4OTlhwIABiI2NhaenJ8LDwyGr4BKWw4cPw8PDA3l5eRg1qvjg7vbt28PBwQHvv/++YlwWLVqkWCn32muv4ZdffgFQvEpOU7mjR4/itddeQ7t27dChQwe8/fbbyMzM1HoMiKiKOnat3vpKB6Z0JHiXcw6udjVofqTmy9HSxP+WCgyWM9cUb93QtVP1kpowLBEREVW7sh+abR0gePWAmPBNJSuswreGGj6/i0d/BN6cVPwi/5lqi01NlROk/F6O6gdBLqLFbxf10vbNlztAlGr/7zkuLg4hISH44YcfkJKSgrlz58LBwQFjxoxRyhcfH4+OHTsiKioKfn5+kEqlMDY2hr29PUJCQvDjjz/C3t4eVlZWats5ffo0bGxsEBcXh4yMDEyZMgUdOnRQtLNw4UJcvnwZGzZsgK2tLQ4cOIDQ0FAcOnQILi4uiI6OxtChQ+Hv74+AgACEhYXB19cXISHPV7U6ODggKioKwcHBirTExEQEBATodDajtbU1ioqKkJCQgCFDhuhUViKRYPz48Rg3bhxSUlLg6emJrKwsdO/eHXFxcejZ8/kH28TERAwYMAD29vbYvHkzJkyYgOPHj6NZs2Zo0qQJAGD58uWIj4/HunXr4OjoiA0bNmDMmDE4efJkueWePn2KiRMnol27dnjy5AnWrFmD8ePHIzExERKea0RU89SdE1uVewFq5k6BqqtoN0Lp20dL/yw1bKR8/lp+XvX26wXFn85ERER6IFkaDWHoG1WoQU8zNde2z39f1QN3iUhn9vb2iIiIgJubG0aOHImxY8eqvYigJFBmZmYGGxsbxXZQCwsLxXMbGxtINZy3aGZmhuXLl8PNzQ2BgYHw9/fHyZMnARRvG42NjcWmTZvg4+OD1q1bY/LkyejataviLDYPDw/MnTsX4eHhWLJkCbKzs7Fq1SqlNlxdXWFqqhyoP3jwoE7bPQHA29sbYWFhmDZtGjw8PBAaGoqNGzfizp07FRcG4ObmBqB4BRkAGBgYwNXVVWn7bH5+Po4dO4b+/ftDKpXC3NwcANC8eXPY2NjA1NQUT58+xVdffYWFCxeiX79+cHd3x+rVq9GkSRPs2rVLYzkAePXVVzF48GA4OzvDw8MDUVFRSE1NxeXLl3UaCyKqLHXzqqpcSqDH7ZLlNa3Lebil5nmSyM8h9OhX6mFdjRjWLq5QIyIi0gepVH+3Y1bl1qrKXHZAVMeJEgE3X+6gczkDQwMUVeGcnJK2deHl5aX0s8Pb2xubNm2qcLuirtzd3ZWCbba2tkhNTQUApKamQiaToU+fPkplCgoKFAE7AJg0aRIOHDiAbdu2YceOHSrntB0/rnzg95UrV3Dr1i307t1b5/7OmzcPEydOxKlTp5CcnIyvv/4a0dHR2LNnD9q1a1du2ZItsyXj2qJFC5W+nTp1Cs2bN0ebNm001pOZmYnCwkJ07fp861jJ9tUrV66U24erV69izZo1SE5Oxr179yD/37as7OxstG3bttyyRFQN1M6NqjBPs7arfNmapFNArdRvm9sCw96A+N8jxQlcOQuAATUiIqJKE+XySl8xXtVgWo3d/lQRSamAWlUCc0R1iSDotO1SQSqFWMF5NC8qwzJnJgLPA09PnjyBVCpFQkKCygo3ExMTxe/v3r2Lq1evQiqVIiMjA35+fuW2mZiYCF9fX8U2yKZNmwIovpm0rNzcXJXVbZaWlhg6dCiGDh2KefPmYcCAAYiJicH69evLbbck2FX2vLWyfavJywjeffddODo6YtWqVbCzs4NcLke/fv2Uzq0johpUjT/LJdMW6e9LU6D81XFFRZD/oOWtzmXneaVfc5cCAG75JCIiqryLF7TP+/v56mu3ypO08gNh5QYISrfN25+Ial1ycrLS6wsXLsDZ2Vnt1k1DQ8NqX7kGFG/nlMlkyMnJgbOzs9IvGxsbRb7Zs2ejbdu2WLduHZYvX17hKq2y2z0tLCxgaWmJlJQUpXyPHj1CZmYmXFxcNNbVqFEjODk5qdzyWZZcLsfWrVvRqlUreHh4qM0jiiKSkpIwYMAARVpJwLH0+LZu3RqNGjXC2bNnFWmFhYX45Zdf4O7urrHcvXv3kJ6ejhkzZqBPnz546aWXkJubW26/iaiG2DpUvQ4nV7XJwisDq163Vsq/lEDc/6+abaMBYUCNiIioksQnj/XSriTis5ptoLyA2qOHNds2EZUrOzsbS5cuRVpaGr799lts3boV48aNU5vX0dERJ0+exO3bt/HgwYNq64OrqytGjhyJGTNmID4+HtevX0dycjKio6Nx6NAhAMD27dtx/vx5rFu3DiNHjsTAgQMxbdo0FBQ8P9Ta19cXCQkJAIpXs6WkpKisAps4cSKio6Oxd+9eZGZmIjk5GVOmTIGVlRUGDRoEAEhKSkJYWBiSkpKQnp6OtLQ0xMTE4MiRI0pBMAC4f/8+bt++jWvXriExMRHBwcFITk7GmjVrFEHJmzdvwtfXVxG8TElJQV5eHrp166Y0toIg4NChQ8jJycGTJ09gbGyMt956Cx9++CGOHj2Ky5cvY86cOcjLy1NcxqCunLm5OSwsLLBjxw5kZGTg5MmTiIiIqLY/LyLSglg89xE6dnme9iCncnVp2L0glFz8pE+6rMQrb4WajscV1Ffc8klERKQNt3ZAWmqZRP1seRRatKxaBRV1u7zzNf7KKNURzZMpURT1u92BqJ4KCgpCXl4ehgwZAqlUinHjxiE0NFRt3sWLFyMiIgI7d+6EnZ0dzpw5U239iIqKwvr16xEZGYm///4blpaW8PLyQkBAANLS0rBs2TKsXbsWDg7Fqz1WrFiBgIAArF69GgsWLAAApKen4+HD4iB9UlISOnfurHLO2tSpU2FiYoINGzYgMzMT5ubm8PHxQVxcnOLSAHd3dxgZGSEyMhI3btxA48aN4ezsjNWrVyMoKEipvpLAlpGRERwdHdGzZ0+sWrUKzs7OijxFRUVIT0/Hs2fFtx0fPHgQ/fr1g4HB849OLVq0wOzZs/HRRx9h1qxZCAoKwrp16zB//nyIoojp06fjyZMn6NixI/71r38pLiPQVG7Dhg1YvHgx/P394eLigmXLlqn0nYhqUEmwqNTcRfz5ODAhXPe6NGyHFCQVn0MrvD1N9/ZUKinnmS5nqJW93V2pDc7xAAbUiIiItNOQzgvTdrJV3mRKlAMCLzAgqk7ffPON4vcff/yxyvOyAbP+/fur3Jjp4eGB7OxspbTZs2dj9uzZitfr1q1TqTsyMlLptaGhIcLDwxEerv7DZnp6utJrMzMzpa2QAJT6oel2T6lUirFjx2Ls2LFKbZc+W8zJyUnlBtGyWrZsqfK+tc2bmJiI6dOnq+SbOXMmZs6cqZTWpEkTLFu2DMuWLdNYv7pyvr6+OHbsmFJa6T7o0n8iqgRFQK0aNvFpsXpL6NoH4tkTqkX76HbLsc5k2l+ko3yrJ6D8jSwDagC3fBIREWlHXUDthY2xVdBxme4H80qWbQDsSp07In9hB4eI9KBbt24YPny4vruhoqCgAIMHD0a/fmU/WBJRvaJmhVqlaVNHTX5RW17VmuZ47TqpJAllz+YsXS+3fAJgQI2IiEg76iY+lZ1LaNqyaWRcyQqrjyiKOmwHeD4Agp0jhMGja6ZTRFTvTZ06VbE9tC5p1KgRZs2apbhxlIjqqZJ5XiVubxfLzhG1qEMU9XRDtKY5njYBPpEr1MpiQI2IiEgb1fhNomSW5q1AeieKEE8e0i5v2UlZQ9oWS0RERPVHVVaolQ2Oqds2WjatJqdMpd+DtZ3yM00BtWv/26bfqJHmeisRbKzvOCJERETaqEKwSMx7qpxgaq42n9DZp9Jt6KTc9yJC3PtltdRDRERE9EIomdM8faJ72bLHXKgJyklmKZ9DqRKEq0ZCqQtUYGGl3KxMQ0Dt2f/ed3lHdpQOzpXdDtpAMaBGRESkjf9NtATfAc/TtD0nrNTh2cWVqP/2s25cp65LIKxsXgbRiIiI6AVUEuAqfWh/p25altW85VOy9ktI/m8lhLYdq9jBalLBsR7C6HGanwlC+SvYGiAG1IiIiLRRMtFqbPQ8Sc3tTNoQNAXUmlR8hprqjUuVUO7CMh2CYmWDb4ynERER0YtI3VG52p5tK9e85VMwtYDg1q4KHatmFVw8JbRy0a4eHvMBgAE1IiIi7ZTMG0rfanQrW8uylZh0NGqsNll4a6rudelCly0IKhNEUe1viYiIiOq0kvlPZc4JUzlDreJz2AQDQ93bqQTB2V05oahQfUZFgf9n787jqqrzx4+/DhcQMcAVhAspi2SKqbmATqKIW2Y5OrikVga4trjGmBoupZkpY2MZirYY41fUqWYyFLTRn6NYuVBGagrqhATugKls997fH8SVC5f9wkV4Px8PHnLP+ZzPed8Pix/e57NUFLtsRlCcJNSEEEKIyijqLBlbaLbiiysuUvIpaMs2RospVqYYal9OPCWfshbdN2B46WM2TQ0PyNNKIYQQQjzILIqtDVbZfk2JckolknLKX14ovWGACVks/wBl9AsoT08wOK774VvjF3j7FP5bsm8nymVZcREhhBBC6FVn96fKKJXIMlNyqqzOY2WepEpCTQghhBAPoqI+THVGqBXrwymBT1fqEqWVI6qVm9BMeabq96tM/c5uKM5upU/c+d14+TZOhf+6PIwydBTYtyj/BtLnA2SEmhBCCFE5RZ2lam2nXpkyJRJqVdocoIqKd4JKdrbKmvIp/SYhzC4oKIjw8PAyz/v6+hIVFVVhPdHR0fTs2RNXV1eioqJYu3YtgwcPNmWojdbevXv505/+hJubW7lfKyFEPZN6sfDf4v28yvZ9ivWdylvUv14oYyaC7sfv9Z9bBL2IxZA/G7++th4sP6BkhJoQQghRFRWtPWFMGZ2X4pQ/P1fiSC1msFo53r/vo13RpacWu63x++qO7Idx5XcSldZOxaKWDJwQdS02NhZb2/vTx9VqNVu2bGHYsGH6Y7dv32bx4sUsWbKE4cOHY29vj1ar5cUXXzRHyEbNmTOHQ4cOkZGRUaN6IiIiGDduHEeOHCEyMpKTJ0+Sk5ODm5sbAQEBTJ06FWdnZwBOnz7NokWL+PHHH2nZsiXBwcHMnFm5NSv9/PxYvXo1/v7+/PWvf2XcuHEEBwfz0EMP1Sh+IUQdys8r/Pda8d87lezLFO/n1cOEkzLgSXQH9xR+7toe3f+SSxf6/XbdBtVASEJNCCGEqIw/nj7qzv2sP6S086rStcYofgPQXTqPMuDJmkRXJYpDsWH8JRNoZY2Mu3en4ordO1Q/KCFEjbVq1arCMmlpaeTn5xMYGIiTk5P+eLNmzWoztErTaDTs37+fTZs24enpqT8eGhpKx44dmT9/PpaWluTm5gKgUhWudxQZGcnBgwfZvn27/ho7Ozs+++wzFi5cyJgxY4iKisLNzY20tDR27drFxo0bWbp0Kbdv32bChAn069ePVatWcfbsWebOnYu9vT2TJk0qN97Tp0+TlZVFnz59uHPnDtevX6d///60bWt8bSSNRoOiKFhUZ1qZEKLW6S6eq8ZFf/SdFKXMndzNSXl2qj6hRmun8guLKpHf5EIIIURlFOWZineUOnWr5LVlP+G0CJmLxfINpTcbMNfaFDpNuaeVIaPqKBAh6o5Op0OXm2Oejyr+rGs0GhYtWkTHjh3x8fFh9erV+jqKT/n09fUFICQkBLVaja+vLzExMQQGBgLQp08f1Go1qamppaZ8zp49m+DgYCIjI+nevTudO3dm4cKF5OffH6Gbm5vL8uXL6dGjB15eXowYMYKEhAQAcnJyCAgIICwsTF/+0qVLeHt7GyS8jDl+/DhWVlb4+fnh6Oio/7C2tsbGxgZHR0ecnJxwdnbG2dlZf75Zs2aoVCqDa27dukV4eDjBwcFERETQt29f3Nzc8PPzY82aNcyZMweAzz//nPz8fNauXcsjjzzCyJEjCQkJYdOmTRV+PeLi4hgwYADHjh3D27twN72xY8eiVqtJSEggJiaGRx99lPj4eAYMGIC7uztpaWnltl+R77//nlGjRuHp6UnPnj154403uHv3boUxCSFqIOtW1a+pyQ6hdUApvtGCtox+nk+PytZW43gaEhmhJoQQQlRGsaePVVbBlM969TSzoumpjs6Vq0cWqxUPkrxctC+PrfJluSa4tcX7O6CJTaXL79y5k/Hjx7N7925OnTpFWFgYarWaiRMnGpSLjY3lscceIyIigoCAAFQqFba2tri4uDB+/Hi+/vprXFxcyhzVlpCQgKOjIzt37uTixYvMmDGDzp076++zePFizp07x4YNG3BycmLv3r1MmjSJ/fv34+Hhwfr163n66acJDAxk0KBBvPLKK/j7+zN+/Hj9PdRqtX5aZpH4+HgGDRpkkt+Lu3fvJi8vr8ypmw4ODgCcOHECX19frK3vP9jo378/H3zwAZmZmTRv3pyEhATGjBnDt99+i5vb/bUn9+3bx9SpU+nZsyeHDh3C39+fqKgoevbsSfPmzUlNTeXevXt88MEHvPvuu7Ro0YLWrVtX2H6XLl1i4sSJhIWFsXbtWm7cuMHixYtZtGgRf/vb32rcNkKIMrR2gutXCj8vaxmM3BwAlKLf3TVZZ7c4Y5sImFqrMkaoWVVi8ylRSv1MoQohhBD1ja4mmxJUvIZaKQ/ZV/2aalC8Oxse0JQ/Qk0eTAphXi4uLixbtgwvLy9Gjx5NcHCw0Y0IihJlDg4OODo60qpVK5o2bUqLFi305x0dHfVTJktycHBgxYoVeHl5MXjwYAIDAzl8+DBQOG00JiaGjRs34uvrS/v27Zk+fTq9evUiJiYGAB8fH8LCwpg/fz5LliwhLS2N1atXG9zD09MTe3vD33VxcXEMGTKkZo30h4sXL2JnZ2cwtdWYa9eu0bp1a4Njbdq00Z8DaNq0KZ6enlha3h+PkJ6ezpkzZwgICMDa2lpfR/PmzfWj6gDy8/NZuXIlvXr1wsvLi5s3b1bYfu+//z6jRo1iypQpeHh40KtXL95880127dpFTk6OSdpHCFFIl3nj/gvb8tc+1Gk0aF8ei/blseiK+kw12SG0uDoY4aaoH77/eZ+BxU5U8d7y4BSQEWpCCCFE1VRrO/WqdzosQuai/eQ9SD5T9ftVpv63o+DXFOjex/DE5f8ZLa+MnFD4r0PLcpbolWybeEBZNykcKVZFVlZWBtMgq3vvqnj88ccNRm/16NGDjRs3oqkoGV5F3t7eBsk2Jycnzpwp/H105swZNBoN/fr1M7gmLy9Pn7ADmDZtGnv37uXjjz8mOjqali1bGpQ/dOiQwevz589z5coVnnjiCZO8B51OZ7IRwN27dy8Vb3x8PL169dKPdCuLtbU1nTp10r+uTPudPn2aM2fO8MUXX+jP63Q6tFotqampdOgga1YKYQq6X5LQrll4/0CxpLnRpNHv2fc/v3en8AGofoRaDRNiacb7YCbRsjXcvG744NTJBdy94eI5LJ6o5E7P0tUzIAk1IYQQojJq0lkq1iFTxk+p1CWKkwuqv76DZsozVb9fZepv7WR8YdqCPOMX2Pyxc+BjvVCGjkZpX8GGDPLgUjxAFEWp0rRL/XVWVoZr0zQgVkam/xSt1Xbnzh1UKhV79uwpNcKt+OYG169f58KFC6hUKi5evEhAQEC594yPj8ff3x8bm6p/LYzx8PAgOzubK1eulDtKrU2bNly/ft3gWNHItKKRasbs27evUqPpbGxsDBJ7lWm/O3fuMGnSJIKDg0vVp1arK7ynEKJyDJJpgEX/YWgv/FL4wlhCrfixop/rmsxiqGvFZ03odFi89jbcuIrSVn6vVIdM+RRCCCEq48ZVAJTqjFAr6rw8ZI9F4NMmDMr0dGWtoZaeChQmHiyCJqP0NM0IEiFE1SQmJhq8PnnyJO7u7kanblpZWZl85BoUTufUaDTcuHEDd3d3gw9HR0d9uXnz5tGxY0fWrVvHihUrOH/+fLn1mnK6J8BTTz2FtbU1GzZsMHo+KysLKBzl99133xmMNjx06BCenp40b97c6LV37twhISGBoUOHVjmuyrRfly5dOHfuXKnz7u7uBmu9CSFMrIIpn4ZPDP9IoGlNNOWzLuTdf3CqPDG48MFQtZJp8uQUJKEmhBBCVEhXUFDs82pM7zLVYrW1pfhItbIW4D33c8X11Nf3J0QDkpaWxtKlS0lOTubLL7/ko48+IiQkxGhZV1dXDh8+zNWrV8nMzDRZDJ6enowePZpZs2YRGxvLr7/+SmJiIuvXr2f//v0AfPLJJ5w4cYJ169YxevRohg0bxssvv0xesT/m/P392bNnD1A4mu3UqVMGu43WlFqtZsmSJWzZsoV58+Zx9OhRLl++zLFjxwgLC2PdunUAjBo1CisrK+bNm8cvv/zCv/71L7Zs2cLUqVP1dSUmJuLv7096ejoABw4cwMPDw2CDgsqqTPvNnDmT48ePs2jRIpKSkrhw4QJxcXEsWrSo5g0jhChbsZkIOmNJIyP5tPsbV9Xj9MrNwlG4ukNx94/ZNiujcDmkr2egHn/FhRBCiHoir9hefsXXzqgsUy1WW0ssXlup/1xRlbEahGVVd3+SJ5dC1IagoCBycnIYMWIEixYtIiQkhEmTJhktGx4ezqFDh+jVq1e1RlKVJyIigqCgIJYvX46/vz8hISH8+OOPqNVqkpOTefPNN1m5cqV+euLKlSu5efMm7777rr6OlJQUsrMLf6fu27ePbt26lVpnraYmT57Mtm3byMjIIDQ0lP79+zN//nzs7OyYPn06APb29mzbto3U1FSefPJJli9fzpw5cwza9d69e6SkpFDwxwOWuLi4GiX/yms/gE6dOvHPf/6TCxcuMHr0aIYOHcq7775b4QYLQogqampr+FpVrK9mbF1ZnbERavX8wWkxuhNH7r+ozwnAB4SsoSaEEEJUxKJYBykjrerX1/e1NZrZFXthPBGm+DxeN7EIIcq0a9cu/eerVq0qdf67774zeD1kyJBSUyh9fHxISzP8PTZv3jzmzZunf100cqu45cuXG7y2srJi/vz5zJ8/32isKSkpBq8dHBw4duyYwbHicVQ03bP4ezem5Hsozt/fH39//3Kv79Spk8EGACX17dtXH29BQQH/+c9/iI6ONijj4OBQqm3HjRvHuHHjStVXUfsBdOvWjf/7v/8rN24hRA15+8CP399/XXxdzCuV7PMV9fPq6YPTMtUkXnluCsgINSGEEKJixTsND9nf/7yyCTJTjVCr4m6AlVZ87SUj6zABYGSB8tLqacJQCFHv9e7dm5EjR5o7jErJzMxkypQpdOvWzdyhCCFqSCm5QVNZ/SA9XenPrxZOBSc700RR1Q2lwvdq9CqTx/Egk4SaEEIIUZHiOyK1aFV+UZ0ObfQGtPFf3j9oou3UK7tDaJXrLT6ds/n996eMGF+8VK3cWwghoHDNsAdl98rWrVsze/Zsg507hRAPqDbOBi9L7tys/XoHuqKEGRgdmaX98O3aiKx2FG26UK1kmihJEmpCCCFEOXS3s9DOmlDGSSO9qpQz6P7fXnQ7P7p/zERra+i+2l6j68tVtHZasfdkMM2zVZuq1VfG5gZCCCGEEPWGrsROyCVmE+i+jEa7fFaxA8UesupAl5tLTSj9h9Xo+irTxy8PBExB1lATQgghyqGd+5zBa8XJBV3qxbIvyLlX+ph+96cadl7u/l6z6ytDW6yjWFCAxStvoPvlJ5Q+AbV/byGEEEKIOqQ7+5PhAWN9tdwc4xcX5INWY/xcZTkVG5nbtXfN6qoM/UPeGtYjD04BGaEmhBBCVE1b1/ufG+tMGOtfmGg7dWXIqBpdX37lf/yrKdYxbN4S5bFeWIwJLjUFotw6hBBCCCEeBMU3JKiMYn0/XcxmTNn5UTp1M1ldZdLWcISa9PUMSEJNCCGEqDaj2bPSh7Qm2v3pdmbNri9PQQEAuou/6A8pTi41qFCeXAohhBCi4dId+69pR3rV8MFrle5X01kTd26jk1FqklATQgghqq2y/QididZQu3GtRtdX6h5ffFbr9xBCCCGEEJRIqNXB/dTtanavu3cA0K4KQ/d/G00T0wNMEmpCCCFEdZ1LKn3M6DTQP47VdIRaXewop6nBWiCy450QQgghGrLimxKU5OxWnQrvf9q0WTWur6TWToX//i/5jwM177PpDsTWuI4HnSTUhBBCiGrSfX/IyEFjBcvpfFXFg5SwklkAQgghhGhoSvZv8vPvf56eWo36iu2u3vOJ6sVUGa7tDV8/SH3KekwSakIIIUR5WjkavKx4cf7SmSTd8SOFn6T9z0RB1aJHupg7AiFEGYKCgggPDy/zvK+vL1FRURXWEx0dTc+ePXF1dSUqKoq1a9cyePBgU4baKMXExPDoo4/W2/oAEhISUKvVZGVlmbReIRqNEg9JtWEv1rC+Ygk1VSU2gKq2kgk0SaiZgqW5AxBCCCHqM6VTN3T/jb9/wM7B4Lx21ycoAcNRihJvRqZ8Glxf3xXF36K1eeMQQlRZbGwstra2+tdqtZotW7YwbNgw/bHbt2+zePFilixZwvDhw7G3t0er1fLiizX8o9CE5syZw6FDh8jIyKhRPREREYwbN44jR44QGRnJyZMnycnJwc3NjYCAAKZOnYqzszMAp0+fZtGiRfz444+0bNmS4OBgZs6cWan7+Pn5sXr16hrFKoR4QJh6If66Wti/ZP5Mk2+0mKgaSagJIYQQ5SnZ0SkxRF4X9zm6H75D9daHRUcMz2dcrtn9vTvDuZ9rVkdVFO1IWq0Hl/K0UzyYdDoduZqq/1GjQUt+Qc2mdDdRKSgmmnrTqlWrCsukpaWRn59PYGAgTk5O+uPNmtXi2j1VoNFo2L9/P5s2bcLT01N/PDQ0lI4dOzJ//nwsLS3Jzc0FQPXHiI7IyEgOHjzI9u3b9dfY2dnx2WefsXDhQsaMGUNUVBRubm6kpaWxa9cuNm7cyNKlS7l9+zYTJkygX79+rFq1irNnzzJ37lzs7e2ZNGlSufGePn2arKws+vTpw+eff14LLSKEqFe0DWRNiz92dxc1Iwk1IYQQohy6s6cqLnQlzfi1Oh3culGj+yut26L7I6Gm+PSoUV2Vop/KUMM/8GUrdfEAydXoGBdzziz3jhnnjY1l5X/eNBoNixYt4p///CeWlpY8//zzvPbaayiKgq+vL6GhoUyZMgVfX18AQkJCAHB1dWXu3LnMnTsXgD59+gDw7bffsmPHDvbu3cu+ffsAmD17NtnZ2fTu3ZuNGzeSl5fHyJEjWbZsGVZWVgDk5ubyzjvv8K9//YusrCw6duzIwoUL6du3Lzk5OTz55JP06tVLP3Lr0qVLDBkyhOXLlzN+/Pgy39/x48exsrLCz8/PINFobW2NjY0Njo6OWFlZkZ9vOLqiWbNmqFQqHB3vT9P/7bffCA8PJzg4mGXLlumPu7m54efnp5/2+Pnnn5Ofn8/atWuxtrbmkUce4eeff2bTpk0VJtTi4uIYMGCAvl1K+vTTT9m4cSO//fYbbm5uzJo1i6CgIP35rKwsVqxYQVxcHLdv36Z9+/a8/vrrRqfg3rhxg0mTJuHi4sKGDRuwsrLigw8+4B//+AfXrl3D3d2d2bNnM2LECP0133zzDUuWLCE9PZ3u3bszZsyYct+PEI2ZxYqN8Ht2BaUe0BFqolbIGmpCCCFEea5fMXxd0d+9xfpFJpnqWXxtDZfq7B5VRfoRajLaTIj6aOfOnahUKnbv3s3y5cvZtGkT27ZtK1UuNrZw97WIiAgSExOJjY3lmWee0Y/g+vrrr0lMTMTFxcXofRISErh06RI7d+5k3bp17Nixgx07dujPL168mBMnTrBhwwb279/PiBEjmDRpEhcuXMDGxob169ezc+dO4uLi0Gg0vPLKK/j7+xsk09RqNTExMQb3jY+PZ9CgQSYZtbd7927y8vLKnLrp4FA4hf/EiRP4+vpibW2tP9e/f39SUlLIzMzUt4darSY11XDR8X379jF06FCj9e/Zs4clS5YwdepUvvnmGyZNmsTcuXM5cqRwXU2tVsukSZM4fvw469ev58CBA7z++uv6UXfFpaWlMWrUKB555BE2bdpEkyZNWL9+Pbt27WLVqlX85z//YcqUKbz66qscPXpUf82UKVMYPHgwcXFxTJgwgbfffrtqjShEY9G1N4qjc8XlTLXRVBFNHY0Uk7xdrZARakIIIURVVNQhKZYA00VvQJmzvIb3K3ZDpZ4/B5McnHhANVEpxIzzrvJ1VpZW5BfUbB2aJqqq/eC4uLiwbNkyFEXBy8uLs2fPEhUVxcSJEw3KFU3/dHBwMBi11aJFC/354sdLcnBwYMWKFahUKry8vAgMDOTw4cNMnDiRtLQ0YmJi+P7772nbti0A06dP58CBA8TExPD666/j4+NDWFgY8+fPZ+TIkaSlpfHpp58a3MPT0xN7e3uDY3FxcSxdurRKbVKWixcvYmdnZzC11Zhr167h5mb4wKJNmzb6c82bN6dp06Z4enpiaXn/z6f09HTOnDlDQECA0XojIyMZO3YskydPBgrf78mTJ4mMjORPf/oT//3vf/nhhx84ePCgfnpru3btStWTnJzMs88+y5NPPqn/2ufm5rJ+/Xq2b99Oz5499dceO3aM6Oho+vTpw9atW2nXrh1LliwB0H+/fPDBB5VoPSEaCWc3SE/FYvCfC19XlMw39ZTPZnamra9MtZNR0+XcRbGxrbhgAyUJNSGEEKK2mGQYf/GEmgmqq4itadZR0s56FotZS1F8HjdJfULUJkVRqjTtsoiVlQWqOp7w8fjjjxuM3urRowcbN25Eo9GY9D7e3t4GI6WcnJw4c+YMAGfOnEGj0dCvXz+Da/Ly8vQJO4Bp06axd+9ePv74Y6Kjo2nZsqVB+UOHDhm8Pn/+PFeuXOGJJ54wyXvQ6XQmW5+ue/fupeKNj4+nV69e+pFuJSUnJ5dKdPbq1YstW7YA8PPPP+Ps7GywVlxJOTk5jB49mj//+c8sX37/Ac2lS5e4d+8ezz77rEH5/Px8fHx89Pfv3r27wfkePepg6QAhHiS6EiPzm9hUUN60iSnFfyj8egGlay+T1ltKbU0t/f02SEJNCCGEEJVS1b/Niv0xp/TqV07BMhh0gOogo5ad+cetan4v7XtLUUX9u8b1CCHqnrE1wXR//D66c+cOKpWKPXv2lJqeWHxzg+vXr3PhwgVUKhUXL14scyRXkfj4ePz9/bGxqeAP2kry8PAgOzubK1eulDtKrU2bNly/ft3g2LVr1/TnyrJv3z6GDBlS7fgq8z6tra3p168f33zzDTNmzNDvSnrnzh0Atm7dqh8lWPwaIUQlFS11YVHY71HUpUeJGjJxQs3KGuXFWSatU9Sdej53RAghhHjAFZ8CmnOvGtcX+7ydV83jKUvRuiGpFwv/lTXUhKiXEhMTDV6fPHkSd3d3o+tuWVlZmXzkGoCPjw8ajYYbN27g7u5u8FF8Gum8efPo2LEj69atY8WKFZw/f77ceuPi4mqUoCrpqaeewtramg0bNhg9X7QpQY8ePfjuu+8MNjo4dOgQnp6eNG/e3Oi1d+7cISEhocz106BwiuXx48cNjh07dowOHToA8Oijj5Kenk5KSkqZdVhYWPD3v/+dLl26MGbMGDIyMoDCEYRNmjQhLS2t1NdArVbr7//DDz8Y1Hfy5Mky7yVEo1TUT6vsshrlTfm0NL45Sb1QWyPUGvmmCjJCTQghhKhF2s+KrVXz0/GyC5ap2KYEZewiZxJ375igEknCPch27NjBrl27DI65uLiwbt06oHA639atW0lISCA/P5+uXbsSGhpqkHC4fv06UVFR/Pzzz9jY2NC/f38mTJhgkOz5+eef2bp1K6mpqbRq1Yq//OUvDBgwwOC+e/fu5auvviIzM5N27doRHByMl1ctJpQfIGlpaSxdupRJkyaRlJTERx99RHh4uNGyrq6uHD58mF69emFtbV1mcqiqPD09GT16NLNmzSI8PBwfHx9u3LjB4cOHefTRRxk0aBCffPIJJ06cYN++fajVar755htefvllvvrqK/0IKn9/f15//XWefPJJrl+/zqlTp/jkk09MEiMUbnqwZMkSFi9ezO+//05QUBBubm6kp6ezc+dOmjVrxpIlSxg1ahR/+9vfmDdvHi+99BJnz55ly5YtBmu5JSYmMmvWLGJiYnB2dubAgQN4eHiUWnutuBkzZjB9+nQ6d+5Mv3792LdvH3v27NFvDNGnTx98fX2ZOnUqS5YsoX379iQnJ6MoisFoPpVKxfvvv8/MmTMZO3Ysu3btwtHRkWnTprF06VK0Wi29e/fm9u3bHDt2jIceeoixY8fy/PPPs2nTJt58802effZZfvrpJ4ONJYQQFBuhVnFCTXfnd7Q7tpRd4BEfEwUlHhQyQk0IIYQwqRJP6q5l1LA6Mz35kxFqjZKbmxubNm3SfxRfs+nTTz/lxIkTzJ07l2XLlnHr1i3Wrl2rP6/Vann77bcpKCjgrbfe4qWXXuLgwYMGuzhevXqVVatW0blzZ1avXs1TTz1FZGSkwSiahIQEtm7dSlBQEO+88w7t2rVjxYoV+tFEjV1QUBA5OTmMGDGCRYsWERISwqRJk4yWDQ8P59ChQ/Tq1avckVTVERERQVBQEMuXL8ff35+QkBB+/PFH1Go1ycnJvPnmm6xcuVI/WmrlypXcvHmTd999V19HSkoK2dnZQOH0yW7dupVaZ62mJk+ezLZt28jIyCA0NJT+/fszf/587OzsmD59OgD29vZs27aN1NRUnnzySZYvX86cOXMM2vXevXukpKRQUFC4I19cXByDBw8u997Dhg1j2bJlbNy4kYEDBxIdHU1ERAR9+/bVl4mKiqJr167MnDmTgIAAVqxYYXRUoaWlJRs2bOCRRx5h7NixXL9+nbCwMGbPns3777/PgAEDmDhxIt988w0PP/wwUJhQ3LRpE3v37mXIkCF89tlnLFiwoMZtKkSDoh+hVnG/R7c9CpJPl12gPo9Qy80xdwQNkqLTNfIxehSukVB8iLepKIqCs7Mz6enpSDPXDWlz85B2Nw9p97qhmfKMwWuL514yHHX2h6K1wnQnE9B+uKrM+qq6pph207vojv23WtdWhWbOJPg9+/4BRxdUKyKrVIeuoADtjNEGx0wVc336freysip3XaUH1Y4dOzh27JhBwqPI3bt3CQkJYdasWfj5+QGFI6XmzJnDW2+9hbe3N4mJiaxatYqNGzfqR0LFx8fzj3/8gy1btmBpaUl0dDSJiYkGibh169Zx584dFi1aBMDChQvx9PQkJCQEKEzUzZgxgyeffJI///nPZcafn59v0J9TFIWmTZty7do1fRKkuKysrFI7TFaVlZVVrfQhG6PJkyfTu3dvZs6cWWFZc7d7QUEBXbt2JTo6utSi/w1Fdna2frMFRVFo27YtGRkZZv/925hIu9eNgtCnAVC98TeUdl4oikJ+yIhS5Sw3f0XBkpch7X9l1qV090P10qJai7Umit5ncZabv6pxPaq3o1DatC2jdOXVp+93S0vLSvfzZMqnEEIIUUlK0IsVFzJ1H+BBGqEmo9oeeBkZGUybNg0rKyu8vb2ZMGECrVu35sKFC2g0Grp06aIvq1arad26NefOncPb25tz587x8MMPG0wr7NatG5s3byY1NRV3d3fOnz9vUAdA165d9dP8CgoKuHDhgkHizMLCgi5dunDu3LlyY//iiy8Mpqy6u7vzzjvvlNkpvnfvntGF96vKFHUI8PPzY/To0ZVuT3O2e2ZmJtOnT6dXr14m20W0vrG2ttZvgFCk5OYHom5Iu9eerH9spOhRYis7O5r88T2faqSss7MzGdbWlJfKt7G1pXWJn5v6oqz3VNN6HNu0wdKE7/lB+36XhJoQQghRSRZDR6E7FFd+oaLt102lzhJq8vS7sevQoQMzZ87ExcWFW7dusWvXLsLDw1m7di2ZmZlYWloa7OAI4ODgQGZmJlCYZCi5RlfRCJfiZYqOFS9z79498vLy+P3339FqtaXqad68Ob/99lu58Y8aNYoRI+6PKihKdJQ1Qi0vL6/Go5zMPVKqISmaflmZ9jR3uzdv3pxXXnnF6PdVQ5GXl0d6ejpQv0aONCbS7rWvYFuU/vMb166ipKeXmSRPT0+noKD8TV5ycnP1PzcPAlPEejX9NxRKb4pTVfXp+11GqAkhhBAmZjF/RaXKmboToCuooz8aNSUSgQ1z0IUoR/Gpa+3atdMn2I4ePapfRL4+s7KyKnPUkrk750I8iEr+3Oh0OvlZMgNp97qhUyzKfYip0+kqHomvWDxQXytTxKq7dQPaupogmj/qe8C+32VTAiGEEKI8tg8V/uvQovDfijpT2nJGqFViB6mSFO/OhZ/U9kK3ufdK3rnqdUgSrkFp1qwZLi4uZGRk0Lx5cwoKCrhzx3A32KysLP1osubNm+tHohU/X3Su6N+SmwtkZWXRtGlTrK2tsbe3x8LColQ9xka/CSGEENWhy89D+81uw4OVmb5diYRao9PIlz1ohF9xIYQQogr0uz/98V9mTRZe7dq7ypcoA59GeXEWFm9VbYOAKrMsMWi9ga4LJCovJydHn0zz8PBApVLx008/6c//9ttvXL9+HW9vbwC8vb359ddfDRJmp06domnTpri6Fj697tChg0EdRWWK6rC0tMTDw4OkpCT9ea1WS1JSkr6MEEIIURO6r7aj276p6hdW1DeqxoPTB532nQXoLl80dxhm0/i+4kIIIURVFK2J9kcnSun4WLnFFRvbss9ZVX3anGJpiUXfQJRWtbyrpK2d4et0Y8vXioZs69atnD59mqtXr/LLL7/w7rvvYmFhwRNPPIGtrS0DBw5k69atJCUlceHCBTZs2IC3t7c+0dW1a1dcXV15//33uXTpEj/88APbt29n6NCh+qmYQ4YM4erVq0RHR5OWlkZcXBxHjx7lqaee0scxYsQIvvnmGw4ePMjly5fZvHkzubm5DBgwwBzNIoQQooHRnUuquJAxFY5Qa5wPI3UHYs0dgtnIGmpCCCFEefQj1P5IqFXYWSpn3QfrJqaJqTY8QOtViNpx8+ZN3nvvPW7fvo29vT0dO3ZkxYoV2NvbA/DCCy+gKApr166loKCArl27Ehoaqr/ewsKCBQsWsHnzZhYvXkyTJk3o378/48aN05dxdHRkwYIFfPrpp8TGxtKqVSumT59Ot27d9GX69u1LdnY2O3bsIDMzk/bt27Nw4UKZ8imEEMK8KugDKo/3qaNA6pvGmUiEKibUduzYYbAdOYCLiwvr1q0DCneD2bp1KwkJCeTn5+s7WsU7QNevXycqKoqff/4ZGxsb+vfvz4QJE1Cp7u8M8fPPP7N161ZSU1Np1aoVf/nLX0o9ldy7dy9fffUVmZmZtGvXjuDgYLy8vKr27oUQQoiKFI1Qq8Qwfl3mTbT//LTsAvbNTRNTbTDJNIXG26FqCGbPnl3ueWtra0JDQw2SaCW1adOG119/vdx6OnfuzOrVq8stM2zYMIYNG1ZuGSGEEKJasm5V77qKHqpWY2mPBqERd/+qPELNzc2NN954Q//aolgH/NNPP+XkyZPMnTsXW1tbtmzZwtq1a3nzzTeBwjUw3n77bZo3b85bb73FrVu3eP/991GpVEyYMAGAq1evsmrVKgYPHswrr7xCUlISkZGRNG/eXP/0MiEhga1btzJlyhQ6dOjA119/zYoVK1i3bl2prdiFEEKIGtEajlArt+im1eVPlazPUwHu/l4r1ep0ukqM6hNCCCGEqCPXrxg5eH+kvsVD9mh/zy5dJOVsudU22v5OY33fVCOhZmFhYXTI/d27d/nPf/7DrFmz8PHxAWDmzJnMmTOHc+fO4e3tzY8//sjly5d54403aN68Oe3bt2fcuHH84x//YOzYsVhaWhIfH4+joyPPP/88AK6urpw9e5avv/5an1DbvXs3gYGBBAQEADBlyhROnjzJgQMH+POf/1xm7Pn5+eTn5+tfK4pC06ZN9Z+bmlLp6UHCVKTNzUPa3Tyk3etIQeH/G4qFCkVRymxvRVHg/Ony67KwqL9fr9ycUoeqHKuR8kp16jFatXy/CxEUFESnTp1Yvny50fO+vr6EhoYyZcqUcuuJjo5m3bp1ZGRksGTJErKzs9m7dy/79u2rjbBFLUlISGDMmDGcPn1aBhUIUVPFd+i0qEZfw7Oj6WJ54DTevlmVE2oZGRlMmzYNKysrvL29mTBhAq1bt+bChQtoNBq6dOmiL6tWq2ndurU+oXbu3Dkefvhhg4Rct27d2Lx5M6mpqbi7u3P+/HmDOqBwkdtPPvkEgIKCAi5cuGCQOLOwsKBLly6cO3eu3Ni/+OILgymr7u7uvPPOO7RpU7sLPbdtW4Md4US1SJubh7S7eUi7156sf2yk6PmkU9u2qFq2BsDYGDRnZ2ejx4uzs3fAwdnZlCGaTFnvqSp0Oh2XS9bRti1KsWUdakq+34UoW2xsLLa29zdGUavVbNmyxWD67O3bt1m8eDFLlixh+PDh2Nvbo9VqefHFF80RslFz5szh0KFDZGRk1KieiIgIxo0bx5EjR4iMjOTkyZPk5OTg5uZGQEAAU6dO1f+eO336NIsWLeLHH3+kZcuWBAcHM3PmzErdx8/Pj9WrV5Oens7SpUs5c+ZMleKMiYmp1nVCCBNy71DsRdUTRBZT5psulgdN482nVS2h1qFDB2bOnImLiwu3bt1i165dhIeHs3btWjIzM7G0tKRZs2YG1zg4OJCZmQlAZmZmqdFtRU9Tipcp+YTFwcGBe/fukZeXx++//45Wqy1VT/Pmzfntt9/KjX/UqFGMGDFC/7roKfe1a9coKCioTBNUiaIotG3bloyMDHSy2HOdkDY3D2l385B2r30F26L0n1+5ehUlN7/MEVLp6emFI7TK+Vr8fucOd9PTTR5nbUmvYqzGvg/Tf/sNxbLmeyDVp+93S0vLWn8YJ+qWTqdDo6n6dYqio6CgZt+PKpXpRl62atWqwjJpaWnk5+cTGBiIk5OT/njJPry5aDQa9u/fz6ZNm/D09NQfDw0NpWPHjsyfPx9LS0tyc3MB9OswR0ZGcvDgQbZv366/xs7Ojs8++4yFCxcyZswYoqKicHNzIy0tjV27drFx40aWLl3K7du3mTBhAv369WPVqlWcPXuWuXPnYm9vz6RJk8qN9/Tp02RlZdGnTx8+//zzWmgRIURdUCzuP/zTZmdWvYI/drNunBpvRq1KPdzu3bvrP2/Xrp0+wXb06FGsra1NHpypWVlZ6bdtL6k2O+c6nc7snf/GRtrcPKTdzUPavW7oKkiW6XS6ChNqOguLB+prVdVYjZXX5d4D1UOmCkm+30Wt0Ghgzz+zzHLvJ//iQFVyzhqNhkWLFvHPf/4TS0tLnn/+eV577TUURTGY8unr6wtASEgIULiMyty5c5k7dy4AffoU7kb37bffsmPHDoMpn7NnzyY7O5vevXuzceNG8vLyGDlyJMuWLdP3pXNzc3nnnXf417/+RVZWFh07dmThwoX07duXnJwcnnzySXr16qXfgOLSpUsMGTKE5cuXM378+DLf3/Hjx7GyssLPz88g0WhtbY2NjQ2Ojo5YWVkZLOMChQlBlUqFo6Oj/thvv/1GeHg4wcHBLFu2TH/czc0NPz8/srIKv+aff/45+fn5rF27Fmtrax555BF+/vlnNm3aVGFCLS4ujgEDBnDs2DF926rVagDmzp3LvHnzyMzMJDw8nP3795Obm0ufPn1Yvnw5Hh4eJCQklHndrl272LJlCykpKdja2vKnP/2JZcuW0bp163JjEkKYgYXpRuOLB0eNtvRq1qwZLi4uZGRk0Lx5cwoKCrhz545BmaysLP1osubNm+tHohU/X3Su6N+iY8XLNG3aFGtra+zt7bGwsChVj7HRb0IIIUR16fLzShypxNM3pYL/Vhvj+l8m2T1UCFFk586dqFQqdu/ezfLly9m0aRPbtm0rVS42NhYonPaYmJhIbGwszzzzjH4E19dff01iYiIuLi5G75OQkMClS5fYuXMn69atY8eOHezYsUN/fvHixZw4cYINGzawf/9+RowYwaRJk7hw4QI2NjasX7+enTt3EhcXh0aj4ZVXXsHf398gmaZWq4mJiTG4b3x8PIMGDTLJqL3du3eTl5dX5tTNolkxJ06cwNfX12CAQP/+/UlJSdH/zZGQkIBarSY11XCC/L59+xg6dCg9e/Zk2bJl2NnZkZiYSGJiItOnTwcKp7CeOnWKjz/+mH//+9/odDqee+458vPzy72uoKCA1157jX379rFlyxZSU1OZM2dOjdtFCFELGnNCrRH39Wo0ByMnJ4eMjAz69euHh4cHKpWKn376CT8/P6DwqdD169fx9vYGwNvbm88//5ysrCz9f2CnTp2iadOmuLq6AoXTShMTEw3uc+rUKX0dlpaWeHh4kJSURO/ehdvSarVakpKSZHt1IYQQJqFL/BbthpWGByuzQG1FfwBWlHB7wBn7A1j3f1HwwisojbizJeo/lapwpFhVGRspVZ17V4WLiwvLli1DURS8vLw4e/YsUVFRTJw40aBc0fRPBwcHg1FbLVq00J8vfrwkBwcHVqxYgUqlwsvLi8DAQA4fPszEiRNJS0sjJiaG77//Xr+u4fTp0zlw4AAxMTG8/vrr+Pj4EBYWxvz58xk5ciRpaWl8+umnBvfw9PTE3t7e4FhcXBxLly6tWqOU4eLFi9jZ2RlMbTXm2rVruLm5GRwrmtZ97do1mjdvTtOmTfH09MSy2HDC9PR0zpw5Q0BAANbW1tjZ2aEoikG7Xrhwgfj4eL788kt69eoFwPr16+nVqxd79+7l6aefNnodYJB8bNeuHW+++SbDhw/nzp079WaKrhAPEt2V39AlfFO9aysaHa+q+fIW4sFTpa/61q1b6dmzJ61bt+bWrVvs2LEDCwsLnnjiCWxtbRk4cCBbt27loYcewtbWlo8++ghvb299Mqxr1664urry/vvvM3HiRDIzM9m+fTtDhw7VDx8fMmQIcXFxREdHExAQQFJSEkePHmXBggX6OEaMGMEHH3yAh4cHXl5exMbGkpuby4ABA0zXMkIIIRot7ea1pQ9WZrRERWUaYVJJl/ANymM9ocefzB2KEGVSFKVK0y6LWFoq6HR1O/L08ccfN0he9+jRg40bN6KpziJw5fD29tavTwbg5OSkXzT/zJkzaDQa+vXrZ3BNXl6ePmEHMG3aNPbu3cvHH39MdHQ0LVu2NCh/6NAhg9fnz5/nypUrPPHEEyZ5DzqdzmTr03Xv3r1UvPHx8fTq1avcHTaTk5OxtLTk8ccf1x9r2bIlnp6eJCcnl3vPU6dOsXbtWv06bVqtFihcB6/o7yshROVpF0+v/sUVJtTqeR+vTVu4VrONXkRpVeo63Lx5k/fee4/bt29jb29Px44dWbFihf7J0gsvvICiKKxdu5aCggK6du1KaGio/noLCwsWLFjA5s2bWbx4MU2aNKF///6MGzdOX8bR0ZEFCxbw6aefEhsbS6tWrZg+fTrdunXTl+nbty/Z2dns2LGDzMxM2rdvz8KFC2XKpxBCCNMw9gdYZUaXVfR3WxObaoXzoNNlZzXi5WqFeDAZW3e4aITGnTt3UKlU7NmzxyDpBoabG1y/fp0LFy6gUqm4ePEiAQEB5d4zPj4ef39/bGxM87vSw8OD7Oxsrly5Uu4otTZt2nD9+nWDY9euXdOfK8u+ffsYMmSISWIt6e7du0yYMIEBAwbw/vvv06pVK9LS0pgwYQJ5eSWXJBBC1Lo/Etplqu8PTWtz/dnGuKTJH6qUUJs9e3a5562trQkNDTVIopXUpk0bXn/99XLr6dy5s34B07IMGzZMpngKIYSoJcYSatW8rvjZPuX/MdlgNd5+lhAmV3JplJMnT+Lu7l4qsQWFSTFTj1wD8PHxQaPRcOPGDf3mB8bMmzePjh078uyzz/Laa6/Rr18/OnToUGb5uLi4UlNXa+Kpp57i7bffZsOGDQabEhQpWoamR48erF69mvz8fH0i8dChQ3h6epb5wP7OnTskJCTw9ttv649ZW1uXam8vLy8KCgo4efKkfsrnzZs3SUlJ0beFseuSk5O5desWr7/+un6zgh9//LF6DSGEqDHdv/9R7nmlvq+hVpsJtUac5K/naVQhhBDCDHLvGTlYiaxQeX+4PtIFxar+74gthKjf0tLSWLp0KcnJyXz55Zd89NFH+p08S3J1deXw4cNcvXq11IZeNeHp6cno0aOZNWsWsbGx/PrrryQmJrJ+/Xr2798PwCeffMKJEydYt24do0ePZtiwYbz88ssGo6v8/f3Zs2cPUDia7dSpUwwePNhkcarVapYsWcKWLVuYN28eR48e5fLlyxw7doywsDDWrVsHwKhRo7CysmLevHn88ssv/Otf/2LLli1MnTpVX1diYiL+/v6kp6cDcODAATw8PAzWXnN1deXOnTv897//5ebNm9y7dw8PDw+GDh1KWFgY33//PT///DOvvvoqbdu2ZejQoWVep1arsba25uOPP+Z///sf8fHx+niFEHVPt+ef5g6h/iqo2VqiDzJJqAkhhBCVUokne5qCss/ZNDVdKA8cGaImhKkEBQWRk5PDiBEjWLRoESEhIUyaNMlo2fDwcA4dOkSvXr30yRtTiYiIICgoiOXLl+Pv709ISAg//vgjarWa5ORk3nzzTVauXKkfXbVy5Upu3rzJu+++q68jJSWF7OxsoHD6ZLdu3Uqts1ZTkydPZtu2bWRkZBAaGkr//v2ZP38+dnZ2+t007e3t2bZtG6mpqTz55JMsX76cOXPmGLTrvXv3SElJoaCg8Pd8XFxcqeRfr169eO6555gxYwZdunRhw4YN+rbq0qULL7zwAs888ww6nY7PPvtMPxrO2HWtWrXib3/7G7t37yYgIID333+fN954w6RtI0Rjocu+hS77lrnDMK/aHKFWnUVIGwhFV+F2FQ3ftWvXarxDkzGKouDs7Ex6enrFu4IIk5A2Nw9pd/OQdq89minPlDpm8f5OlCZNUBSFgtCnS51XRf3b6HX668PfQ3FzN2mcpmQsdlXUv01SjzJpJhb9a7ZMQ336freysip3XSVRv5TVz8vOzi61w2RVmWKXT1Fo8uTJ9O7dm5kzZ1ZY1tztXrRWdHR0NN27dzdbHLWp+M9Hffr925hIu5uGriAf7Yy/VFiuqM9TVj+vstfXV5q/BsNNw7UiqxOz9otodLE7DI4p/kOxeO6lGsVXn77fq9LPkxFqQgghRCUoTZrUrAL1w6YJ5EEkA9SEEBXo3bs3I0eONHcYlZKZmcmUKVMMNk0TQtRT9+6aO4L6QWuiJJWRPp3uUJxp6n4ASUJNCCGEqAuV2SVUCCEaqZkzZ+qnh9Z3rVu3Zvbs2SiNeGc7IR4c8nMKgIe3uSNokBrvZFchhBCiDjXqP7wkmSiEEEIIYTYWz72EztEF3f+S4UwNdgyWPp0BaQ0hhBBCCCGEEEI0PJV4nqmMn1L7cZiZ8pA9Fn95AaWtaw0rMk08DYWMUBNCCCH+oMu+Belp5g6j4WnMo/OEEEIIYT6V6IMonRrm5iLG1XQtNSPt2albDet8cElCTQghhPiDdt4L5g5BCCGEEEKYTCUe6tk2q/0w6ota2EBTcW+867PJlE8hhBBC1C4ZoSaEEEKIekpxaGHw2tLN3UyR1H9K117mDqFekYSaEEIIYQK6y5fMHUI9Jgk1IYQQQphBNbogqhIJtoalZkPUlHZeWCz/wJRVPtAkoSaEEEKYgHbZq+YOof6SfJoQJhEUFER4eHiZ5319fYmKiqqwnujoaHr27ImrqytRUVGsXbuWwYMHmzJUUccSEhJQq9VkZWWZOxQhHnia7Exzh1CvKc5uJY403oyarKEmhBBCCCFEAxAbG4utra3+tVqtZsuWLQwbNkx/7Pbt2yxevJglS5YwfPhw7O3t0Wq1vPjii+YI2ag5c+Zw6NAhMjIyalRPREQE48aN48iRI0RGRnLy5ElycnJwc3MjICCAqVOn4uzsDMDp06dZtGgRP/74Iy1btiQ4OJiZM2dW6j5+fn6sXr0af3//GsVbWUFBQXTq1Inly5fXyf2EeKBptOWfd3QpdUh3904tBVMP6Bpv8qs2SEJNCCFEo6bT6dBtfR/atDV3KA1DOy/4X7LhMUUGxAtRF1q1alVhmbS0NPLz8wkMDMTJyUl/vFmz+rEot0ajYf/+/WzatAlPT0/98dDQUDp27Mj8+fOxtLQkNzcXAJVKBUBkZCQHDx5k+/bt+mvs7Oz47LPPWLhwIWPGjCEqKgo3NzfS0tLYtWsXGzduZOnSpdy+fZsJEybQr18/Vq1axdmzZ5k7dy729vZMmjSp3HhPnz5NVlYWffr0qYXWMJSXl4e1tXWt30eIhkR3aG+55xXPR4wcLL9OixUb0S6adv/1a29XJzSzULr0Qvf/9kKTpjWrqHN3+Dmx8PNGnKSTHq4QQojGLeUsusP70H3xmbkjaRiatyx97Oa1uo9DiCrQ6XTk5+eb5UNXxT9ENBoNixYtomPHjvj4+LB69Wp9HcWnfPr6+gIQEhKCWq3G19eXmJgYAgMDAejTpw9qtZrU1NRSUz5nz55NcHAwkZGRdO/enc6dO7Nw4ULy8/P1ZXJzc1m+fDk9evTAy8uLESNGkJCQAEBOTg4BAQGEhYXpy1+6dAlvb2+DhJcxx48fx8rKCj8/PxwdHfUf1tbW2NjY4OjoiJOTE87Ozjg7O+vPN2vWDJVKZXDNrVu3CA8PJzg4mIiICPr27Yubmxt+fn6sWbOGOXPmAPD555+Tn5/P2rVreeSRRxg5ciQhISFs2rSpwq9HXFwcAwYMwNLSki5durB79279ucGDB9O9e3f96++//x53d3fu3bsHFCY3X3zxRTp06MAjjzzCtGnTuHbt/u/Loq/Ltm3b8PPzw8PDg9mzZ3P06FG2bNmCWq3Wfw2LnDp1iieffBJPT0+eeeYZkpNLPOAQorG5VsFIVwsjKZGKHgSW2GxJ8e5cxaDM6LGeWLz2NhZvV/z7rTzK433vv9BoahjUg0tGqAkhhGjc8nLMHUGDp/syGp4aa+4whChTQUEBH374oVnuPWPGDKysrCpdfufOnYwfP57du3dz6tQpwsLCUKvVTJw40aBcbGwsjz32GBEREQQEBKBSqbC1tcXFxYXx48fz9ddf4+LiUuaotoSEBBwdHdm5cycXL15kxowZdO7cWX+fxYsXc+7cOTZs2ICTkxN79+5l0qRJ7N+/Hw8PD9avX8/TTz9NYGAggwYN4pVXXsHf35/x48fr76FWq/XTMovEx8czaNAgFBPsDrx7927y8vLKnLrp4OAAwIkTJ/D19TUY/dW/f38++OADMjMzad68OQkJCYwZM4Zvv/0WN7f76wft27ePqVOnoigKfn5+HD16lBEjRpCZmUlycjI2NjYkJyfj5eXF0aNH6dq1K02bNtVPs23WrBn//Oc/KSgoYNGiRcyYMYNdu3bp67906RKxsbFs3rwZCwsLXF1duXDhgn60HhSOTCxKqr3zzjuEh4fTqlUrFixYwLx58/jXv/5V47YUosHKyyt1yNLRGc3V9LKvsVDVYkC1S1EUMHECUBf/BYypP8sG1CVJqAkhhGjkZMV8AGXUc6YZpWeCP4KFEGVzcXFh2bJlKIqCl5cXZ8+eJSoqqlRCrShR5uDggKOjo/54ixYt9OeLHy/JwcGBFStWoFKp8PLyIjAwkMOHDzNx4kTS0tKIiYnh+++/p23bwuny06dP58CBA8TExPD666/j4+NDWFgY8+fPZ+TIkaSlpfHpp58a3MPT0xN7e3uDY3FxcSxdurTa7VPcxYsXsbOzM5jaasy1a9cMkmQAbdq00Z9r3rw5TZs2xdPTE0vL+38+paenc+bMGQICAoDCUX/R0dEAfPfdd3Tu3BlHR0cSEhL0CTU/Pz8ADh8+zNmzZzl69ChqtRqA9957j4CAAH744Qe6desGQH5+Pu+9955B4rP4aL2S/vrXv+qnn7700ks8//zz5OTkYGNjU+l2E6JBycst97Tu2H9h6msGxyzsHMqvU/o64g+SUBNCCNG4VaZT5OZe+3GYmcXwMej8AtBGrYHk0zWoSTqZ4sFjaWnJjBkzqnydlZWVwTTI6t67Kh5//HGD0Vs9evRg48aNaEw85cbb21u/PhmAk5MTZ86cAeDMmTNoNBr69etncE1eXp4+YQcwbdo09u7dy8cff0x0dDQtWxpOCT906JDB6/Pnz3PlyhWeeOIJk7wHnU5nkpFuAN27dy8Vb3x8PL169dKPdPPz8yM8PJwbN25w9OhR+vbtS5s2bTh69CjPPvssx48f14+WO3/+PC4uLvpkGhS2uYODA+fPn9cn1NRqdaXWxivSqVMn/edFicQbN24Y3EeIRsXYlM4Kr6loETVZOUtp36ER7+15nyTUhBBCiApYzHvL3CHUCaVlaxNUUvMqhKhriqJUadplkepc86Aw9t6K1mq7c+cOKpWKPXv2GCTdwHBzg+vXr3PhwgVUKhUXL17Uj+QqS3x8PP7+/iYbTeXh4UF2djZXrlwpd5RamzZtuH79usGxorXMikaqGbNv3z6GDBmif/3oo4/SvHlzjh49yrfffstf//pX2rRpw4YNG/jhhx8oKCigZ8+eVXoPxXdtrQxjCVqttoJdDoVoyAoKqnFRRQk16ewoD3uYO4R6QVKrQgghRAWUZnbmDqEO1fB5o5HRIMrIiUYKCiGqIzEx0eD1yZMncXd3L5XYgsKkmKlHrgH4+Pig0Wi4ceMG7u7uBh/FpyHOmzePjh07sm7dOlasWMH58+fLrTcuLs4gQVVTTz31FNbW1mzYsMHo+aysLKBwlN93331nMNrw0KFDeHp60rx5c6PX3rlzh4SEBIYOHao/pigKvr6+xMXFce7cOXr37k2nTp3Iy8sjOjqaxx57TJ8g69ChA7/99htpaWn668+dO0dWVhbe3t7lvi8rKytJkglRSbqfT5ZfoKmRpHVFI9CUB3cNNVNSAp8u/Ne3v5kjMR9JqAkhhGjcZB0MEzPSns0eqvswhGig0tLSWLp0KcnJyXz55Zd89NFHhISEGC3r6urK4cOHuXr1KpmZmSaLwdPTk9GjRzNr1ixiY2P59ddfSUxMZP369ezfvx+ATz75hBMnTrBu3TpGjx7NsGHDePnll8krtgC4v78/e/bsAQpHs506dcpgt9GaUqvVLFmyhC1btjBv3jyOHj3K5cuXOXbsGGFhYaxbtw6AUaNGYWVlxbx58/jll1/417/+xZYtW5g6daq+rsTERPz9/UlPL1yo/MCBA3h4eJRae61Pnz7861//olOnTjRr1gwLCwt8fX354osv9GubAfTr14+OHTvyyiuv8NNPP5GYmMisWbPo06cPXbt2Lfd9ubm5kZiYSGpqKjdv3pTkmhDlyS+96YDiP+z+54NGlj5fUd/QQkEZOKLw05cW1Sy+B9md2wDovvt/Zg7EfCShJoQQQoj7dDUboWYx5M8mr1MIcV9QUBA5OTmMGDGCRYsWERISwqRJk4yWDQ8P59ChQ/Tq1ctgJJUpREREEBQUxPLly/H39yckJIQff/wRtVpNcnIyb775JitXrtSv3bVy5Upu3rzJu+++q68jJSWF7OxsoHD6ZLdu3Uqts1ZTkydPZtu2bWRkZBAaGkr//v2ZP38+dnZ2TJ8+HQB7e3u2bdtGamoqTz75JMuXL2fOnDkG7Xrv3j1SUlIo+GP6WFxcnNHkn5+fHxqNhr59++qP9enTB41GY5BQUxSFjz/+GAcHB0aPHs348eN5+OGHK7Xb7LRp07CwsGDAgAF06dLFYJSbEKIEVelp0BbPFdv5t4WRNQorHKFmgTJ+ChbrtqF0861hgA8u3bcHzR2C2Sk6nfRyr127VuMFZY1RFAVnZ2fS09ORZq4b0ubmIe1uHtLupqH75Se0a8p/uqiK+rf+c0VRKAh9usr3KV5HfaZZFQYpZ4Hqx6y78zva2RP0r5Vnp2Lxx5Pc6qpP3+9WVlblrqsk6pey+nnZ2dmldpisKlNsSiAKTZ48md69e+sX7S+Pudu9oKCArl27Eh0dTffu3c0WR20q/vNRn37/NibS7qaheW8pJBlO+1RF/Rvttwfg7CmUSS+hFFt7UFEUrP+xgbsH9pRZp8X67Sg2VVvfsCHSTHlG/3lN+7n16fu9Kv082ZRACCGEECallJziKX8HCCEq0Lt3b0aOLD31qj7KzMxkypQp+p04hRD1mfHpmxZ+AeBXxkYpFY1Qk36N+IMk1IQQQjRuFez+pDxhuvV8Hgi18lRQep5CiPJVZmRafdG6dWtmz55t7jCEEJVSnT5IBWuoyfK74g+yhpoQQohGTbt7e/kFnF2rXKfF+hjD139dVeU6zEXp9Mf0Jesm5g1ECCGEEMIMFIvyM2Yy3VMUkRFqQgghGrfkM+WfV6rx7KnENYpXp6rXYSbK8DHQqg3Ko91qVo//MHSH9ha+kLVfhBBCCGEO1emDVKfvJxol+U4RQgghylPR1unGVLT2Rj2mWFlh8cRglFY1XHRfpdJ/qjuXVMOohBBCCCGqoVozPsvv+5l70fz6SJeba+4QzOLB7fELIYQQdeHCL1W/pjpJuIYs8VtzRyCEEEKIxqhpU5NXqUg/rxTty2PMHYJZSEJNCCGEKIfu/OmqX1TB2huNgr2DuSMQQgghRCOndO9TnatMHodomCShJoQQonGraPF9K6uq1ylrb6AM/rO5QxBCCCFEYyejyeqM7lqGuUOoc9LjF0II0bjlVbDmg2XVE2oyFQCUJjZg37zw8x5/Mm8wQgghhGicZL2zunPrurkjqHOSUBNCCCHKoQweWbMKuvQ0TSAPIKXovds2M28gQjQQQUFBhIeHl3ne19eXqKioCuuJjo6mZ8+euLq6EhUVxdq1axk8eLApQ220jh07RmBgIO3atSM4ONjc4QghqkOei1aL7sY1c4dQ5yzNHYAQQghR31j8fTvaV8cDoDi0qF4ds5aiO5mAMi7UlKE9UHRH9hf++994eP5lM0cjRMMXGxuLra2t/rVarWbLli0MGzZMf+z27dssXryYJUuWMHz4cOzt7dFqtbz44ovmCNmoOXPmcOjQITIyajZ9KCIignHjxnHkyBEiIyM5efIkOTk5uLm5ERAQwNSpU3F2diYnJ4cFCxbw008/cf78eQYNGsRHH31U6fsEBQUxevRoJkyYwLJly+jUqROfffYZzZrJwwQhzK4aI9Qs7GQd2Gr5PdvcEdQ5SagJIYQQxdk5oDS9/wcpbV2rVY3i8ziKz+MmCkoIISrWqlWrCsukpaWRn59PYGAgTk5O+uP1Jfmj0WjYv38/mzZtwtPTU388NDSUjh07Mn/+fCwtLcnNLZyur1KpAIiMjOTgwYNs375df42dnR2fffYZCxcuZMyYMURFReHm5kZaWhq7du1i48aNLF26FK1Wi42NDcHBwcTGxlYp3lu3bnH8+HE+/PBDAC5dusRzzz2Hi4uL0fI6nQ6NRoOlpfwZJkTdqHpCzX7sZH4/mwS9+6Hb8rdaiKlhUtp3MHcIdU6mfAohhBBGWCx9H4v5K1Acnc0dihCitul0oM2rxkduNa8r9lHF0RMajYZFixbRsWNHfHx8WL16Nbo/6ig+5dPX1xeAkJAQ1Go1vr6+xMTEEBgYCECfPn1Qq9WkpqaWmvI5e/ZsgoODiYyMpHv37nTu3JmFCxeSn5+vL5Obm8vy5cvp0aMHXl5ejBgxgoSEBABycnIICAggLCxMX/7SpUt4e3sbJLyMOX78OFZWVvj5+eHo6Kj/sLa2xsbGBkdHR5ycnHB2dsbZ2Vl/vlmzZqhUKoNrbt26RXh4OMHBwURERNC3b1/c3Nzw8/NjzZo1zJkzBwBbW1tWrVrFxIkTadOmTZW+Ht988w0+Pj7k5OSgVqu5desWc+fORa1WExMTQ0JCAmq1mv/85z8MGzYMd3d3vv/+e7RaLevXr8fPzw9PT08GDRrE7t27Deo+e/YskyZNokOHDnTt2pVXXnmFmzdvVik+IRq9aiyhZmH7EKpZS7DwCzB9PA2ZReNLL8mjESGEEI2bohj+QfvH54r6YTMFJISoc7p8HC8sMcutr3osA8W60uV37tzJ+PHj2b17N6dOnSIsLAy1Ws3EiRMNysXGxvLYY48RERFBQEAAKpUKW1tbXFxcGD9+PF9//TUuLi5ljmpLSEjA0dGRnTt3cvHiRWbMmEHnzp3191m8eDHnzp1jw4YNODk5sXfvXiZNmsT+/fvx8PBg/fr1PP300wQGBjJo0CBeeeUV/P39GT9+vP4earVaPy2zSHx8PIMGDTLJ5i67d+8mLy+PmTNnGj3v4FD5aV2pqan4+fmxc+dO+vbtaxDv0KFDcXFxITExEX9/f+bPn88zzzyDnZ0diYmJAKxcuZLw8HAefvhhHBwcWL9+PZ9//jmrVq3C3d2db7/9lldffZVWrVrRp08fsrKyGDt2LM8++yxLly4lJyeHFStWMG3aNHbu3FmzhhGiMZFNCUQtkoSaEEIIUVxl/oizsACttvZjEUKIElxcXFi2bBmKouDl5cXZs2eJiooqlVArSpQ5ODjg6OioP96iRQv9+eLHS3JwcGDFihWoVCq8vLwIDAzk8OHDTJw4kbS0NGJiYvj+++9p27YtANOnT+fAgQPExMTw+uuv4+PjQ1hYGPPnz2fkyJGkpaXx6aefGtzD09MTe3t7g2NxcXEsXbq02u1T3MWLF7GzszOY2lpdlpaWeHp60rRpU/2x3NxcDh48yLx58/Sj4xRFwc7OrlTbvvbaa/j7++uvW79+Pdu3b6dnz8LNW9q1a8exY8eIjo6mT58+fPzxx/j4+PD666/r61i7di29evUiJSXFYDqsEKI8klATtUcSakIIIUSVyfZPQjQoilXhSLEqsrKyJD+/oMb3rorHH3/cYPRWjx492LhxIxqNpmZxlODt7a1fnwzAycmJM2fOAHDmzBk0Gg39+vUzuCYvL0+fsAOYNm0ae/fu5eOPPyY6OpqWLVsalD906JDB6/Pnz3PlyhWeeOIJk7wHnU5nkpFuAM7OzqXiPXLkCK1bt+aRRx6p8PrHHntM//mlS5e4d+8ezz77rEGZ/Px8fHx8ADh9+jQJCQl06FB6TaL//e9/klATorIkn1Z3TPT79kEiCTUhhBCNnEKVe1sWCsgANSEaDkWp0rRLPQurBrtmjJVV6URf0Vptd+7cQaVSsWfPHoOkGxhubnD9+nUuXLiASqXi4sWLBASUvx5RfHw8/v7+2NjYmOAdgIeHB9nZ2Vy5csUko9RKio+PN1h7rjzFd1+9c+cOAFu3btWP8CtibV34fXj37l0GDx7MwoULS9VVG+9FCCFqzPYhc0dQ5xpmD0AIIYSotBLJtMqstaHIf59VpZM1TIQwiaI1uYqcPHkSd3f3UoktKEyKmXrkGoCPjw8ajYYbN27g7u5u8FF8quO8efPo2LEj69atY8WKFZw/f77ceuPi4hgyZIjJ4nzqqaewtrZmw4YNRs9nZWVVu26dTse+ffsYOnRola/19vamSZMmpKWllWo/tVoNFLbxL7/8gpubW6kyxZNzQoiKSP+j7jS+tpa/CIQQQjRuRkZhVMRUU4gaE+3br5k7BCEahLS0NJYuXUpycjJffvklH330ESEhIUbLurq6cvjwYa5evUpmZqbJYvD09GT06NHMmjWL2NhYfv31VxITE1m/fj379+8H4JNPPuHEiROsW7eO0aNHM2zYMF5++WXy8vL09fj7+7Nnzx6gcDTbqVOnKj3iqzLUajVLlixhy5YtzJs3j6NHj3L58mWOHTtGWFgY69at05c9d+4cSUlJZGZmkp2dTVJSEklJSfrz6enp+Pv76xOap06dIicnh969e1c5roceeohp06axdOlSduzYwaVLl/jpp5/46KOP2LFjBwCTJ08mMzOTmTNn8sMPP3Dp0iUOHjzInDlzaiVJKkSDJQ/06k4jbGqZ8imEEKJRU4aORvfV9mIHKpEsk4Ra1V08Z+4IhGgQgoKCyMnJYcSIEahUKkJCQpg0aZLRsuHh4Sxbtoxt27bRtm1bvvvuO5PFERERwXvvvcfy5cvJyMigZcuWPP744wwaNIjk5GTefPNN1q5dqx9xtXLlSgYNGsS7777LokWLAEhJSSE7OxuAffv20a1bt1LrrNXU5MmT8fDwYOPGjYSGhpKTk4OrqyuDBg1i6tSp+nLPPfccly9f1r8uGnmWlpYGQEFBASkpKdy7dw8oHE03cOBALC2r9+dUWFgYrVq14v333+fXX3/F3t6eLl268MorrwDQtm1bvvzyS1auXMmECRPIzc3F1dWVAQMGYNFApxkLUSskoVaHGl9bKzqZg8G1a9fIz883eb2KouDs7Ex6erpMdakj0ubmIe1uHtLupqHd/290MZvvH7BzQBXxWZnlFUVB8/JYdDn3yiyjivq3KUN8YGmmPGPw2uLDz1Gq+cdnffp+t7Kyok2bNmaNQVReWf287OzsUjtMVpWVlVWt9CEbo8mTJ9O7d29mzpxZYdn60O6DBg3i1Vdf5Zlnnqm48AOo+M9Hffr925hIu5uG9r/x6La+b3CsvH5ayXbX7v8Xur2fQ9atSl3fmJTq5y3/AMXZrVp11afv96r08+TxhhBCiEau6muoWbq2r51QGrq8XHNHIISop3r37s3IkSPNHUal5OXlMXz4cAYOHGjuUIQQFclIq9HlFoNGYvH25ooLikY5GlCmfAohhGjcqvF/vyLTbaqp8XW0hBCVU5mRafWFtbU1c+fONXcYQohK0MV/UfNKZKmPymmE3Tz5i0AIIUTjVvJpmqbAPHE0BhqtuSMQQgghRCOmDBhejYskoWaM0jewxJHGl1GThJoQQohGrsR//vfuVnjFQ8+ML/zk0a61EE8DplKZOwIhhBBCNFZ2DigjxlX9OkmoGaV0LbHLsUz5FEIIIRqZavzf3yzgSbLsW6Fr0xbtjNGmj6mhkqmyQgghhDATizWfVm/ZDkmoGWdRol00GvPEYUbSsxVCCCGqQXFxq/aOlUIIIYQQom5Vdw1cRRJqximGMw9033xlpkDMRxJqQgghGrnGNzzdfKSthRBCCCEahBIJSt3ZU2YKxHwkoSaEEKJxa4TrPZiNNLUQQgghRMNQcsRfI+xTS0JNCCFE49b4/u83n0bY0RJCCCGEaJBKToXNvGGeOMxIEmpCCCEaOUnyCCEeDEFBQYSHh5d53tfXl6ioqArriY6OpmfPnri6uhIVFcXatWsZPHiwKUNtlBISElCr1WRlZdXL+gBSU1NRq9UkJSWZrE4hRCMlu7fLLp9CCCEaORk1VYekrYWoTbGxsdja2upfq9VqtmzZwrBhw/THbt++zeLFi1myZAnDhw/H3t4erVbLiy++aI6QjZozZw6HDh0iIyOjRvVEREQwbtw4jhw5QmRkJCdPniQnJwc3NzcCAgKYOnUqzs7O5OTksGDBAn766SfOnz/PoEGD+Oijjyp9n6CgIEaPHk379u1rFK8QQjxQFBmfJQk1IYQQjZsk1OqONLUQtapVq1YVlklLSyM/P5/AwECcnJz0x5s1a1aboVWaRqNh//79bNq0CU9PT/3x0NBQOnbsyPz587G0tCQ3NxcA1R8jJCIjIzl48CDbt2/XX2NnZ8dnn33GwoULGTNmDFFRUbi5uZGWlsauXbvYuHEjS5cuRavVYmNjQ3BwMLGxsVWK99atWxw/fpwPP/yQ8+fPm6AFhBDiAWEhu59KSlEIIYQQdUQyaqJ+0ul0FGhzqv6hqcY1JT50VUzqazQaFi1aRMeOHfHx8WH16tX6OopP+fT19QUgJCQEtVqNr68vMTExBAYGAtCnTx/UajWpqamlpnzOnj2b4OBgIiMj6d69O507d2bhwoXk5+fry+Tm5rJ8+XJ69OiBl5cXI0aMICEhAYCcnBwCAgIICwvTl7906RLe3t4GCS9jjh8/jpWVFX5+fjg6Ouo/rK2tsbGxwdHREScnJ5ydnXF2dtafb9asGSqVyuCaW7duER4eTnBwMBEREfTt2xc3Nzf8/PxYs2YNc+bMAcDW1pZVq1YxceJE2rRpU6WvxzfffIOPj0+Z13399dcEBATg7u6Or68vkZGRBudzc3NZsWIFPXv2xN3dnT/96U/83//9n9G67t27x6RJkxg5cqR+Gui2bdvo378/Hh4e+Pv788knnxhck5iYyJAhQ/Dw8ODJJ5+UqZ5CVJfqj7FItvXj4UO9ICPUZISaEEKIRq6GI9SUoaPRxX1uomCEEOag0eXyzzNTzHLvvzwahaViU+nyO3fuZPz48ezevZtTp04RFhaGWq1m4sSJBuViY2N57LHHiIiIICAgAJVKha2tLS4uLowfP56vv/4aFxeXMke1JSQk4OjoyM6dO7l48SIzZsygc+fO+vssXryYc+fOsWHDBpycnNi7dy+TJk1i//79eHh4sH79ep5++mkCAwMZNGgQr7zyCv7+/owfP15/D7VarZ+WWSQ+Pp5BgwahlFzsuhp2795NXl4eM2fONHrewcGh0nWlpqbi5+fHzp076du3r0G8Q4cONXrNqVOnmD59OnPnzuWZZ57h+PHjLFy4kBYtWujf86xZszhx4gRvvvkmnTp14tdff+XmzZul6srKyuL555+nWbNmbN++naZNm/L555+zZs0a3nrrLXx8fEhKSuK1117D1taWsWPHcufOHV544QX8/f1Zv349v/76K0uWLKn0exZC3Gex4B20X0ZjETTZ3KHUHxayhpok1IQQQjRut2q2I5FF0GR0vv3RLp9looAaMJleK0SNubi4sGzZMhRFwcvLi7NnzxIVFVUqoVaUKHNwcMDR0VF/vEWLFvrzxY+X5ODgwIoVK1CpVHh5eREYGMjhw4eZOHEiaWlpxMTE8P3339O2bVsApk+fzoEDB4iJieH111/Hx8eHsLAw5s+fz8iRI0lLS+PTTz81uIenpyf29vYGx+Li4li6dGm126e4ixcvYmdnZzC1tbosLS3x9PSkadOm+mO5ubkcPHiQefPmGb1m06ZNPPHEE/qRcJ6enpw/f57IyEjGjRtHSkoKX331Ff/3f/+Hv78/AO3atStVz7Vr15g+fTru7u588MEHWFtbA7B27VrCw8MZPnw4AA8//DDnzp0jOjqasWPH8sUXX6DValmzZg02NjY88sgjpKen8/rrr9e4PYRobJT2HVDNXmbuMOoXCxmhJgk1IYQQjZru0F5zh9B4SD5N1FMqpQl/ebTi3TFLsrK0Ir8gv+KCFdy7Kh5//HGD0Vs9evRg48aNaDSaGsVRkre3t359MgAnJyfOnDkDwJkzZ9BoNPTr18/gmry8PH3CDmDatGns3buXjz/+mOjoaFq2bGlQ/tChQwavz58/z5UrV3jiiSdM8h50Op1JRroBODs7l4r3yJEjtG7dmkceecToNefPny81eq1Xr15s3rwZjUbDzz//jEqlok+fPuXee/z48XTv3p3IyEj91+Tu3btcunSJefPm8dprr+nLajQa7Ozs9Pd/9NFHsbG5PwKyR48elX/TQghRHhP9fn2QSUJNCCGEKEYZF2LuEBowyaiJ+klRlCpNuyxiqbJCp22YU16srKxKHStaq+3OnTuoVCr27NljkHQDw80Nrl+/zoULF1CpVFy8eJGAgIBy7xkfH4+/v79BAqgmPDw8yM7O5sqVKyYZpVZSfHy8wdpzVVXZ9xkYGEhsbCznzp3j0UcfBQq/BgDvvvsu3bt3Nyhf8msihBC1QkaoyaYEQgghRHFKl17mDqHBUJ78i7lDEKLBSUxMNHh98uRJ3N3djSZRrKysTD5yDcDHxweNRsONGzdwd3c3+Cg+jXTevHl07NiRdevWsWLFigp3wYyLi2PIkCEmi/Opp57C2tqaDRs2GD1ftLB/deh0Ovbt21fm+mkAHTp04NixYwbHjh07hoeHByqVikcffRStVsvRo0fLvVfRLqXjxo3j3LlzALRp04a2bdvyv//9r9TX4OGHH9bf/8yZM+Tk5OjrOnnyZHXfshBCGFLJ+CxJqAkhhBB/sFjydxQnl6pf2NTW9ME0BNYlprLptOaJQ4gGJC0tjaVLl5KcnMyXX37JRx99REiI8ZG1rq6uHD58mKtXr5KZmWmyGDw9PRk9ejSzZs0iNjaWX3/9lcTERNavX8/+/fsB+OSTTzhx4gTr1q1j9OjRDBs2jJdffpm8vDx9Pf7+/uzZswcoHM126tSpGo34KkmtVrNkyRK2bNnCvHnzOHr0KJcvX+bYsWOEhYWxbt06fdlz586RlJREZmYm2dnZJCUlGeyImZ6ejr+/vz6heerUKXJycujdu3eZ9582bRqHDx/mb3/7GykpKezYsYOPP/6YadOmAeDm5saYMWOYN28ee/fu5ddffyUhIYF///vfpeoKDw9n1KhRjB07luTkZKAwYfn++++zZcsWUlJSOHPmDDExMWzcuBGAUaNGoSgKr732GufOneObb74ptcuoEA3ewx4AKMPHmjmQBqit2twRmJ2kFIUQQog/KK7tq3dda9NPJWoQSm6nnp0F9i2MlxVCVEpQUBA5OTmMGDEClUpFSEgIkyZNMlo2PDycZcuWsW3bNtq2bct3331nsjgiIiJ47733WL58ORkZGbRs2ZLHH3+cQYMGkZyczJtvvsnatWtRqwv/4Fq5ciWDBg3i3XffZdGiRQCkpKSQnZ0NwL59++jWrVupddZqavLkyXh4eLBx40ZCQ0PJycnB1dWVQYMGMXXqVH255557jsuXL+tfF408S0tLA6CgoICUlBTu3bsHFI6mGzhwIJaWZf851aVLFyIjI1mzZg3vvfcejo6OvPbaawa7mr799tusWrWKhQsXcuvWLVxcXHj11VeN1rds2TK0Wi1jx45l586dTJgwgaZNm/Lhhx/y1ltvYWtrS8eOHQkNDQUKp99+8sknLFiwgKFDh9KhQwcWLVrElCnm2dFWCLMo6nc4Ops3jgbIVGtUPsgUnU623Lp27Rr5+TVbUNYYRVFwdnYmPT0daea6IW1uHtLu5iHtbhqaKc/oP1dFlR4VUFJZ7V7VehoD7Z5d6D7fqn+tPDEYixdeqVZd9en73crKijZt2pg1BlF5ZfXzsrOzS+0wWVVWVla10odsjCZPnkzv3r2ZOXNmhWXrQ7sPGjSIV199lWeeeabiwg+g4j8f9en3b2Mi7W4amveWQdIJlBdnYdE3sMLy0u5VU7z/C2Dx/g6UJlVfB7M+tXtV+nky5VMIIYQQtaPkk0vpmAohytC7d29Gjhxp7jAqJS8vj+HDhzNw4EBzhyKEEOZl+5DBS+3LjWtqrSTUhBBCCFE7Su7+JAk1IUQZZs6cqZ8eWt9ZW1szd+5cHnrooYoLCyHMTPoetcnijb+ZOwSzkoSaEEIIIWpHiTXUdAnfmCkQIYQQQghhao19HWFJqAkhhBCidpQcoSaEEEIIIRo03bUMc4dQZ6SnK4QQQojaIQk1IYQQQohGRftR45kGKj1dIYQQwpQcWpo7gvpDtlMXQgghRL0gfZI6c+uGuSOoM5JQE0IIIUzJtZ25I6g/ZISaEEIIIUrQaTR1eDPZlKDO3blt7gjqjPR0hRBCCBOwWLgGpW8gFi/ONnco9Yci3QwhhBBC3Kfd/y+0L49Bl3LW3KGI2tKIcpjS0xVCCCFMQHH3xuLFWSgOLcwdSv1hoTJ3BEIIIYSoR3QxW6CgAO0nfzd3KKLWNJ6MmiTUhBBCCFE7ZA01IUwqKCiI8PDwMs/7+voSFRVVYT3R0dH07NkTV1dXoqKiWLt2LYMHDzZlqKIOpKamolarSUpKMncoQlRdxmVzRyBqS26OuSOoM5bmDkAIIYQQDZSFJNSEqEuxsbHY2trqX6vVarZs2cKwYcP0x27fvs3ixYtZsmQJw4cPx97eHq1Wy4svvmiOkI2aM2cOhw4dIiMjo0b1REREMG7cOI4cOUJkZCQnT54kJycHNzc3AgICmDp1Ks7OzuTk5LBgwQJ++uknzp8/z6BBg/joo48qfZ+goCBGjx5N+/btGTNmDKdPn8bBwaHS1yckJFTrOiEedLrsWyj2MrJfVEx3LQPdF5+hDBuN8rCnucPRk4SaEEIIIWqHrKEmRJ1q1apVhWXS0tLIz88nMDAQJycn/fFmzZrVZmiVptFo2L9/P5s2bcLT8/4fTaGhoXTs2JH58+djaWlJbm4uACpV4dTyyMhIDh48yPbt2/XX2NnZ8dlnn7Fw4ULGjBlDVFQUbm5upKWlsWvXLjZu3MjSpUvRarXY2NgQHBxMbGxsleK9desWx48f58MPP+T8+fMmaAEhGg/tvBdQRf27dm9SNPtQRs0/0LQfrIC0/6E79t/a/56pAunpCiGEEIDylxfMHUKDo3Tqbu4QhKgcnQ5Fo63yBxpNta4zqKOKO9BpNBoWLVpEx44d8fHxYfXq1ej+qKP4lE9fX18AQkJCUKvV+Pr6EhMTQ2BgIAB9+vRBrVaTmppaasrn7NmzCQ4OJjIyku7du9O5c2cWLlxIfn6+vkxubi7Lly+nR48eeHl5MWLECBISEgDIyckhICCAsLAwfflLly7h7e1tkPAy5vjx41hZWeHn54ejo6P+w9raGhsbGxwdHXFycsLZ2RlnZ2f9+WbNmqFSqQyuuXXrFuHh4QQHBxMREUHfvn1xc3PDz8+PNWvWMGfOHABsbW1ZtWoVEydOpE2bNlX6enzzzTf4+PiQk5PDmDFjAOjUqRNqtZrZs2fr2+qNN97gsccew8PDgz//+c/88MMPQOG0zbKuO3DgAH/+85959NFH6dy5M88//zyXLl2qUnxCCNEg1NMpwjJCTQghROP2sCf8moLi2t7ckTQ4ip29uUMQolIUrQ7nn342y73Tu3RGp6r8yImdO3cyfvx4du/ezalTpwgLC0OtVjNx4kSDcrGxsTz22GNEREQQEBCASqXC1tYWFxcXxo8fz9dff42Li0uZo9oSEhJwdHRk586dXLx4kRkzZtC5c2f9fRYvXsy5c+fYsGEDTk5O7N27l0mTJrF//348PDxYv349Tz/9NIGBgQwaNIhXXnkFf39/xo8fr7+HWq3WT8ssEh8fz6BBg1BMMJpk9+7d5OXlMXPmTKPnqzK9MjU1FT8/P3bu3Enfvn0N4h06dCguLi5ERUUxZcoUDh06hJ2dHTY2NgCsWLGC2NhY1q1bh6urKxs2bGDixIkcPny43Ovu3r3L1KlTefTRR7lz5w5r1qwhNDSU+Ph4LCxkXIQQQpibJNSEEEI0co1nJyIhxIPPxcWFZcuWoSgKXl5enD17lqioqFIJtaJEmYODA46OjvrjLVq00J8vfrwkBwcHVqxYgUqlwsvLi8DAQA4fPszEiRNJS0sjJiaG77//nrZt2wIwffp0Dhw4QExMDK+//jo+Pj6EhYUxf/58Ro4cSVpaGp9++qnBPTw9PbG3N0y8x8XFsXTp0mq3T3EXL17Ezs7OYGprdVlaWuLp6UnTpk31x3Jzczl48CDz5s1DpVLRvHlzAFq3bq1P1t29e5etW7fyt7/9jYEDBwLw7rvv4ufnx/bt25kxY4bR6wCeeuopgxgiIiLo0qUL586do2PHjjV+T0LUB7rcXJQmTcwdhqjHdNcyQKMxdxhGSUJNCCGEEHVGe/QAFn0CzB2GEAZ0FgrpXTpX+TpLK0sK8gtqfO+qePzxxw1Gb/Xo0YONGzeiMfEfG97e3vr1yQCcnJw4c+YMAGfOnEGj0dCvXz+Da/Ly8vQJO4Bp06axd+9ePv74Y6Kjo2nZsqVB+UOHDhm8Pn/+PFeuXOGJJ54wyXvQ6XQmGekG4OzsXCreI0eO0Lp1ax555JEyr7t06RL5+fn06tVLf8zKyopu3bpVuObahQsXWLNmDYmJidy8eROtVgsUroMnCTXRYGgKANMn1HTXMtB+tA6ST5u8blG3tOuWmjuEMklCTQghhBC1Rgl6Ed2uj/WvdR/9DSShJuobRanStEs9lQrdH0mOhsbKyqrUsaK12u7cuYNKpWLPnj0GSTcw3Nzg+vXrXLhwAZVKxcWLFwkIKP9nPz4+Hn9/f/2Ux5ry8PAgOzubK1eumGSUWknx8fEGa8+Z2uTJk3F1dWX16tW0bdsWrVbLwIEDDdayE+KBV0ubBWg/+8AwmSZ7EpiFTvvHeqNG/k+ptKu/mS4gE5PJ90IIIYSoNRZDR5k7BCEalMTERIPXJ0+exN3dvVRiCwqTYqYeuQbg4+ODRqPhxo0buLu7G3wUn0Y6b948OnbsyLp161ixYkWFI7Li4uIYMmSIyeJ86qmnsLa2ZsOGDUbPZ2VlVbtunU7Hvn37GDp0qP5YURKyeJu3b98ea2trjh07pj+Wn5/PDz/8gLe3d5nX3bx5k5SUFGbNmkW/fv3o0KFDjeIVot6q4sYslXbndu3UK6pEu3oB2tnPosu5a+5QaoUk1IQQQghAHl0KIR4EaWlpLF26lOTkZL788ks++ugjQkJCjJZ1dXXl8OHDXL16lczMTJPF4OnpyejRo5k1axaxsbH8+uuvJCYmsn79evbv3w/AJ598wokTJ1i3bh2jR49m2LBhvPzyy+Tl5enr8ff3Z8+ePUDhaLZTp06ZdMSXWq1myZIlbNmyhXnz5nH06FEuX77MsWPHCAsLY926dfqy586dIykpiczMTLKzs0lKSiIpKUl/Pj09HX9/f31C89SpU+Tk5NC7d299GVdXVxRFYf/+/dy4cYM7d+5ga2vLc889x1tvvcWBAwc4d+4cr732Gjk5OfoNGoxd17x5c1q0aEF0dDQXL17k8OHDLFu2zGRtI0S9UVsJNenX1Q8pZyEvD87+ZO5IaoUk1IQQQghRp3QXfjF3CEI8sIKCgsjJyWHEiBEsWrSIkJAQJk2aZLRseHg4hw4dolevXgYjqUwhIiKCoKAgli9fjr+/PyEhIfz444+o1WqSk5N58803WblyJWq1GoCVK1dy8+ZN3n33XX0dKSkpZGdnA7Bv3z66detWap21mpo8eTLbtm0jIyOD0NBQ+vfvz/z587Gzs2P69On6cs899xxDhw5l3759HD16lKFDhxq0WUFBASkpKdy7dw8oHE03cOBALC3vr6Dj7OzMvHnzePvtt+natSuLFi0CYOHChQwfPpxXX32VYcOGcenSJf7xj3/oNyMwdp2FhQUbNmzgp59+IjAwkKVLl7J48WKTto0Q9YKulqbNX6m/0wQbpXq6qUBNKTpdraWEHxjXrl2rlbUIFEXB2dmZ9PR0pJnrhrS5eUi7m4e0u2lols+C1ItYzFqK4vN4heWl3atOM+UZg9dKr35YTH2tSnXUp3a3srKiTZs2Zo1BVF5Z/bzs7OxSO0xWlZWVlaxnZSKTJ0+md+/ezJw5s8Ky9aHdBw0axKuvvsozzzxTceEHUPGfj/r0+7cxaajtXrJPYBHxGYqdQxmlTXcfJWQOFn4Vr+HaUNu9NpVsawBV1L8NzilT5mPR27/MOspr91LfMx/sRLGuvZ1hq9LPkxFqQgghhKhbDXQRdyFE9fXu3ZuRI0eaO4xKycvLY/jw4QwcONDcoQjx4JM+QeNw46rp6vq9/qyPJ7t8CiGEqFd0Oh3k5qDYNDV3KKKWyBPfB8OXX37Jtm3bGD58OJMnTwYKEwlbt24lISGB/Px8unbtSmhoqH7qGhSuhRUVFcXPP/+MjY0N/fv3Z8KECQaL5v/8889s3bqV1NRUWrVqxV/+8hcGDBhgcP+9e/fy1VdfkZmZSbt27QgODsbLy6sO3rkwh8qMTKsvrK2tmTt3rrnDEOKBo8u5Z+RgXSXUZE01c9J9vhWtnQMWT5hincz604+UEWpCCCHqFd2nf0f7yjh0v6bU7Y1radt2YUSddZ5FdSUnJ7Nv3z7atWtncPzTTz/lxIkTzJ07l2XLlnHr1i3Wrl2rP6/Vann77bcpKCjgrbfe4qWXXuLgwYPExMToy1y9epVVq1bRuXNnVq9ezVNPPUVkZCQ//PCDvkxCQgJbt24lKCiId955h3bt2rFixQrZ5VAIIR5guq+2lz6orT/JEVG7dJ+uN01FSv1JY9WfSIQQQghAd+Sbwn/3/NPMkYhaIyPU6rWcnBzWr1/PtGnTaNasmf743bt3+c9//sMLL7yAj48PHh4ezJw5k19++YVz584B8OOPP3L58mVeeeUV2rdvT/fu3Rk3bhxxcXEUFBQAEB8fj6OjI88//zyurq4MGzYMPz8/vv76a/29du/eTWBgIAEBAbi6ujJlyhSsra05cOBA3TaGEEIIk9FdSTNyUPoED7wuPev2fvVomrAk1IQQQtRLuuOHzR2CqC0Zl80dgSjH5s2b6d69O4899pjB8QsXLqDRaOjSpYv+mFqtpnXr1vqE2rlz53j44YcNpoB269aNe/fukZqaCsD58+cN6gDo2rWrvo6CggIuXLhgUMbCwoIuXbroyxiTn5/P3bt39R9FuzFC4WLHJT+EEOUr+fNi7OdIPmr3o8G1u5ER6oqmAHRaNCvmov1ghUnbzuA+Fo243Wv7o0npDQLK/DpUo91L1aHT1ur7qQpZQ00IIUS9pdPpqvwfW9VvUrvVCyMyjDyhriHdjWvoDu9DCXgSxb6FyetvLI4cOcLFixd5++23S53LzMzE0tLSYNQagIODA5mZmfoyxZNpReeLzhX9W3SseJl79+6Rl5fH77//jlarLVVP8+bN+e2338qM/YsvvmDXrl361+7u7rzzzjtl7tR17949rKysyqyvskxRh6g6affaZW1tjbOzs8Gxtm3bmimaxq0htfs16ybklDhme+IwTf80kKuXktGRjFNzByya2tb4XqklXjdv3oJmJb6ny9OQ2r22Xbe1peTqeEW/P0p+HUr+XinJWLuXrKNN69ZYVeFrWZskoSaEEKLe0i6ahmrlJnOHIR4A2rWL4FoGuvM/o5q/wtzhPJCuX7/OJ598wuLFi7G2tjZ3OFU2atQoRowYoX9dlIy/du2afrppcXl5eeTn59fonlZWVjWuQ1SdtHvty8vLIz09HSj8WWrbti0ZGRmyqUwdaojtrrnze6ljv19M5u4j90dEZ5z8HsXjEZPfOzMzk+w/vqfL0xDbvbZpcnJLHUsvo63LOl6Vdr+WkY6i1F4qy9LSssyHcaXK1loUQgghRE1dy6i7e8kMsAdb0ffKLz+ZN44H2IULF8jKyuKvf/2r/phWq+XMmTPs3buXRYsWUVBQwJ07dwxGqWVlZelHkzVv3pzk5GSDeos2EihepuTmAllZWTRt2hRra2vs7e2xsLDQj2grYmz0W3FWVlZljlqSP4qEqLqSPzc6nU5+lsygIbW77syPpY9d/c1gsoD2wi9YuHvX/GbqdpD2v/v3qWI7NqR2r226kwmlj+l0aA98bfR4uXVVot11Gm29WXtPEmpCCCGEEIIuXbqwZs0ag2MffvghLi4ujBw5ktatW6NSqfjpp5/w8/MD4LfffuP69et4exf+8ePt7c3nn39OVlaWflrnqVOnaNq0Ka6urgB06NCBxMREg/ucOnVKX4elpSUeHh4kJSXRu3dvoDCxl5SUxLBhw2qvAYQQQtS9tP8Z7tqYetE09VqXXtdL1BIjo8B150+j27axdu6nKX0/c5FNCYQQQgjxQNNpNOYOoUFo2rQpDz/8sMFHkyZNsLOz4+GHH8bW1paBAweydetWkpKSuHDhAhs2bMDb21ufDOvatSuurq68//77XLp0iR9++IHt27czdOhQ/eixIUOGcPXqVaKjo0lLSyMuLo6jR4/y1FNP6WMZMWIE33zzDQcPHuTy5cts3ryZ3NxcBgwYYI6mqTeCgoIIDw8v87yvry9RUVEV1hMdHU3Pnj1xdXUlKiqKtWvXMnjwYFOGKupYamoqarWapKQkc4ciRNVZ3J8moPvxO9PUebHEJja1vSavMKD7tnK7cldnFKD2zdnotPWj7ycj1IQQQjRKuvw8+Pkk5JZcRlU8aHRbIswdQqPxwgsvoCgKa9eupaCggK5duxIaGqo/b2FhwYIFC9i8eTOLFy+mSZMm9O/fn3HjxunLODo6smDBAj799FNiY2Np1aoV06dPp1u3bvoyffv2JTs7mx07dpCZmUn79u1ZuHBhuVM+BcTGxmJre38xb7VazZYtWwxG9t2+fZvFixezZMkShg8fjr29PVqtlhdffNEcIRs1Z84cDh06REZGzab9R0REMG7cOI4cOUJkZCQnT54kJycHNzc3AgICmDp1Ks7OzuTk5LBgwQJ++uknzp8/z6BBg/joo48qfZ+goCBGjx7NhAkTahRvZc2ePZvs7OwqxSjEA8P2IXNHIEyhEglM3YkEtP/4EItpYSgdH6uwvIHffgVX92oGZzqSUBNCCNEo6bZHoTsUZ+4whAnojv3X3CE0WEuXLjV4bW1tTWhoqEESraQ2bdrw+uuvl1tv586dWb16dbllhg0bJlM8q6hVq1YVlklLSyM/P5/AwECcnJz0x0vu3mouGo2G/fv3s2nTJjw9PfXHQ0ND6dixI/Pnz8fS0pLc3MJFsFUqFQCRkZEcPHiQ7du366+xs7Pjs88+Y+HChYwZM4aoqCjc3NxIS0tj165dbNy4kaVLl6LVarGxsSE4OJjY2NgqxXvr1i2OHz/Ohx9+aIJ3Xz6NRlP7O18LUdec3QzXw6qttbHySi+cL2pRJX5XaSNXFf67ZhEWm78yWqbMEWyKqtqhmZJM+RRCCNEo6f4bb+4QBKA7dQzdDyaa3iFENel0OnS5Oeb5qOIfjxqNhkWLFtGxY0d8fHxYvXq1vo7iUz59fX0BCAkJQa1W4+vrS0xMDIGBgQD06dMHtVpNampqqSmfs2fPJjg4mMjISLp3707nzp1ZuHChwc6aubm5LF++nB49euDl5cWIESNISChcmDonJ4eAgADCwsL05S9duoS3t7dBwsuY48ePY2VlhZ+fH46OjvoPa2trbGxscHR0xMnJCWdnZ5ydnfXnmzVrhkqlMrjm1q1bhIeHExwcTEREBH379sXNzQ0/Pz/WrFnDnDlzALC1tWXVqlVMnDix0ju7Ffnmm2/w8fGhTZs2PPnkk0RGRurPBQcH065dO+7cuQMUrjmoVqu5eLFwjajMzExeffVVOnXqhKenJ5MmTeLChQv662NiYnj00UeJj49nwIABuLu7M3fuXHbu3ElcXBxqtRq1Wq1vd4Bff/2VoKAgPD09GTRoEMePH6/S+xGizllZgWWxcT61NEJN9+n6WqlXAN38Sh+rYvK/zP8LUy8YP25jU6X6a4uMUBNCCNFIKWCwr5Q89a9ruoJ8tOvfBMBi3TaUZjLNQ5hJXi7al8dW+TJTjHeweH8HNKn8HwY7d+5k/Pjx7N69m1OnThEWFoZarWbixIkG5WJjY3nssceIiIggICAAlUqFra0tLi4ujB8/nq+//hoXF5cyR7UlJCTg6OjIzp07uXjxIjNmzKBz5876+yxevJhz586xYcMGnJyc2Lt3L5MmTWL//v14eHiwfv16nn76aQIDAxk0aBCvvPIK/v7+jB8/Xn8PtVqtn5ZZJD4+nkGDBplkJNbu3bvJy8tj5syZRs8XbZxRGampqfj5+bFz50769u1rEO/QoUMB8PPzIyEhgenTp6PT6fjuu++wt7fn+++/JyAggG+//Za2bdvi7l44TWnOnDlcvHiRjz/+mIceeoiVK1fy3HPPcfDgQf2ag/fu3eOD5Fi5CQABAABJREFUDz7g3XffpUWLFjg5OZGTk8Pvv/9OREThdPfmzZtz5coVAN555x3eeOMN3N3deeedd3jppZc4cuQIlpbyZ5+on5R2XoabEjSppc0EHFrUTr0CpY0TpdJhShXHbpWRUNOVtUmFVlu1+muJjFATQgjROFlIAs3sineG7v5uvjiEeIC4uLiwbNkyvLy8GD16NMHBwUY3IihKlDk4OODo6EirVq1o2rQpLVq00J93dHTUT5ksycHBgRUrVuDl5cXgwYMJDAzk8OHDQOG00ZiYGDZu3Iivry/t27dn+vTp9OrVi5iYGAB8fHwICwtj/vz5LFmyhLS0tFLTfD09PbG3tzc4FhcXx5AhQ2rWSH+4ePEidnZ2BlNbq8vS0hJPT0+aNm2qP5abm8vBgwf18fbp04djx46h0Wg4ffo01tbWjBo1iqNHjwJw9OhR+vTpA8CFCxeIj4/n3XffxdfXl86dO7N+/XoyMjLYu3ev/h75+fmsXLmSXr164eXlhZ2dHTY2NlhbWxuM3isyffp0Bg0ahKenJ/Pnz+fy5ctcunSpxu9fiFrTzgt0xfoD2tqZ8qk82q1W6hUYT55V+aFIGV/3O2X0D+/eqWL9tUMeVQghhGicZGdI8yve2ZKvhzAn6yaFI8WqyMrKymAaZHXvXRWPP/64weitHj16sHHjRjQm/hny9vY2SLY5OTlx5swZAM6cOYNGo6Ffv34G1+Tl5ekTdgDTpk1j7969fPzxx0RHR9OyZUuD8ocOHTJ4ff78ea5cucITTzxhkveg0+lMtuaYs7NzqXiPHDlC69ateeSRR4DCaba///47SUlJHD9+HD8/P/r27cv7778PFCbUZsyYAUBycjKWlpY8/vjj+vpatmyJp6cnycnJ+mPW1tZ06tSp0nE++uij+s8dHR0BuH79Ol5eXlV8x0LUEZ3OMImmq6WRR/VkimCD1NS29LGq/u4tK5FqZW30sDZ2J6oZC6p2j1ogCTUhhBBCmJ8k1IQZKYpSpWmX+uusrFAs6sfCyKZWNOWwuKI1bu7cuYNKpWLPnj2lRrgV39zg+vXrXLhwAZVKxcWLFwkICCj3nvHx8fj7+2Njoj98PTw8yM7O5sqVKyYZpVZSfHy8wdpzDg4OdOrUiaNHj3LixAn69euHr68vM2bMICUlhYsXL+LnZ2StoXLY2NhUKSlYfGpn0XXaejI1SgijUs6Cu3et30YZOKLW79FYKX0HovvXP2pYSxkJtbKmq2ffquH9TEOmfAohhKjXqrpgt3iAFPva6s6eMmMgQjw4EhMTDV6fPHkSd3d3o1M3raysTD5yDQqnc2o0Gm7cuIG7u7vBR9GoKIB58+bRsWNH1q1bx4oVKzh//ny59ZpyuifAU089hbW1NRs2bDB6Pisrq9p163Q69u3bp18/rYifnx9Hjhzh22+/pW/fvrRo0QIvLy/+/ve/4+TkpN+51MvLi4KCAk6ePKm/9ubNm6SkpNChQ4dy721tbV0rX1chzEH37QEMkikpZ2tep7GfDxsjo6iEaViWfgBTZWX198sYoVZru8FWUY0Sal9++SVjx47lk08+0R/Ly8tj8+bNBAcH89xzz7FmzRoyMzMNrrt+/Tpvv/02kyZNIjQ0lM8++6zUfwo///wzf/3rX5kwYQKvvPIKBw8eLHX/vXv38tJLLzFx4kQWLlxoMDxaCCHEg0d387qRg3X0ZN1E04JE9ei2b0J3IqHigpWpq550soSoDWlpaSxdupTk5GS+/PJLPvroI0JCQoyWdXV15fDhw1y9erVUf7wmPD09GT16NLNmzSI2NpZff/2VxMRE1q9fz/79+wH45JNPOHHiBOvWrWP06NEMGzaMl19+mby8PH09/v7+7NmzByj8++DUqVMGI75qSq1Ws2TJErZs2cK8efM4evQoly9f5tixY4SFhbFu3Tp92XPnzpGUlERmZibZ2dkkJSWRlJSkP5+eno6/v78+oXnq1ClycnLo3bu3wT379u3L//t//w9LS0v9NMs+ffrwxRdfGIxO8/DwYOjQoYSFhfH999/z888/8+qrr9K2bdtSSbqSXF1dOXPmDMnJydy8ebPm046FMDPdL0kVF6psXT+dQDvzL6VPqGQsUa0xRR+6jJG0irOr8fL15KFCtb+rkpOT2bdvH+3atTM4/umnn3LixAnmzp3LsmXLuHXrFmvXrtWf12q1vP322xQUFPDWW2/x0ksvcfDgQf0CpgBXr15l1apVdO7cmdWrV/PUU08RGRnJDz/8oC+TkJDA1q1bCQoK4p133qFdu3asWLGiRk+ahBBCmJf2k/eMHJTkSINV4kur/ceHJqpXpjeJhisoKIicnBxGjBjBokWLCAkJYdKkSUbLhoeHc+jQIXr16lVhkqaqIiIiCAoKYvny5fj7+xMSEsKPP/6IWq0mOTmZN998k5UrV6JWqwFYuXIlN2/e5N1339XXkZKSQnZ2NgD79u2jW7dupdZZq6nJkyezbds2MjIyCA0NpX///syfPx87OzumT5+uL/fcc88xdOhQ9u3bx9GjRxk6dKhBmxUUFJCSksK9e/eAwtF0AwcOLLV7Zu/evdFqtQbJs759+6LRaPQbEhSJiIigS5cuvPDCCzzzzDPodDo+++wzo9Nti5s4cSKenp4MHz6cLl26cOzYsWq3jxD1gW7nRyarS/v3ZcaTMw10en59pdv/76pdUNbU9KKHpC1bGx4vKKh6ULWgWmuo5eTksH79eqZNm8bnn3+uP3737l3+85//MGvWLHx8fACYOXMmc+bM4dy5c3h7e/Pjjz9y+fJl3njjDZo3b0779u0ZN24c//jHPxg7diyWlpbEx8fj6OjI888/DxQ+hTl79ixff/013bp1Awq3wQ4MDNSvxTBlyhROnjzJgQMH+POf/1yDJhFCCGE2V9NLH5PRRqKqtFrpOIsGadeuXfrPV61aVer8d999Z/B6yJAhpaZQ+vj4kJaWZnBs3rx5zJs3T/+6+MitIsuXLzd4bWVlxfz585k/f77RWFNSUgxeOzg4lEr8FI+joumexd+7MSXfQ3H+/v74+/uXe33JtivJzc3NIN74+HheffXVUuVatGjB5cuXDY4NGzasVJsDNG/enL///e9l3nPcuHGMGzeu1PFWrVrxf//3f6WOl7yHg4OD0fsK0ShJv6AW1d4ItbLuUebItTpWrYTa5s2b6d69O4899phBQu3ChQtoNBq6dOmiP6ZWq2ndurU+oXbu3Dkefvhhmjdvri/TrVs3Nm/eTGpqKu7u7pw/f96gDoCuXbvqp5YWFBRw4cIFg8SZhYUFXbp04dy5c2XGnZ+fbzAkWlEU/dbXptoBqLiiOmujbmGctLl5SLubR4NsdyMjixR0UFCAdv1ylI6PYTF8TO3cW1Eq1ZYNst1rW9NmcM9we3NFUUr3v3LuldmuVWl3xYQ7+wkh6kbv3r0ZOXKkucOolLy8PIYPH87AgQPNHYoQorIsZMpnrTFFl0tbxhTOsp6rd+1dxom6VeWE2pEjR7h48SJvv/12qXOZmZlYWloa7O4DhU9HitZtyMzMNEimFZ0vOlf0b9Gx4mXu3btHXl4ev//+O1qttlQ9zZs357fffisz9i+++MLg6Za7uzvvvPMObdq0Ke8t11jbtm1rtX5RmrS5eUi7m0dDavffLFSU/O+05a2raK5d4ebpH9Cd/gGX4FdMkixJLfG6VatW2Dg7V/r6htTutS1NpaJkqtTZ2Rltzj0Mxk7k5+FcwdfAWLuX/Fq2dXTEwtgW7kKIemvmzJnmDqHSrK2tmTt3rrnDEKLR06VfRncyASXwaRSbpuUXViShVmuqu5OwQ0vIulm5Ourpg9IqJdSuX7/OJ598wuLFi7G2LmO3hXps1KhRjBhxf7vcoj/Irl27RkEtzMFVFIW2bduSkZEhCyTXEWlz85B2N4+G2O6anHuljt04+zO6axn61+n/u4TSxMbk975x4wYW6UamnJbQENu9tmmNPF5MT09Hl5tj9LgxVWn3jN/SUGwfql6wlWBpaVnrD+OEEEIIUT5t+B+J+OxMlGenmjeYxsxIf64iOo3mfjINDBJqOq0G3b5/o3j7YDBEzd0bLv4xI/H6lWoGa1pVSqhduHCBrKws/vrXv+qPabVazpw5w969e1m0aBEFBQXcuXPHYJRaVlaWfjRZ8+bNS+3GWbSRQPEyJTcXyMrKomnTplhbW2Nvb4+FhUWp3YqMjX4rzsrKqsxFPmvzjyKdTid/dNUxaXPzkHY3jwbV7r9nlzqky8uF/Pu7wulyc8G6ienvXcV2bFDtXtt+v13qkE6nQ5edafR4eSrT7jqNRtbeE0IIIRoJXcrZShSqH7tCNkjVmE6rO7LP8ECxKZ+6I9+g2/UxOsDi9fsb2Vi8OFufRNV9GQ1Pja1WuKZUpYRaly5dWLNmjcGxDz/8EBcXF0aOHEnr1q1RqVT89NNP+p1tfvvtN65fv463tzcA3t7efP7552RlZemndZ46dYqmTZvi6lq4sFyHDh30W1IXOXXqlL4OS0tLPDw8SEpK0m9VrdVqSUpKYtiwYVVtAyGEEPWY7lAcSodO9w/k3AU7+5pXbGVtkKirr0PJGyrd5Utol5Ve0Nskqjv1QAghhBANk2X5u+eKGnBoUeVLdP/52vBA8b5besnFPChc67iebERQXJVSiU2bNuXhhx82+GjSpAl2dnY8/PDD2NraMnDgQLZu3UpSUhIXLlxgw4YNeHt765NhXbt2xdXVlffff59Lly7xww8/sH37doYOHaofPTZkyBCuXr1KdHQ0aWlpxMXFcfToUZ566il9LCNGjOCbb77h4MGDXL58mc2bN5Obm8uAAQNM1zpCCCHM71pGYfLrD7qfT5qm3uLJNFHntGsX1Vrdum8P1FrdQgghhHjwKJJQq19uXjN8XbxfXnzEWzkzDnS1sGxXVVVrl8/yvPDCCyiKwtq1aykoKKBr166Ehobqz1tYWLBgwQI2b97M4sWLadKkCf379zfYEtrR0ZEFCxbw6aefEhsbS6tWrZg+fTrdunXTl+nbty/Z2dns2LGDzMxM2rdvz8KFC8ud8imEEOIBpNMV7hJZxMg6a+IBlJdbYRHdtQx0P36H0m8Yik3l183T7fwYhoyqSXRCCCGEMBPtN19hEfh05S/4X3LFZUS9ofslCe7dNTimCX8J7c6DhS+MTSE1NpMk9x5Y2pk+wCqocUJt6dKlBq+tra0JDQ01SKKV1KZNG15//fVy6+3cuTOrV68ut8ywYcNkiqcQQjRwSo8/GW7HbYKlsYyuwSVTPutWJXbb0i55ufCJ5fWrUMXFhnVaLUo11vQQQgghhHnptkehC3iqcJaCo7NJdncX9Yd2zUKjx+8e3Avd/wQWqspVVA/Wy5WephBCiPqtVesS/7GaJKNW+tCV32per6i8ynSO/xj+r/vmqzKL6H69YPyErKMmhBBCPLB0//gQ7eLp5fYBxAOmtVO5p3PPJhV+UuyBqO7A12WUxiR/EtSUJNSEEELUb1otSjtPg9c1ZiyhdnifkYLCJFq2Ln2sEiPUKkP79Q7jJ3SSUBMNT1BQEOHh4WWe9/X1JSoqqsJ6oqOj6dmzJ66urkRFRbF27VoGDx5sylAbrb179/KnP/0JNze3cr9WQojy6Q7FFf77721mjkSYzPUr5Z5u2qNP4SfFHqTrvvt/hZ8Y7TeaP6Nm8jXUhBBCCFNS3L0ND5hieLeRZIvyp0E1r1cYZ2Nb+pipZm9cSTN+XEaoiUYoNjYWW9v7P29qtZotW7YYLJFy+/ZtFi9ezJIlSxg+fDj29vZotVpefPFFc4Rs1Jw5czh06BAZGRk1qiciIoJx48Zx5MgRIiMjOXnyJDk5Obi5uREQEMDUqVNxdnYmISGBqKgofvjhB27fvo27uzszZsxg9OjRlbqPn58fq1evxt/fn7/+9a+MGzeO4OBgHnrooRrFL4SAGncY2rTFYtwU04QiapXmdnbhJ+mXS5+0qJ/TfiWhJoQQon6zUBkm0UySUDNSh3WTmtcrjCs5vdPS0mQj1Liabvz477ehSeU3MhCiIWjVqlWFZdLS0sjPzycwMBAnp/vTb5o1a1bOVXVHo9Gwf/9+Nm3ahKfn/dHJoaGhdOzYkfnz52NpaUlubuHGJipV4UiGyMhIDh48yPbt2/XX2NnZ8dlnn7Fw4ULGjBlDVFQUbm5upKWlsWvXLjZu3MjSpUs5fvw4jz76KDNnzqRNmzbs37+fWbNmYWdnV+HIvdOnT5OVlUWfPn24c+cO169fp3///rRt27bM96coChayxqMQlXPvTo0uV63cZKJARG2zdHYFQPf9/yt9sqjf2MwO7twu/Dw9Fewc6ig64+Q3uRBCiPpNpzMc0W2KhJq2dB2KQ/Oa1yuMUtTtjRw0UeWuRuoGdAnfmOgGojHQ6XTkFGir/pFfjWtKfBjdJKUcGo2GRYsW0bFjR3x8fFi9erW+juJTPn19fQEICQlBrVbj6+tLTEwMgYGBAPTp0we1Wk1qamqpKZ+zZ88mODiYyMhIunfvTufOnVm4cCH5+fn6Mrm5uSxfvpwePXrg5eXFiBEjSEhIACAnJ4eAgADCwsL05S9duoS3t7dBwsuY48ePY2VlhZ+fH46OjvoPa2trbGxscHR0xMnJCWdnZ5ydnfXnmzVrhkqlMrjm1q1bhIeHExwcTEREBH379sXNzQ0/Pz/WrFnDnDlzAHj11VcJCwujV69etG/fntDQUAYMGMCePXsq/HrExcUxYMAAjh07hrd34YjqsWPHolarSUhIICYmhkcffZT4+HgGDBiAu7s7aWlp5bZfke+//55Ro0bh6elJz549eeONN7h7966xMIQQwoxM06mzaGpkRoP+ZGHqyuL/s3fvcVGVa9/Af2uGQdQQjyAMhBwkFRRREbBECPOclmFaus0Ejx08xlZxK2qaWpI9GmFkpvL0RtrJbQhI6WMGbjVRU2kLgqmEIipgxHFm3j+IJQMDzMDADPD7fj7IzFr3utc19wizuNZ9WBz2aFu79no5b2OwhxoRERk11bVUCL1dq2xomiGf6O3W+HpJI+GZiZrvNuoDe6GRHpQoVJgac9Ug546Z6gIzE+3/GDlw4ACmTZuGw4cP4+LFiwgJCYFcLsf06dPVysXGxmLAgAEIDw+Hv78/pFIpOnToABsbG0ybNg3ff/89bGxsau3VlpSUBEtLSxw4cACZmZlYsGABXF1dxfOsXr0aV69eRUREBKysrBAXF4cZM2YgMTERjo6O2LFjB5599lkEBARg5MiReOONN+Dr64tp06aJ55DL5eKwzEoJCQkYOXKkXlb1O3z4MEpLS7Fw4UKN+y0sau/Z8PDhQ/Tu3Vt8npSUhClTpuDUqVOws7MTtx89ehRz587FkCFDcOLECfj6+iIqKgpDhgxB586dcfPmTRQVFeHDDz/Eu+++iy5duqB79+71tt/169cxffp0hISEYNu2bbh37x5Wr16N0NBQvP/++41uG6LWSNcbFNT8VJfO1bGzjvfv7x5qQq9Hv5fx1596iqrh2EONiIiMmirhW1TtoqbK+K3xlZaX1dwm5UdikzGRNVnVQrcemndwDjVqpWxsbLBu3To4Oztj8uTJmD17tsaFCCoTZRYWFrC0tES3bt3Qvn17dOnSRdxvaWkpDpmszsLCAhs3boSzszOeeeYZBAQE4OTJkwAqho3GxMRg165d8PLyQq9evTB//nx4enoiJiYGAODm5oaQkBAsX74ca9euRVZWFrZu3ap2DicnJ3Tq1EltW3x8PEaNGtW4RvpbZmYmzM3N1Ya2auPQoUO4cOGCWqKvffv2cHJygonJo/4I2dnZSE1Nhb+/P0xNTdG9e8UCLJ07dxZ71QFAWVkZNm3aBE9PTzg7O+P+/fv1tt/OnTvx/PPPY86cOXB0dISnpyc2bNiAgwcPori4uLFNQ9Q6/dW44aHU9JQfhNW6T1XXglJVh8lX9mSzd9ZPUI3AHmpERGT0VKd/evTkyvlG1aX8KQGqfTtr7tDXnF5Ukx56mtRa9aBhUP2sYXinorzJzkmtTzupgJipLvUXrEZmIkOZpgS9jufWxaBBg9R6bw0ePBi7du2CQqFoVBzVubi4qCXbrKyskJqaCgBITU2FQqHA8OHD1Y4pLS0VE3YAMG/ePMTFxWHPnj2Ijo5G165d1cqfOHFC7XlaWhru3LmDp556Si+vQaVS6dzT7eeff8bSpUuxdetWPPHEE+J2Dw+PGvEmJCTA09Ozzp5uAGBqaop+/fqJz7VpvytXriA1NRXffPON2utRKpW4efOmWu85opagWXqPcYXvlk1Rx/tX5Xe5ZOseQFEOoYPh5/5kQo2IiIye6sxP9RfSti5NyTRAL8OLSEvl5RWLBuiTvTPwe/qj50r9JheodRMEQadhl5VkMgmkrXTAh0xWs2dp5R/EhYWFkEqlOHLkSI0eblUXN8jNzUVGRgakUikyMzPh7+9f5zkTEhLg6+sLMzP9DOV2dHREQUEB7ty5o1UvteTkZMyaNQthYWGYMmVKveWPHj2qVW86MzMztc8YbdqvsLAQM2bMwOzZs2vUJ5fL6z0nkdFpjp7j1c9RfaV4Mm5a9lATzAw/d1ql1nkFQEREREakCZOVlTe8qydEDbzqE1FTSUlJUXt+7tw5ODg4aBy6KZPJ9N5zDagYzqlQKHDv3j04ODiofVlaWorlli1bhj59+mD79u3YuHEj0tLS6qxXn8M9AWD8+PEwNTVFRESExv35+fni46SkJMycOROhoaGYMWNGvXUXFhYiKSkJo0eP1jkubdqvf//+uHr1ao39Dg4O4lBSohalOXqolav3Thf6DWz6c5L+aDvk04gYZ1RERETUehig85/wuFPzn5SoGWRlZSEsLAzp6en49ttv8emnnyIoKEhjWVtbW5w8eRI5OTnIy8vTWwxOTk6YPHkyFi1ahNjYWNy4cQMpKSnYsWMHEhMTAQCfffYZfvnlF2zfvh2TJ0/GmDFj8Prrr6O0tFSsx9fXV1xJMzc3FxcvXlRbbbSx5HI51q5di927d2PZsmVITk7GrVu3cObMGYSEhGD79u0AKoZ5zpw5E7Nnz8a4ceOQk5ODnJwcPHjwQKwrJSUFvr6+yM7OBgAcO3YMjo6OagsUaEub9lu4cCHOnj2L0NBQXLp0CRkZGYiPj0doaGjjG4bIIOpIqMnt9XOGrz5T38DRBy3Kg4/Dobx4RvNOI52ahUM+iYio1VIV/QXczQbsHDmk05D00EOm9rlX/t7O95faiMDAQBQXF2PChAmQSqUICgqqtUfVmjVrsG7dOnz++efo2bMn/vOf/+gtjvDwcHzwwQdYv349bt++ja5du2LQoEEYOXIk0tPTsWHDBmzbtk0cnrhp0yaMHDkS7777rpgUunbtGgoKCgBUDJ8cOHBgjXnWGmvWrFlwdHTErl27EBwcjOLiYtja2mLkyJGYO3cugIqVU4uKirBz507s3PloWgAfHx8cPHgQAFBUVIRr166h/O8eMPHx8Y1K/tXVfgDQr18/fPXVV9iyZQsmT54MlUoFe3t7TJw4scHnJDKounqoZf2un1OcVp/n0FiTMKRZ+Y0M4H/Wa94pMc7rPEHFtWVx9+5dlJU1bkJZTQRBgLW1NbKzs7mEbzNhmxsG290wWmO7K+Zo94eCNOqQdvWFzAYe5EKyaC0Et8F1nkPbOltjuzc15Yk4qPZrHnJVXdX3oep7Jf34W9jIbcV2V+VkA117AJfPQbnz7Yp5UjKviuUlS9ZB6OehvxdRhUwmQ48etawuSkantuu8goKCGitM6komkzXJNWRbNGvWLAwdOhQLFy6st6yh2728vBzu7u6Ijo6Gh0fT/J4xtKo/H/zcM4zW1u6q0hIoX/t7bsLHHYEbGVofW9s1mnid4NQH0hVba1zjSbb/L4SO5jrF2dravTmoysuhXDC59gKPdQL+LGjcSZ7oD+nyjY2rQ0u6XOcxZUtERK3Xg1wAgOpcsoEDaePqWrWpFqoLp9U3VJloWHUuGcrQeVDuWK92x1uY9PKj4v/+Qvc4ichghg4dikmTJhk6DK3k5eVhzpw5GDhwoKFDIWo5quSmJG+9o9+6TWouogJA52QaNVA9owQk81c0/hycQ42IiMhAeIfRoITe/XQ+Rrk7vNqGRwk15bHvKx5cOa9+nnEvPnqSnqrzOYnIcBYuXNhiVq/s3r07Fi9ezKkEiHRRdcJ5/uy0LU59Gl+HkQ7fNc6oiIiIqPXo0FH3Y4r+Un9eJaGmfpfy0RxqgpHevSQiImrz7uU8eqzvhJqmG6c9eur3HNRw+pj/zEiv8YwzKiIiojqoCh/qeIDuQw7JcDTNWaI20bBEWvMg3u0mIiIyWqrU81We6L32Glsk6z7U90moofTRu8xIr/OYUCMiohZHuWODTuVVScfqLmCk3cjbKmXEpprb9u2EIu9+xZOqdyk5mpeIiMj4/VXYdHVruBEnyDTPq0bNS5j8in6Gx+c/aHwdTYB/QRARUctz7Teosm9BeeQrqEpK6i9fXw8147zp1fbITCu+n/+Pxt3lf9yoeFClh5qqvLypoyIiIqLGqjp1g76vu3hzzXhZdNZPPTeu6acePTMxdABEREQNoVyzsOJBYQGEwFcNGwzph1V9E5L/fQVepYea6pP3mi4eIiIi0o+u3cWHgmk7AwZCzaqVjwJp3a+OiIhaPVXGf+stIwx+su4Cdo56ioYa5VZmnbul3a0AQH3xgco73q38go2IiKhFUzZ8PlvV/dz6SjS4bmpirXwUCK8+iYjIKKiUioYdqMW8DKq8e7UfPnwUJAtWNOzc1MwqLphVZ0/W3GWkqz8RERERGpVQw/2cuvdrWuWTjEM9b40w8/XmiaOJ8OqTiIiMg6KBF1pZN+ovc+23WndJZr4OoZtlw85NWtLT7cm6LsY1JFY5vxq1NoGBgVizZk2t+728vBAVFVVvPdHR0RgyZAhsbW0RFRWFbdu24ZlnntFnqG1STEwM+vbta7T1AUBSUhLkcjny8/P1Wi9RvRqRUFNuW63HQEjv6rjM03gT9G/SqEOQDB+l3TlMjHO2MuOMioiI2p6G9lArfKjfOMh41XUH+u8easJLc6H6fx9XbDPSJdaJmkpsbCw6dOggPpfL5di9ezfGjBkjbnv48CFWr16NtWvXYty4cejUqROUSiVefdV45qJcsmQJTpw4gdu3bzeqnvDwcEydOhU///wzIiMjce7cORQXF8POzg7+/v6YO3curK2tkZSUhKioKJw/fx4PHz6Eg4MDFixYgMmTJ2t1Hm9vb2zdurVRsRK1dqpjsQ0/uL4bZOyhZrz0tTqnRVf91KNnTKgREZFxUDR9b6IGDyulZqP69Wzt+5RKtRU+1fw9h5owaNijhFpRIfBYJ32HSGS0unXrVm+ZrKwslJWVISAgAFZWVuL2jh07NmVoWlMoFEhMTMTHH38MJycncXtwcDD69OmD5cuXw8TEBCV/r/AslVb8ToiMjMTx48fxxRdfiMeYm5tj//79WLVqFaZMmYKoqCjY2dkhKysLBw8exK5duxAWFoazZ8+ib9++WLhwIXr06IHExEQsWrQI5ubm9fbcu3LlCvLz8+Hj44Ovv/66CVqEqJXI+aPp6q6eUHPWb89OaoTf0+sv49SnztEkAB6tBG9kOOSTiIiMg6IZkl3p9XxYk8Ep/2d97TtVdQwXqZxDraP5o20djCNBQMZPpVKhvNwwXyode1YoFAqEhoaiT58+cHNzw9atW8U6qg759PLyAgAEBQVBLpfDy8sLMTExCAgIAAD4+PhALpfj5s2bNYZ8Ll68GLNnz0ZkZCQ8PDzg6uqKVatWoaysTCxTUlKC9evXY/DgwXB2dsaECROQlJQEACguLoa/vz9CQkLE8tevX4eLi4tawkuTs2fPQiaTwdvbG5aWluKXqakpzMzMYGlpCSsrK1hbW8Pa2lrc37FjR0ilUrVjHjx4gDVr1mD27NkIDw/HsGHDYGdnB29vb7z33ntYsmQJAODNN99ESEgIPD090atXLwQHB8PPzw9Hjhyp9/2Ij4+Hn58fZDKZxv179+7FsGHD0KtXLwwfPhwHDx5U25+fn4+QkBC4u7vD0dERTz/9NI4ePaqxrnv37mHs2LEICgpCSUkJlEolduzYAW9vbzg5OWHkyJE4fPiw2jE//PADnnrqKTg5OSEwMBA3b96s9zURtUSqwj/Fx5Ln/mHASEhN1euyWgg9etZfxsFFH9HoHXuoERGRcWiGHmo1zmFt1/TnJP1RqoBaOqhVDu8UZDJI3o8GJBIItfVmI6pGoQCOfGWYOaXGvmCh09QwBw4cwLRp03D48GFcvHgRISEhkMvlmD59ulq52NhYDBgwAOHh4fD394dUKkWHDh1gY2ODadOm4fvvv4eNjU2tvdqSkpJgaWmJAwcOIDMzEwsWLICrq6t4ntWrV+Pq1auIiIiAlZUV4uLiMGPGDCQmJsLR0RE7duzAs88+i4CAAIwcORJvvPEGfH19MW3aNPEccrlcHJZZKSEhASNHjoSghyHbhw8fRmlpKRYuXKhxv4WFRa3HPnz4EL179xafJyUlYcqUKTh16hTs7B59dhw9ehRz587VWMeRI0ewdu1ahIWFYfjw4UhMTMTSpUthbW2NJ598EkqlEjNmzEBhYSF27NgBe3t7XL16Vex1V1VWVhZeeuklDBo0CNu2bYNUKsUHH3yAr7/+Gps3b4aDgwNOnTqFN998E926dYOPjw+ysrIwZ84cvPLKK5g+fTouXryI9evruGlB1FJlXoXqRPyj52ZmhouF1Nk51F9GmxtLRjqNBxNqRERkHJqjh1q1CXGFob5Nf07SH216qAEQOMyTWjEbGxusW7cOgiDA2dkZv/32G6Kiomok1CoTZRYWFrC0fLTwSpcuXcT9VbdXZ2FhgY0bN0IqlcLZ2RkBAQE4efIkpk+fjqysLMTExOD06dPo2bOiZ8H8+fNx7NgxxMTEYOXKlXBzc0NISAiWL1+OSZMmISsrC3v37lU7h5OTEzp1Uv95jY+PR1hYWIPbp6rMzEyYm5urDW3VxqFDh3DhwgVs2bJF3Na+fXs4OTnBpEr2Mzs7G6mpqfD399dYT2RkJF588UXMmjULQMXrPXfuHCIjI/Hkk0/ip59+wvnz53H8+HFxeKu9vX2NetLT0/HSSy9h7Nix4ntfUlKCHTt24IsvvsCQIUPEY8+cOYPo6Gj4+Phg3759sLe3x9q1awFA/P/y4Ycf6tQeRC1C1XyLkSZf2iRtbm5q01HbSFdzZ0KNiIiMgyESauyh1qLUOTROMM4LLWoZpNKKnmK6kslkasMgG3puXQwaNEit99bgwYOxa9cuKPT8O9TFxUWtp5SVlRVSU1MBAKmpqVAoFBg+fLjaMaWlpWLCDgDmzZuHuLg47NmzB9HR0ejaVX1S6RMnTqg9T0tLw507d/DUU0/p5TWoVCqde7r9/PPPWLp0KbZu3YonnnhC3O7h4VEj3oSEBHh6etba0y09Pb1GotPT0xO7d+8GAFy+fBnW1tZqc8VVV1xcjMmTJ+O5555T6112/fp1FBUV4aWXXlIrX1ZWBjc3N/H8Hh4eavsHDx5c67mIDE3y5loo/2ddAw+ueh3AhJrR0FcijAk1IiKiOtS1nLogqbt3kpZU509Vq7fRVZJW9LP6VsmlFKj6emjeKeGbSQ0nCIJOwy4rmZgIUKla5/89TXOCVSa1CwsLIZVKceTIkRrDE6subpCbm4uMjAxIpVJkZmbW2pOrUkJCAnx9fWGmp+Fajo6OKCgowJ07d7TqpZacnIxZs2YhLCwMU6ZMqbf80aNHMWrUqAbHp83rNDU1xfDhw/HDDz9gwYIFsLa2BlDxHgDAvn37xF6CVY8hMnoyU6CsVH1bd916k6qpeqnxIBd43LHhdZH+aJUI0+I6kYsSEBER1aGuFTj1NLm82vwa1OLkfbQFijenadwnsIcatREpKSlqz8+dOwcHBweN827JZDK991wDADc3NygUCty7dw8ODg5qX1WHkS5btgx9+vTB9u3bsXHjRqSlpdVZb3x8fKMSVNWNHz8epqamiIiI0Lg/P//RvHlJSUmYOXMmQkNDMWPGjHrrLiwsRFJSEkaPHl1rGWdnZ5w9q75y8ZkzZ8S52fr27Yvs7Gxcu3at1jokEgn+53/+B/3798eUKVNw+/ZtABU9CNu1a4esrKwa74FcLhfPf/78ebX6zp07V+9rI2oWmnqP3rvT8Pqyfhcfqv582PB6SL+0GfKpxaIEMNG88Iuh8eqTiIiMg2m72vcVNu7CSPCppVeEba9G1Ut60lnzpOg6MdKhAET6lpWVhbCwMKSnp+Pbb7/Fp59+iqCgII1lbW1tcfLkSeTk5CAvL09vMTg5OWHy5MlYtGgRYmNjcePGDaSkpGDHjh1ITEwEAHz22Wf45ZdfsH37dkyePBljxozB66+/jtLSRz1SfH19xZU0c3NzcfHiRbXVRhtLLpdj7dq12L17N5YtW4bk5GTcunULZ86cQUhICLZv3w6gYpjnzJkzMXv2bIwbNw45OTnIycnBgwcPxLpSUlLg6+uL7OxsAMCxY8fg6OiotkBBdQsWLMCXX36JvXv3IiMjA7t27cKRI0cwf/58ABUrrXp5eWHu3Lk4ceIEbty4gR9//BHHjh1Tq0cqlWLnzp3o168fXnzxReTk5OCxxx7DvHnzEBYWhi+//BLXr1/Hr7/+ik8//RRffvklAGDmzJnIzMzEhg0bkJ6ejm+++UbcR2Rwmm6Elde9QJWqIA+KVXOhPPT/au6selOhdXYcNmJ1NLj07/fZ3rn2o8cGQnh6AmR1rOQpPD2hocE1KV59EhGRcagyP5ZkvZ4nTK5lyW7B0ka/56EGEQYP00MlvKShtiEwMBDFxcWYMGECQkNDERQUVGuPqjVr1uDEiRPw9PSssydVQ4SHhyMwMBDr16+Hr68vgoKCcOHCBcjlcqSnp2PDhg3YtGmT2Ftq06ZNuH//Pt59912xjmvXrqGgoABAxfDJgQMH1phnrbFmzZqFzz//HLdv30ZwcDBGjBiB5cuXw9zcXExsHThwAEVFRdi5cyc8PDzErzlz5oj1FBUV4dq1ayj/+w/++Pj4epN/Y8aMwbp167Br1y48/fTTiI6ORnh4OIYNe/Q7LyoqCu7u7li4cCH8/f2xceNGjb0KTUxMEBERgSeeeAIvvvgicnNzERISgsWLF2Pnzp3w8/PD9OnT8cMPP+Dxxx8HUJFQ/PjjjxEXF4dRo0Zh//79WLFiRaPblEhbqrIyKML/BdWVlJo7NU3V0Kt3zW1V64v/Brh7G6p/10yoqX75WXws2Dyuc6zURP7uLSh5c02tRYR2ZpC+PA/t3IeobZfM/ydg7wzJorUQunZv0jAbSlDVOcNv23D37t1GTyiriSAIsLa2RnZ2dt0TKZPesM0Ng+1uGK2t3VU52VCGzgPamUG680so5kzU+lhp1CGN2yvrEAKehWTanBp11nZcXVpbuzcH1f27UP5Tcw8aAJBs+hjKVXMbdQ7B2w+SoKWNqkNbMpkMPXr0aJZzUePVdp1XUFBQY4VJXeljUQKqMGvWLAwdOhQLFy6st6yh2728vBzu7u6Ijo6uMel/a1H154Ofe4bRGtpdsX4RcDOzxnZp1CEo3nwJKCpU2y559zMo35pV8aRzNyDvntoxyi93Q3X0u0d11HKtKNn+vxBquZlan9bQ7s1NpVRCOe85zTsdXCBd9R4A1HkdLggCzL7/An9+87/iNknEQQgGmDtNl+s83s4lIiLjUHlHvqTYsHGQ/tV3PSrVwxpJOq7kR0TGZejQoZg0aZKhw9BKXl4e5syZg4EDBxo6FCLjpiGZJtL0sZ1//9HjdhoW7dBmPi4AaK+fuXep8QQr7UeDVJ8P1xDJNF1xlU8iIjIKqp+ad8EAybyQZj0f1UEfK3RyDjWiFk2bnmnGonv37li8eLGhwyBq4TR89ledeF7T6u7afNbL7SHwmsB4mFtoX7YF3hvl/zQiIjIKqp8Tm7Dyml2khCFPNd35qJo6uqgNGqaf+c84hxoREVHL8defNbd1rTLMTtNwS20+69k7zagIo57XoXDLu5ZreRETEVHrVM/KTo3COTCMluSpkXq5I6nKvtn4SoiIiEh/pFoO0axkUs8AOm16tLN3mnHRJcHZAqfv4P82IiIyDprmytAbJtQMqq6EpiCBXjJq135rfB1ERESkPzomSASZqZiAEVzc6qxPlXlVL+ekJlZLElSYvUTDxpb33jGhRkRExqG3a9PVzXya8RKEFnkBRURERPWp4/PdRfN1n+Rf70N44RUIU4MhWbCiWnWP0heqmxma6y0u0jVIakq1XONJfPw1FG1514NMqBERkXHQNPms/ipvwrqpUQTxHyIiImpNystq39euvcbNQo+ekIx5AUL7DoDDExUbK4dxVk24XE/XXO/vtWynZiF5axMEv7GPNugwL1pZXavCGikm1IiIyDgoG55QU5XVccFGRk6oM58mTA1uvlCIiIjIeFReHyiVUP1xQz2hpuv8bNR0qr4v7doDnbtp3lePoqRjj560kMUlmFAjIiLj0JiE2r6d9RRgDzVjpbpV+91IadQhSEZO1KoeYcQYfYVERERE+iCtZ5GBej1KxijXvg7VpV/0WDc1ier5s4YO47S1b3QozYEJNSIiMg6/nm3woapTx+op0OCqSR/qSmgW/qmfOdQe69T4OoiMXGBgINasWVPrfi8vL0RFRdVbT3R0NIYMGQJbW1tERUVh27ZteOaZZ/QZKjWDpKQkyOVy5OfnGzoUIs0GDq25rZ8HAEDo2r3+46tPaJ+e+ugxE2rGq8p1X4PnRcvP008sTYz/C4mIqA1QT+gITwYYKA6qISdbPwk19kIkQmxsLDp06CA+l8vl2L17N8aMedSD8+HDh1i9ejXWrl2LcePGoVOnTlAqlXj11VcNEbJGS5YswYkTJ3D79u1G1RMeHo6pU6fi559/RmRkJM6dO4fi4mLY2dnB398fc+fOhbW1NZKSkhAVFYXz58/j4cOHcHBwwIIFCzB58mStzuPt7Y2tW7ciOzsbYWFhSE1Nrf+gKmJiYhp0HJGxE2zsofolSW2bZNLLFfv6DoTq/+Lqq6H2XRzyaaQE/VyT5fzR+DqaARNqRETU+qlUUBX9JT4VRmv3RxI1PVVZKYT6FiWw7QXcul53mY7m+gqJqMXq1q1bvWWysrJQVlaGgIAAWFlZids7djSO+WoUCgUSExPx8ccfw8nJSdweHByMPn36YPny5TAxMUFJSQkAQPr3H9WRkZE4fvw4vvjiC/EYc3Nz7N+/H6tWrcKUKVMQFRUFOzs7ZGVl4eDBg9i1axfCwsJw9uxZ9O3bFwsXLkSPHj2QmJiIRYsWwdzcvN6ee1euXEF+fj58fHzw9ddfN0GLELVgmhIrDi7aH1/XDTcm1IxXbQk1KzlwJwt43LF542lCHPJJREStX1kpVN9/+eh5LStLUfMTHJ8AzOp5P7S501l9WAiRDlQqFcrKygzypdLxTr5CoUBoaCj69OkDNzc3bN26Vayj6pBPLy8vAEBQUBDkcjm8vLwQExODgICKHro+Pj6Qy+W4efNmjSGfixcvxuzZsxEZGQkPDw+4urpi1apVKKuyAExJSQnWr1+PwYMHw9nZGRMmTEBSUkVPlOLiYvj7+yMkJEQsf/36dbi4uKglvDQ5e/YsZDIZvL29YWlpKX6ZmprCzMwMlpaWsLKygrW1NaytrcX9HTt2hFQqVTvmwYMHWLNmDWbPno3w8HAMGzYMdnZ28Pb2xnvvvYclS5YAAN58802EhITA09MTvXr1QnBwMPz8/HDkyJF634/4+Hj4+fnhzJkzWLp0KQoKCiCXyyGXy7Ft2zYAQF5eHt58803069cPTk5OmDFjBjIyMgBUDNus7biDBw9i7NixcHFxwcCBA/Haa68hNze33piIjIa0WrpBkOg2BLCusgUc6myUBAH4s0DjLsmSdRBGT4bktdWaD23fQeN2Y8YeakRE1OqpTh2H4DXi0QZ9DDEk/ZBIIEj0cX+P7yk1XHl5OT766CODnHvBggWQyWRalz9w4ACmTZuGw4cP4+LFiwgJCYFcLsf06dPVysXGxmLAgAEIDw+Hv78/pFIpOnToABsbG0ybNg3ff/89bGxsau3VlpSUBEtLSxw4cACZmZlYsGABXF1dxfOsXr0aV69eRUREBKysrBAXF4cZM2YgMTERjo6O2LFjB5599lkEBARg5MiReOONN+Dr64tp06aJ55DL5eKwzEoJCQkYOXJkw+fdqeLw4cMoLS3FwoULNe63sLCo9diHDx+id+/e4vOkpCRMmTIFp06dgp2dnbj96NGjmDt3LoYMGYJ169bhvffew4kTJwA86vW3ZMkSZGZmYs+ePXjsscewadMm/OMf/8Dx48frPK68vBxvvfUWnJyckJubi3Xr1mHJkiXYv39/4xqGqLkI1T7f1W5+aXEzoc7fA5qPF+aGaNxOzUd17HuN24VulhACZ9V6XKfAV5C/3zCfxQ3FhBoREbUYQsCzUP3w7wYeXOWijgm15lVXDxxtkmna9ODh0A9qI2xsbLBu3ToIggBnZ2f89ttviIqKqpFQq0yUWVhYwNLSUtzepUsXcX/V7dVZWFhg48aNkEqlcHZ2RkBAAE6ePInp06cjKysLMTExOH36NHr27AkAmD9/Po4dO4aYmBisXLkSbm5uCAkJwfLlyzFp0iRkZWVh7969audwcnJCp07qC4rEx8cjLCyswe1TVWZmJszNzdWGtmrj0KFDuHDhArZs2SJua9++PZycnGBi8ujPp+zsbKSmpsLf3x+mpqYwNzeHIAhq7ZqRkYGEhAR8++238PT0BADs2LEDnp6eiIuLw7PPPqvxOABqyUd7e3ts2LAB48aNQ2FhodEM0SWqU/XrLZ2vv3S/XhPYY92wGtH8JakX9BdHM2FCjYiIWgzh2WkNT6iplFUq4sWW0ZBokQhjQo2amImJCRYsWKDzcTKZTG0YZEPPrYtBgwap9d4aPHgwdu3aBYVC0ag4qnNxcRHnJwMAKysrcdL81NRUKBQKDB8+XO2Y0tJSMWEHAPPmzUNcXBz27NmD6OhodO3aVa18ZY+sSmlpabhz5w6eeuopvbwGlUqlc0+3n3/+GUuXLsXWrVvxxBNPiNs9PDxqxJuQkABPT886e7qlp6fDxMQEgwYNErd17doVTk5OSE9PrzOWixcvYtu2beI8bUplxedYVlYWXFx0mIeKyEBUP/+gvkHXHul1/vzWsq+LFquHklEqSb1o6BB0xoQaERG1IA1PhKn+8396jIN0U0dCTF+JMBPth8wRVScIgk7DLis15JiWQtNrq5yrrbCwEFKpFEeOHFFLugHqixvk5uYiIyMDUqkUmZmZ8Pf3r/OcCQkJ8PX1hZmZmR5eAeDo6IiCggLcuXNHq15qycnJmDVrFsLCwjBlypR6yx89ehSjRo3SR6g1/PXXX3j55Zfh5+eHnTt3olu3bsjKysLLL7+M0tLSJjknkd5VX6mx6mgBbW6W1ZVQq6UnmuD4hMbt1HT0MUQfAFSFf+qlnubERQmIiKjlKKv7jwiVSgXVtd+gKi6qux5FuR6DokapPr+KpiLPTKq/DO9IUxuRkpKi9vzcuXNwcHCokdgCKpJi+u65BgBubm5QKBS4d+8eHBwc1L6qDltctmwZ+vTpg+3bt2Pjxo1IS0urs974+Hi9JqjGjx8PU1NTREREaNyfn/9oUvOkpCTMnDkToaGhmDFjRr11FxYWIikpCaNHjxa3mZqa1mhvZ2dnlJeX49y5c+K2+/fv49q1a+IcbZqOS09Px4MHD7By5Up4eXnB2dmZCxJQy6dr4qXO4hxtQIbHhBoREbUc7eueM0aVfAzKzSFQbl1Rdz0dO9W9n/RMi2Xv3YfWfvRTz0C6PgLtn3xafYfN448e9xnQiPiIWo6srCyEhYUhPT0d3377LT799FMEBQVpLGtra4uTJ08iJycHeXl5eovByckJkydPxqJFixAbG4sbN24gJSUFO3bsQGJiIgDgs88+wy+//ILt27dj8uTJGDNmDF5//XW13lW+vr7iSpq5ubm4ePGi2mqjjSWXy7F27Vrs3r0by5YtQ3JyMm7duoUzZ84gJCQE27dvB1AxzHPmzJmYPXs2xo0bh5ycHOTk5ODBgwdiXSkpKfD19UV2djYA4NixY3B0dFRboMDW1haFhYX46aefcP/+fRQVFcHR0RGjR49GSEgITp8+jcuXL+PNN99Ez549xWScpuPkcjlMTU2xZ88e/P7770hISBDjJWqxlFWm3+jUuf7ydd10qzqVBxmXzl3rL9NKMKFGREQGo8r6HYo5E6FM/E67A0xNHz2W1py1QJX8Y8WDm5na10PNoP5FCSQvzq61iCAIEGzsIFQb1ilZsh7ClFch2f6/ehtuQGTsAgMDUVxcjAkTJiA0NBRBQUG19qhas2YNTpw4AU9PT7WeVPoQHh6OwMBArF+/Hr6+vggKCsKFCxcgl8uRnp6ODRs2YNOmTZDL5QCATZs24f79+3j33XfFOq5du4aCggIAFcMnBw4cWGOetcaaNWsWPv/8c9y+fRvBwcEYMWIEli9fDnNzc8yfPx9AxcqpRUVF2LlzJzw8PMSvOXPmiPUUFRXh2rVrKC+v6OEcHx9fI/nn6emJf/zjH1iwYAH69+8v9owLDw9H//798corr2DixIlQqVTYv3+/OKxW03HdunXD+++/j8OHD8Pf3x87d+7Ev/71L722DVGzKy159Li3qxYH1P7Zrvq/uMbHQ02gbV2PCSqVNoOXW7e7d+82ekJZTQRBgLW1NbKzs8Fmbh5sc8NguxtGa2h3xZyJGrdLow5p3Ke2fZAPcC5Zfd/7a4Ar5+uso3JfQ7WGdm9uqpw/oAydr3Gf8OoiSIYFQJV7B8qVc9T2VX2fBEGAaXQE/jp+ROP+5iSTydCjRw+DnJt0V9t1XkFBQY0VJnWlj0UJqMKsWbMwdOhQLFy4sN6yhm738vJyuLu7Izo6Gh4eHgaLoylV/fng555htIZ2r+1arrb91T/XVaUlUL5W/3yGddWhq9bQ7oZQ+V5K1v4PlB+sA/LuAdD+/RAEAeXBz6ptawnXeeyhRkRErYc2K0ZS86vzevTvO5narPzVtm56ErUpQ4cOxaRJ9c+XaAzy8vIwZ84cDBw40NChELUemnqaazHPKhkZQUA9F361knSqfcVkY8X/oURE1HpoM+xvoFfTx0FaEzp0qHigTTJUm6QbEbVICxcuFIeHGrvu3btj8eLFHGpOpEfCUxrmT+SPWMv0uFODDrPa8f/0HEjT45UpERG1TJq64WuTcGEvtuZX15CJAX8vRqBhTryaeGVNRETUGgnT5mja2uxxUGMJEKxtG3SkSfdHq0TDomUsbMCEGhERtUyakjRVegvUNu+FwF5ORkV8P6T1JzoFCS+siYiIWiPBtF3Njbp+7nPFb8OT6uk628FFP/U0Mf5VQURELZJgbadhY5ULr4tnNR/IHmrGqVoPNcmCFRoKMaFGRETUdmj5uS+VAn3dIZmzvGnDoVoJIydB8PYHrPQzdF9w7qOXepqaNuMriIiIjIZk1XtQnUuGMH4qUPQXVMdjH+2s2kPt1zO1VMB7Sc1Pi8lpTdQvSYRBw2qW4XtHRETUdmg7T6H7UEgXrGzaWKhOkqlB4mN9rI2q+r84YPRkPdTUtHhlSkRELYrg4ALJC69AaNfu0QS2nbv9vbNKQu3szxqPV5061tQhUh0kaz8Aeta8eyloMeRTcfdOU4RERERERkjbhT8Eiy5NHAk1u6K/DB2BVphQIyKilku8zqq4FyZUXV69rLTZwyEtWNlCsHNs0KHF55IfPenQUU8BERERUUsmTAmqvxA1Iz1M0WFu0fg6mgETakREZFSEJwN0KV3xLe8+FBuXQVVS/GiXVqtGUrMTUPeqn9pW03dgo+sgIiKiFs7OAYJMZugoSM+Ep8cbOgStMKFGRETGpVNn7ctWHQpwPQ34tcpCBCZMqBmN6vkzPSTUtJ5XhagVCQwMxJo1a2rd7+XlhaioqHrriY6OxpAhQ2Bra4uoqChs27YNzzzzjD5DpWaWlJQEuVyO/Px8Q4dC1Lxybhs6AmoKMlNDR6AV/rVBRESG09cdSL2gtkkYMrzi+9ARUJ3+v7qPryunwh5qRkqASg8JNdXZk8C8ED3EQ9R6xMbGokOHDuJzuVyO3bt3Y8yYMeK2hw8fYvXq1Vi7di3GjRuHTp06QalU4tVXXzVEyBotWbIEJ06cwO3bjftDOTw8HFOnTsXPP/+MyMhInDt3DsXFxbCzs4O/vz/mzp0La2trJCUlISoqCufPn8fDhw/h4OCABQsWYPJk7SbE9vb2xtatW+Hr69uoeLUVGBiIfv36Yf369c1yPiKjJuENNjIc/rVBRESGc+ePmttse1V879q9/uOFOjpaazHJPRlKPQm1bpbNEwZRK9OtW7d6y2RlZaGsrAwBAQGwsrISt3fsaBzzEioUCiQmJuLjjz+Gk5OTuD04OBh9+vTB8uXLYWJigpKSEgCA9O/f9ZGRkTh+/Di++OIL8Rhzc3Ps378fq1atwpQpUxAVFQU7OztkZWXh4MGD2LVrF8LCwnD27Fn07dsXCxcuRI8ePZCYmIhFixbB3Ny83p57V65cQX5+Pnx8fJqgNdSVlpbC1LRl9NogajYtZPL6NkUvowhaRqKUQz6JiMhwJNU+hiy6QKi+rU51fNiyh5oRqZZAUyo1lhKemQQAkEyZ3dQBEalTqQBlaQO+Shp4XJUvHXtsKhQKhIaGok+fPnBzc8PWrVvFXp9Vh3x6eXkBAIKCgiCXy+Hl5YWYmBgEBFTMU+nj4wO5XI6bN2/WGPK5ePFizJ49G5GRkfDw8ICrqytWrVqFsrIysUxJSQnWr1+PwYMHw9nZGRMmTEBSUhIAoLi4GP7+/ggJedSL9Pr163BxcVFLeGly9uxZyGQyeHt7w9LSUvwyNTWFmZkZLC0tYWVlBWtra1hbW4v7O3bsCKlUqnbMgwcPsGbNGsyePRvh4eEYNmwY7Ozs4O3tjffeew9LliwBALz55psICQmBp6cnevXqheDgYPj5+eHIkSP1vh/x8fHw8/ODiYkJ+vfvj8OHD4v7nnnmGXh4eIjPT58+DQcHBxQVFQGoSG6++uqr6N27N5544gnMmzcPd+/eFctXvi+ff/45vL294ejoiMWLFyM5ORm7d++GXC4X38NKFy9exNixY+Hk5ISJEyciPT293tdARGR0WkY+jT3UiIjIgCTVepHpekerruK1zKEmWcIhMgYlCMD5/2jeNWU2hPEvQuhornG/aZ/+KP3t16aMjtoqVRksM9Ya5NQ5jusAQfteRwcOHMC0adNw+PBhXLx4ESEhIZDL5Zg+fbpaudjYWAwYMADh4eHw9/eHVCpFhw4dYGNjg2nTpuH777+HjY1Nrb3akpKSYGlpiQMHDiAzMxMLFiyAq6ureJ7Vq1fj6tWriIiIgJWVFeLi4jBjxgwkJibC0dERO3bswLPPPouAgACMHDkSb7zxBnx9fTFt2jTxHHK5XByWWSkhIQEjR46EoIceDocPH0ZpaSkWLlyocb+FRe2ryD18+BC9e/cWnyclJWHKlCk4deoU7OzsxO1Hjx7F3LlzIQgCvL29kZycjAkTJiAvLw/p6ekwMzNDeno6nJ2dkZycDHd3d7Rv314cZtuxY0d89dVXKC8vR2hoKBYsWICDBw+K9V+/fh2xsbH45JNPIJFIYGtri4yMDLG3HlDRM7EyqbZlyxasWbMG3bp1w4oVK7Bs2TJ89913jWpHIiKdNOLXt+A5HKqrlyAMavpev/rAhBoRERlO9WGZOvVOQ90JuNoWJejRU7dzUONp2QNHEASglmQaAAjsdUgEGxsbrFu3DoIgwNnZGb/99huioqJqJNQqE2UWFhawtHw0jLpLly7i/qrbq7OwsMDGjRshlUrh7OyMgIAAnDx5EtOnT0dWVhZiYmJw+vRp9OxZ8Tt1/vz5OHbsGGJiYrBy5Uq4ubkhJCQEy5cvx6RJk5CVlYW9e/eqncPJyQmdOnVS2xYfH4+wsLAGt09VmZmZMDc3Vxvaqo1Dhw7hwoUL2LJli7itffv2cHJygkmVz5bs7GykpqbC398fQEWvv+joaADAf/7zH7i6usLS0hJJSUliQs3b2xsAcPLkSfz2229ITk6GXC4HAHzwwQfw9/fH+fPnMXDgQABAWVkZPvjgA7XEZ9XeetX985//FIefvvbaa5g5cyaKi4thZmamUxsQETWU8MQAqI581aBjpfNCoFSUQ6h+091I8cqUiIgMp3rSq6450TSpK6FWW/JFodDtHKRfjbhrWXI5RX9xEFUlyCp6iulIJjNBWVl5o8+ti0GDBqn13ho8eDB27doFhZ5/t7m4uIjzkwGAlZUVUlNTAQCpqalQKBQYPny42jGlpaViwg4A5s2bh7i4OOzZswfR0dHo2rWrWvkTJ06oPU9LS8OdO3fw1FNP6eU1qFQqnXu6/fzzz1i6dCm2bt2KJ554Qtzu4eFRI96EhAR4enqKPd28vb2xZs0a3Lt3D8nJyRg2bBh69OiB5ORkvPTSSzh79qzYWy4tLQ02NjZiMg2oaHMLCwukpaWJCTW5XK7V3HiV+vXrJz6uTCTeu3dP7TxEzUGnBYjM2jddINTsBFePihEhPW0bdnwLSaYBTKgREZEh3cxUf171Dx+t/gZqQEKtU2dtKiaitkQQdBp2KZLIdO9Z20LIZDUTfZV/IBcWFkIqleLIkSNqSTdAfXGD3NxcZGRkQCqVIjMzU+zJVZuEhAT4+vrqrTeVo6MjCgoKcOfOHa16qSUnJ2PWrFkICwvDlClT6i1/9OhRjBo1Snzet29fdO7cGcnJyTh16hT++c9/okePHoiIiMD58+dRXl6OIUOG6PQaqq7aqg0TDb2zlbXMW0nUpHRJqGmbeOnaHbif27B4qFkJ/QYaOoRm0TqvAIiIqGXSeQ61uhJqmu9uCR2MYyW7tquFzDJLZKRSUtR7ap47dw4ODg41EltARVJM3z3XAMDNzQ0KhQL37t2Dg4OD2lfVYYjLli1Dnz59sH37dmzcuBFpaWl11hsfH6+WoGqs8ePHw9TUFBERERr35+fni4+TkpIwc+ZMhIaGYsaMGfXWXVhYiKSkJIwePVrcJggCvLy8EB8fj6tXr2Lo0KHo168fSktLER0djQEDBogJst69e+OPP/5AVlaWePzVq1eRn58PFxeXOs8tk8mYJKMWQPuEmuDUR6tyko0fQxj/ovpGFzddgiLSKybUiIjIeOg85LOOfbUk1IiIWrKsrCyEhYUhPT0d3377LT799FMEBQVpLGtra4uTJ08iJycHeXl5eovByckJkydPxqJFixAbG4sbN24gJSUFO3bsQGJiIgDgs88+wy+//ILt27dj8uTJGDNmDF5//XWUlpaK9fj6+ooraebm5uLixYtqq402llwux9q1a7F7924sW7YMycnJuHXrFs6cOYOQkBBs374dQMUwz5kzZ2L27NkYN24ccnJykJOTgwcPHoh1paSkwNfXF9nZ2QCAY8eOwdHRUW2BAqBiHrXvvvsO/fr1Q8eOHSGRSODl5YVvvvlGnNsMAIYPH44+ffrgjTfewK+//oqUlBQsWrQIPj4+cHd3r/N12dnZISUlBTdv3sT9+/eZXCPjpNRtBWNtCCYmECZMVd9mXvviIkRNjQk1IiIyHlV7nHWvNjxHbq+hfB0fYy1o/oVWT//X1ERtVmBgIIqLizFhwgSEhoYiKCio1h5Va9aswYkTJ+Dp6anWk0ofwsPDERgYiPXr18PX1xdBQUG4cOEC5HI50tPTsWHDBmzatEmcu2vTpk24f/8+3n33XbGOa9euoaCgAEDF8MmBAwfWmGetsWbNmoXPP/8ct2/fRnBwMEaMGIHly5fD3Nwc8+fPB1CxcmpRURF27twJDw8P8WvOnDliPUVFRbh27RrKyyvmzIuPj9eY/PP29oZCocCwYcPEbT4+PlAoFGoJNUEQsGfPHlhYWGDy5MmYNm0aHn/8cXz00Uf1vqZ58+ZBIpHAz88P/fv3V+vlRmQ0dBnyqQPBRH04ujB5ZpOch0gbgkqn2QJbp7t376KsrEzv9QqCAGtra2RnZ+s2KSM1GNvcMNjuhtEa2l0xZ2KNbdKoQwAAVXkZlAteeLRDbg9p2A61sqq7t6FcNVenc1bW31Ctod2bmyrrBpRhrwMAJB9/B+XcSeI+bd8PQRBQHvys2rbGvpcNJZPJ0KNHD4Ocm3RX23VeQUFBjRUmdSWTyZrkGrItmjVrFoYOHSpO2l8XQ7d7eXk53N3dER0dDQ8PD4PF0ZSq/nzwc88wWnq7q8pKoVwYWGN71c/uyutAIeBZSKbNqVG2apmqx1bdJtn5JYR2+lvFtqW3e0tlTO2uy3Uee6gREZFx0maib13nXCMD4QUpEdVt6NChmDRpUv0FjUBeXh7mzJkjrsRJRBpoGvJp2k73eupNlvFakAyHq3wSEZGRqnaBpGl4JxNqLY7A94yINNCmZ5qx6N69OxYvXmzoMIiMm0rT3H613GCrawV2jfVUoSgD0IBEHZEesIcaEREZhK7duQV7R01b9RMMNS09dd3vtuIdvdRDRERETU3DZ3+1TZIFKyH4PA1hZM0pQLRmqr/hnkS6Yg81IiIyDF0Tai9qXsWOWqC+7kDqBZ0PM/N8SnwsjJuiz4iIiIhIn7RY5VMY5ANhkE+95eqkzRQhRE2ECTUiIjKM+hJq1YYGCh0ea/w53QY3vg5qvAZf/Fb5P/H3SnukPwkJCUhISMDdu3cBALa2tggMDBQnXS8tLcW+ffuQlJSEsrIyuLu7Izg4GJ07dxbryM3NRVRUFC5fvgwzMzOMGDECL7/8MqTSR6vuXr58Gfv27cPNmzfRrVs3vPDCC/Dz81OLJS4uDv/+97+Rl5cHe3t7zJ49G87Ozk3eBkREpCearvOaYrJ5TiVBBsR0LhERGYYBVvCRBM5q9nMSgO5W+qmnyjWzMGCIfuokUdeuXfHyyy9j8+bNeOedd+Dm5oatW7fi5s2bAIC9e/fil19+wdKlS7Fu3To8ePAA27ZtE49XKpV45513UF5ejrfffhuvvfYajh8/jpiYGLFMTk4ONm/eDFdXV2zduhXjx49HZGQkzp8/L5ZJSkrCvn37EBgYiC1btsDe3h4bN25Efn5+s7UFERE1lr6u8+pOmHFuVjIkJtSIiMgw6kmoaXWBpOU1lGThKkje3QNBbq/dAaRXgll7SML3Q/LB542rp0rPNlXuncaGRdUMGTIEgwYNgrW1NWxsbPDSSy/BzMwMaWlp+Ouvv/Djjz/ilVdegZubGxwdHbFw4UL897//xdWrVwEAFy5cwK1bt/DGG2+gV69e8PDwwNSpUxEfH4/yv3sUJiQkwNLSEjNnzoStrS3GjBkDb29vfP/992Ichw8fRkBAAPz9/WFra4s5c+bA1NQUx44dM0i7EBFRA2gc8slVv6l14ZBPIiIykGa8qFIpIXTu1nznoxoEcwt91PLoYdbveqiPaqNUKpGcnIySkhK4uLggIyMDCoUC/fv3F8vI5XJ0794dV69ehYuLC65evYrHH39cbQjowIED8cknn+DmzZtwcHBAWlqaWh0A4O7ujs8++wwAUF5ejoyMDDz33HPifolEgv79+4uJu9qUlZWhrKxMfC4IAtq3by8+JiLdVP7cVP9OzaPFt7uGsCUTX27U69F0rL7bp8W3ewvVUtudCTUiIjIMLSar1Rs7TSuEksE09GKp6nG8yd0kbty4gdDQUJSVlcHMzAzLly+Hra0trl+/DhMTE3Ts2FGtvIWFBfLy8gAAeXl5asm0yv2V+yq/V26rWqaoqAilpaX4888/oVQqa9TTuXNn/PHHH3XG/s033+DgwYPicwcHB2zZsgU9evTQWL6oqAgymazOOrWhjzpId2z3pmVqagpra2u1bT179jRQNG1bS213hZkpqv/Wtpn9hs4Jk1uCIH7kV/6fvFllf/X/p/rSUtu9pWtp7c6EGhERGYg+MiJaXJTZOUDo0bI+nKkWVd9uOweDhdGa2djY4N1338Vff/2FU6dO4cMPP8S6desMHZZWnn/+eUyYMEF8XvlH2927d8Uhp1WVlpaq9WhrCJlM1ug6dBEYGIh+/fph/fr1Gvd7eXkhODgYc+bMqbOe6OhobN++Hbdv38batWtRUFCAuLg4HD16tCnC1rvmbnddxMXFYcOGDbhx4wZeffXVWt8rY1daWors7GwAFT9LPXv2xO3bt6EywPynbVVLb3flifga227fvq1zPVVfeeX/SdSzrTFaeru3VMbU7iYmJrXejKtRtoljISIi0kylbHwdWtzlFHraNv48pGcN7c7/6DjBxk4/oZAaExMT8e6wo6Mjrl27htjYWAwbNgzl5eUoLCxU66WWn58v9ibr3Lkz0tPT1eqrXEigapnqiwvk5+ejffv2MDU1RadOnSCRSMQebZU09X6rTiaT1dprydAX580lNjYWHTp0EJ/L5XLs3r0bY8aMEbc9fPgQq1evxtq1azFu3Dh06tQJSqUSr776qiFC1mjJkiU4ceJEg/74rio8PBxTp07Fzz//jMjISJw7dw7FxcWws7ODv78/5s6dC2tra6Snp2PFihVIS0vDw4cPYWVlheeeew5Lly7Vqiect7c3tm7dCl9fX/zzn//E1KlTMXv2bDz2mB5Wpzag6j83KpWqzfwsGZOW2u7KfTtrbGvY63j02a/p+KZqm5ba7i1dS2t3JtSIiMgwmumzUqVUNM+JSAcNfPOrJlBb2BwbLZVSqURZWRkcHR0hlUrx66+/wtvbGwDwxx9/IDc3Fy4uLgAAFxcXfP3118jPzxeHdV68eBHt27eHrW1FYrt3795ISUlRO8fFixfFOkxMTODo6IhLly5h6NChYgyXLl1SSwqRZt261T9XZFZWFsrKyhAQEAArq0cr8FYfzmsoCoUCiYmJ+Pjjj+Hk5CRuDw4ORp8+fbB8+XKYmJigpKQEACCVSgEAkZGROH78OL744gvxGHNzc+zfvx+rVq3ClClTEBUVBTs7O2RlZeHgwYPYtWsXwsLCIJPJMGXKFLi5ucHCwgJXrlzBW2+9BaVSiZUrV9YZ75UrV5Cfnw8fHx8UFhYiNzcXI0aMqHXYkkKhgCAIkEi4NhwRUUvH3+RERGQY+uihpgWhwb2hyOgwidakPv/8c1y5cgU5OTm4ceOG+Hz48OHo0KEDnn76aezbtw+XLl1CRkYGIiIi4OLiIibD3N3dYWtri507d+L69es4f/48vvjiC4wePVrs5TNq1Cjk5OQgOjoaWVlZiI+PR3JyMsaPHy/GMWHCBPzwww84fvw4bt26hU8++QQlJSXw8/NrsteuUqlQrizW/UvRgGOqfel6J16hUCA0NBR9+vSBm5sbtm7dKtbh5eWFqKgo8TEABAUFQS6Xw8vLCzExMQgICAAA+Pj4QC6X4+bNm9i2bRueeeYZ8RyLFy/G7NmzERkZCQ8PD7i6umLVqlVqwyxLSkqwfv16DB48GM7OzpgwYQKSkpIAAMXFxfD390dISIhY/vr163BxcVFLeGly9uxZyGQyeHt7w9LSUvwyNTWFmZkZLC0tYWVlBWtra1hbW4v7O3bsCKlUqnbMgwcPsGbNGsyePRvh4eEYNmwY7Ozs4O3tjffeew9LliwBANjb22Pq1KlwdXWFra0tRo0aheeffx6nT5+u9/2Ij4+Hn58fzpw5I/4svPjii5DL5UhKSkJMTAz69u2LhIQE+Pn5wcHBAVlZWXW2X6XTp0/j+eefh5OTE4YMGYJ//etf+Ouvv+qNiai1EyZMrXhg72zYQKjNYw81IiIyjObqofbfi81zItJBwxJj6hMZM7mmb/n5+fjwww/x4MEDdOjQAfb29ggNDcWAAQMAAK+88goEQcC2bdtQXl4Od3d3BAcHi8dLJBKsWLECn3zyCVavXo127dphxIgRmDp1qljG0tISK1aswN69exEbG4tu3bph/vz5GDhwoFhm2LBhKCgowJdffom8vDz06tULq1atqnfIZ2MoVCX4KrXueceaygt9o2AimGld/sCBA5g2bRoOHz6MixcvIiQkBHK5HNOnT1crFxsbiwEDBiA8PBz+/v6QSqXo0KEDbGxsMG3aNHz//fewsbGptVdbUlISLC0tceDAAWRmZmLBggVwdXUVz7N69WpcvXoVERERsLKyQlxcHGbMmIHExEQ4Ojpix44dePbZZxEQEICRI0fijTfegK+vL6ZNmyaeQy6Xi8MyKyUkJGDkyJF6Went8OHDKC0txcKFCzXur75ARqXMzEwcP34cY8eOVWuPKVOm4NSpU7CzezTk/OjRo5g7dy6GDBmCEydOwNfXF1FRURgyZAg6d+6MmzdvoqioCB9++CHeffdddOnSBd27d6+3/a5fv47p06cjJCQE27Ztw71797B69WqEhobi/fffb3TbEDUl1a3MJq1fmDANQm9XwPGJJj0PUX2YUCMiIsPQyxxqWpT582Hjz0PGh/k0vVuwYEGd+01NTREcHKyWRKuuR48e9Q6Rc3V1xdatW+ssM2bMGA7xrIWNjQ3WrVsHQRDg7OyM3377DVFRUTUSapWJMgsLC1haWorbu3TpIu6vur06CwsLbNy4EVKpFM7OzggICMDJkycxffp0ZGVlISYmBqdPnxaHNs6fPx/Hjh1DTEwMVq5cCTc3N4SEhGD58uWYNGkSsrKysHfvXrVzODk5oVOnTmrb4uPjERYW1uD2qSozMxPm5uZqQ1vrMnHiRFy6dAklJSWYPn063nrrLXFf+/bt4eTkBBOTR38+ZWdnIzU1Ff7+/jA1NUX37t0BVMwVWLVty8rKsGnTJri6ugKAVu23c+dOPP/88+ICE46OjtiwYQNeeOEFvPPOOzAz0z4JS9TclOsWNWn9glQK9BvYpOcg0gYTakREZBj66KHWvmVP+NxmNaLniTDAE6r8B4BtL/3FQ22eVGiHF/pG6XyczESGsvLGrTYpFdrpVH7QoEFqvbcGDx6MXbt2QaHQ73yRLi4u4vxkAGBlZYXU1FQAQGpqKhQKBYYPH652TGlpqZiwA4B58+YhLi4Oe/bsQXR0NLp27apW/sSJE2rP09LScOfOHTz11FN6eQ0qlUqnnm4fffQRCgsLceXKFWzYsAGRkZFi7zYPD48a8SYkJMDT07PWnm6VTE1N0a9fP/G5Nu135coVpKam4ptvvlF7PUqlEjdv3kTv3r21fl1EzUmVnqrfCnkDjYwYE2pERGQYOvRQE7z9NW9vp9sfomQcBNdBUP16FjDV/f2TvPEvnf9IJqqPIAg6DbusZCKVQaWU1l+wBdK0umXlXG2FhYWQSqU4cuSIWtINUF/cIDc3FxkZGZBKpcjMzIS/v+bf5ZUSEhLg6+urt95Xjo6OKCgowJ07d7TqpSaXywFUJBMVCgVCQkIwb968Gq+x0tGjRzFq1Kh66zUzM1P7naVN+xUWFmLGjBmYPXt2rXESGSPlkYP6rZCf92TEmFAjIiID0aGLGoe2tCqC31igU2cIzn11P5YX1tTGVV8l9dy5c3BwcNCY9JHJZHrvuQYAbm5uUCgUuHfvnrj4gSbLli1Dnz598NJLL+Gtt97C8OHD6+xZFR8fX2PoamOMHz8e77zzDiIiIrBu3boa+6uuSFudUqlEeXk5lEqlxrYtLCxEUlIS3nnnHZ3j0qb9+vfvj6tXr8LBwUHn+okMqlC/U20Ig3yg+vkHQG6v13qJ9IGrfBIRkWEodUmodWj4ebp0b/ix1CQEqRQSz6cgdNE8GToR1S4rKwthYWFIT0/Ht99+i08//RRBQUEay9ra2uLkyZPIyclBXl6e3mJwcnLC5MmTsWjRIsTGxuLGjRtISUnBjh07kJiYCAD47LPP8Msvv2D79u2YPHkyxowZg9dffx2lpaViPb6+vjhy5AiAit5sFy9eVFtttLHkcjnWrl2L3bt3Y9myZUhOTsatW7dw5swZhISEYPv27QCAr7/+GocOHUJaWhp+//13HDp0CJs3b8bEiRPFnnopKSnw9fVFdnY2AODYsWNwdHRUW6BAW9q038KFC3H27FmEhoaKK+vGx8cjNDRUP41D1FRualiQYJAPJK+talB1wrS5EGa+DsnSDY0MjEj/2EONiIgMRIeEmqW1VsUk70RB+V4ocC9H3CY8GaBrYERERiswMBDFxcWYMGECpFIpgoKCMGPGDI1l16xZg3Xr1uHzzz9Hz5498Z///EdvcYSHh+ODDz7A+vXrcfv2bXTt2hWDBg3CyJEjkZ6ejg0bNmDbtm3i8MRNmzZh5MiRePfdd8Wk0LVr11BQUACgYvjkwIEDa8yz1lizZs2Co6Mjdu3aheDgYBQXF8PW1hYjR47E3LlzAQBSqRQRERHIyMiASqWCra0tZs2aJS4IAABFRUW4du0aysvLAVT0pmtM8q+u9gOAfv364auvvsKWLVswefJkqFQq2NvbY+LEiY1oDaJmUFqi9lQY6gvJnOUNrk4waw9heP1Dq4kMQVBVTobQht29exdlZY2bUFYTQRBgbW2N7OxssJmbB9vcMNjuhtHS2111PxfKf9acG0YadUh8rJhT8YeDMPN1SGq5mKosU3ms6t5dKFc86q0hTJ4JydhAfYXd4tu9pTKmdpfJZOjRo4dBYyDt1XadV1BQUGOFSV3JZLImuYZsi2bNmoWhQ4eKiwDUxdDtXl5eDnd3d0RHR8PDw8NgcTSlqj8fxvT7ty1pqe1e9boMACTvfgahs34T5U2ppbZ7S2dM7a7LdR6HfBIRkYE0zYel0E39A1AYMLRJzkNERPozdOhQTJo0ydBhaCUvLw9z5szBwIEDDR0KkdFrSck0Il1xyCcRERlGM919EuSPN8t5iIio4bTpmWYsunfvjsWLFxs6DCIiMjD2UCMiIsPQlFBzcGn+OIiIiIiIiHTEhBoRERkG56UgIiIiahVU1RYjIGoLOOSTiIgMQ4eEmuDUpwkDISIiIqKGUiZ8A9WBPRCqLwLVzdIwARE1EybUiIjIMLRIqEm27QXy8yDYcB40IiIiImOkOrCn4vuRg2rbBW8/A0RD1HyYUCMiIsPQIqEmdOoCdOrSDMEQERERkV5JOMMUtW46JdQSEhKQkJCAu3fvAgBsbW0RGBgIDw8PAEBpaSn27duHpKQklJWVwd3dHcHBwejcubNYR25uLqKionD58mWYmZlhxIgRePnllyGVSsUyly9fxr59+3Dz5k1069YNL7zwAvz8/NRiiYuLw7///W/k5eXB3t4es2fPhrOzcwObgYiImp2mhBrnVSMiIiJqFQTnvoYOgahJ6ZQy7tq1K15++WVs3rwZ77zzDtzc3LB161bcvHkTALB371788ssvWLp0KdatW4cHDx5g27Zt4vFKpRLvvPMOysvL8fbbb+O1117D8ePHERMTI5bJycnB5s2b4erqiq1bt2L8+PGIjIzE+fPnxTJJSUnYt28fAgMDsWXLFtjb22Pjxo3Iz89vZHMQEVHzYfKMiIiIqLUS+nkYOgSiJqVTQm3IkCEYNGgQrK2tYWNjg5deeglmZmZIS0vDX3/9hR9//BGvvPIK3Nzc4OjoiIULF+K///0vrl69CgC4cOECbt26hTfeeAO9evWCh4cHpk6divj4eJSXlwOo6AVnaWmJmTNnwtbWFmPGjIG3tze+//57MY7Dhw8jICAA/v7+sLW1xZw5c2Bqaopjx47psWmIiKhJKdlDjYhIF4GBgVizZk2t+728vBAVFVVvPdHR0RgyZAhsbW0RFRWFbdu24ZlnntFnqG1STEwM+vbVX48cfdcHVHRMkMvl7IhARKQHDZ5DTalUIjk5GSUlJXBxcUFGRgYUCgX69+8vlpHL5ejevTuuXr0KFxcXXL16FY8//rjaENCBAwfik08+wc2bN+Hg4IC0tDS1OgDA3d0dn332GQCgvLwcGRkZeO6558T9EokE/fv3FxN3tSkrK0NZWZn4XBAEtG/fXnysb5V1NkXdpBnb3DDY7obR4ttdQ9iSCVMb9Xo0Havv9mnx7d5Csd2J6hcbG4sOHTqIz+VyOXbv3o0xY8aI2x4+fIjVq1dj7dq1GDduHDp16gSlUolXX33VECFrtGTJEpw4cQK3b99uVD3h4eGYOnUqfv75Z0RGRuLcuXMoLi6GnZ0d/P39MXfuXFhbWyM9PR0rVqxAWloaHj58CCsrKzz33HNYunQpZDJZvefx9vbG1q1bGxUrUavDz2tqA3ROqN24cQOhoaEoKyuDmZkZli9fDltbW1y/fh0mJibo2LGjWnkLCwvk5eUBAPLy8tSSaZX7K/dVfq/cVrVMUVERSktL8eeff0KpVNaop3Pnzvjjjz/qjP2bb77BwYOPVh5xcHDAli1b0KNHDy1ffcP07NmzSeunmtjmhsF2N4yW2u6lf+XjTrVt8nHP61zPzSqPra2ta92mby213Vs6tjtR7bp161ZvmaysLJSVlSEgIABWVlbi9urX8IaiUCiQmJiIjz/+GE5OTuL24OBg9OnTB8uXL4eJiQlKSkoAQJyHOTIyEsePH8cXX3whHmNubo79+/dj1apVmDJlCqKiomBnZ4esrCwcPHgQu3btQlhYGGQyGaZMmQI3NzdYWFjgypUreOutt6BUKrFy5co6471y5Qry8/Ph4+ODr7/+uglahKiF4qJS1AbonFCzsbHBu+++i7/++gunTp3Chx9+iHXr1jVFbHr3/PPPY8KECeLzyrvcd+/eFYec6pMgCOjZsydu374NFYcxNQu2uWGw3Q2jpbe74su9NbZlZ2c3qk5Nxze2zupaeru3VMbU7iYmJk1+M46amUoFQdMw9PpIFBAUysadWiLo1JNDoVAgNDQUX331FUxMTDBz5ky89dZbEAQBXl5eCA4Oxpw5c+Dl5QUACAoKAlCxmNjSpUuxdOlSAICPjw8A4NSpU/jyyy8RFxeHo0ePAgAWL16MgoICDB06FLt27UJpaSkmTZqEdevWiT22SkpKsGXLFnz33XfIz89Hnz59sGrVKgwbNgzFxcUYO3YsPD09xZ5b169fx6hRo7B+/XpMmzat1td39uxZyGQyeHt7q/VINTU1hZmZGSwtLSGTydRGnQAVCUGpVApLS0tx2x9//IE1a9Zg9uzZan+v2NnZwdvbWxz2aG9vD3t7e3G/ra0tkpKScPr06Xrfj/j4ePj5+dXak23v3r3YtWsX/vjjD9jZ2WHRokUIDAwU9+fn52Pjxo2Ij4/Hw4cP0atXL6xcuVLjENx79+5hxowZsLGxQUREBGQyGT788EP87//+L+7evQsHBwcsXrxY7e+dH374AWvXrkV2djY8PDwwZcqUel8TkT5IVocbOgSiJqdzQs3ExES8O+zo6Ihr164hNjYWw4YNQ3l5OQoLC9XucOXn54u9yTp37oz09HS1+io/yKqWqT6mPz8/H+3bt4epqSk6deoEiUQi9mirpKn3W3UymazWD7umvDhXqVQGv/hva9jmhsF2N4yW2u6qk0drbmvk69B0fFO1TUtt95aO7U5NQVCqYP3rZYOcO7u/K1RS7RNqBw4cwLRp03D48GFcvHgRISEhkMvlmD59ulq52NhYDBgwAOHh4fD394dUKkWHDh1gY2ODadOm4fvvv4eNjU2tvdqSkpJgaWmJAwcOIDMzEwsWLICrq6t4ntWrV+Pq1auIiIiAlZUV4uLiMGPGDCQmJsLR0RE7duzAs88+i4CAAIwcORJvvPEGfH191ZJpcrlcHJZZKSEhASNHjtTL8O7Dhw+jtLQUCxcu1Li/+qiYSpmZmTh+/DjGjh2r1h5TpkzBqVOnYGdnJ24/evQo5s6dq7GeI0eOYO3atQgLC8Pw4cORmJiIpUuXwtraGk8++SSUSiVmzJiBwsJC7NixA/b29rh69arY666qrKwsvPTSSxg0aBC2bdsGqVSKDz74AF9//TU2b94MBwcHnDp1Cm+++Sa6desGHx8fZGVlYc6cOXjllVcwffp0XLx4EevXr9elCYkapnNXCJ27GjoKoibX4DnUKimVSpSVlcHR0RFSqRS//vorvL29AVTcFcrNzYWLiwsAwMXFBV9//TXy8/PFD7CLFy+iffv2sLW1BQD07t0bKSkpaue4ePGiWIeJiQkcHR1x6dIlDB06VIzh0qVLavNDEBERERG1NjY2Nli3bh0EQYCzszN+++03REVF1UioVSbKLCws1HptdenSRdxfdXt1FhYW2LhxI6RSKZydnREQEICTJ09i+vTpyMrKQkxMDE6fPi3eaJ8/fz6OHTuGmJgYrFy5Em5ubggJCcHy5csxadIkZGVlYe9e9Z7JTk5O6NSpk9q2+Ph4hIWFNbh9qsrMzIS5ubna0Na6TJw4EZcuXUJJSQmmT5+Ot956S9zXvn17ODk5wcTk0Z9P2dnZSE1Nhb+/v8b6IiMj8eKLL2LWrFkAKl7vuXPnEBkZiSeffBI//fQTzp8/j+PHj4vDW6v2lKuUnp6Ol156CWPHjhXf+5KSEuzYsQNffPEFhgwZIh575swZREdHw8fHB/v27YO9vT3Wrl0LAOL/lw8//FCr9iBqMGmj0wxELYJO/9M///xzDBw4EN27d0dxcTFOnjyJK1euIDQ0FB06dMDTTz+Nffv24bHHHkOHDh3w6aefwsXFRUyGubu7w9bWFjt37sT06dORl5eHL774AqNHjxZ7jo0aNQrx8fGIjo6Gv78/Ll26hOTkZKxYsUKMY8KECfjwww/h6OgIZ2dnxMbGoqSkBH5+fvprGSIiarksbYCcP4AOxjEnEBEZN5VEQHZ/V52PM5GZoLyscdOGqCS69cQaNGiQWu+twYMHY9euXVAoFI2KozoXFxe1nlJWVlZITU0FAKSmpkKhUGD48OFqx5SWlooJOwCYN28e4uLisGfPHkRHR6NrV/UeKydOnFB7npaWhjt37uCpp57Sy2tQqVQ69XT76KOPUFhYiCtXrmDDhg2IjIwUe7d5eHjUiDchIQGenp619nRLT0+vkej09PTE7t27AQCXL1+GtbW12lxx1RUXF2Py5Ml47rnn1HqXXb9+HUVFRXjppZfUypeVlcHNzU08v4eHh9r+wYMH19UERPrBhBq1ETr9T8/Pz8eHH36IBw8eoEOHDrC3t0doaCgGDBgAAHjllVcgCAK2bduG8vJyuLu7Izg4WDxeIpFgxYoV+OSTT7B69Wq0a9cOI0aMUOvmbWlpiRUrVmDv3r2IjY1Ft27dMH/+fAwcOFAsM2zYMBQUFODLL79EXl4eevXqhVWrVtU75JOIiIyDKu9ek9YvWbYBquOxEPzGNel5iKiVEASdhl2KpFKolI2bQ81YaZompXK4dWFhIaRSKY4cOVJjeGLVqV9yc3ORkZEBqVSKzMzMWntyVUpISICvry/MzMz08AoqpqcpKCjAnTt3tOqlJpfLAVQkExUKBUJCQjBv3jyNQzCBiuGeo0aNanB82rxOU1NTDB8+HD/88AMWLFggLrRTWFgIANi3b1+NxVpMTU0bHBORXtTyM0PU2uiUUFuwYEGd+01NTREcHKyWRKuuR48e9a6W4+rqWu/S02PGjOEQTyKiFkr51qtNWr/QtQeEya806TmIiAyh+tQo586dg4ODg8akj0wm03vPNQBwc3ODQqHAvXv3xMUPNFm2bBn69OmDl156CW+99RaGDx+O3r1711o+Pj6+Ro+uxhg/fjzeeecdREREaFxEreo0NNUplUqUl5dDqVRqbNvCwkIkJSXhnXfeqfX8zs7OOHv2LF588UVx25kzZ8Q26Nu3L7Kzs3Ht2rVae6lJJBL8z//8D1577TVMmTIFBw8eRM+ePeHi4oJ27dohKytLXGBC0/krF5qodO7cuVrjJdKb7Jv1lyFqBSSGDoCIiNoW1cP8+gsREZFGWVlZCAsLQ3p6Or799lt8+umn4kqe1dna2uLkyZPIycmpsaBXYzg5OWHy5MlYtGgRYmNjcePGDaSkpGDHjh1ITEwEAHz22Wf45ZdfsH37dkyePBljxozB66+/jtLSUrEeX19fHDlyBEBFb7aLFy9qXN2yoeRyOdauXYvdu3dj2bJlSE5Oxq1bt3DmzBmEhIRg+/btAICvv/4ahw4dQlpaGn7//XccOnQImzdvxsSJE8WeeikpKfD19RVXjj527BgcHR3VFiiobsGCBfjyyy+xd+9eZGRkYNeuXThy5Ajmz58PoGKlVS8vL8ydOxcnTpzAjRs38OOPP+LYsWNq9UilUuzcuRP9+vXDiy++iJycHDz22GOYN28ewsLC8OWXX+L69ev49ddf8emnn+LLL78EAMycOROZmZnYsGED0tPT8c0334j7iIio8ZhQIyKiZqXcx8mQiYgaKjAwEMXFxZgwYQJCQ0MRFBSEGTNmaCy7Zs0anDhxAp6enhg9erRe4wgPD0dgYCDWr18PX19fBAUF4cKFC5DL5UhPT8eGDRuwadMmcRjlpk2bcP/+fbz77rtiHdeuXUNBQQGAiuGTAwcOrDHPWmPNmjULn3/+OW7fvo3g4GCMGDECy5cvh7m5uZjYkkqliIiIwPjx4zFy5Ei8//77mDVrllqsRUVFuHbtGsrLK+bMi4+Przf5N2bMGKxbtw67du3C008/jejoaISHh2PYsGFimaioKLi7u2PhwoXw9/fHxo0bNfYqNDExQUREBJ544gm8+OKLyM3NRUhICBYvXoydO3fCz88P06dPxw8//IDHH38cQEVC8eOPP0ZcXBxGjRqF/fv3q81LTUREjSOouPY87t69i7KyMr3XKwgCrK2tkZ2dDTZz82CbGwbb3TBaarsr5kysdZ806lCj6mvI8bpqqe3e0hlTu8tkMvTo0cOgMZD2arvOKygoqLHCpK5kMlmTXEO2RbNmzcLQoUPFRQDqYuh2r5wrOjo6usak/61F1Z8PY/r925a0pHbXdG3XHNdkTaEltXtrYkztrst1HnuoERGRcXjcsUGHCUFLAUGAZAHvuhMRtVRDhw7FpEmTDB2GVvLy8jBnzhy1RdOIiKjt4Xq2RERkWK4ekDw9AXDu26DDJd5+UHkOh8AVpYiIWixteqYZi+7du2Px4sWGDoOIiAyMCTUiIjIowdMXwgDPxtXBZBoRERERETUjDvkkIiKDEnz8DB0CERERERGRTphQIyIigxIk7F1GREREREQtCxNqRERkOBJ+DBERERERUcvDv2SIiKjZqIr+Ut9gwqk8iYiIiFoqlVJh6BCIDIYJNSIiaj4P89SfCxzuSURERNRiKZQ1Nkl2fGGAQIiaHxNqRETU5JQ/JUAxZyJUv/2qtl3w8DZQRERERETUaNV6qEk+/g6CWQcDBUPUvJhQIyKiJqfat7Pi+/4P1XcM8DRANERELVNgYCDWrFlT634vLy9ERUXVW090dDSGDBkCW1tbREVFYdu2bXjmmWf0GSo1g6SkJMjlcuTn5xs6FGrL7t999NjDG4IgGC4WombGyWuIiMhgBC5KQESkN7GxsejQ4VHPELlcjt27d2PMmDHitocPH2L16tVYu3Ytxo0bh06dOkGpVOLVV181RMgaLVmyBCdOnMDt27cbVU94eDimTp2Kn3/+GZGRkTh37hyKi4thZ2cHf39/zJ07F9bW1khPT8eKFSuQlpaGhw8fwsrKCs899xyWLl0KmUxW73m8vb2xdetWZGdnIywsDKmpqTrFGRMT06DjiIyB6sfD4mPJ/H8aMBKi5seEGhERGc5jnQwdARFRq9GtW7d6y2RlZaGsrAwBAQGwsrISt3fs2LEpQ9OaQqFAYmIiPv74Yzg5OYnbg4OD0adPHyxfvhwmJiYoKSkBAEilFXNxRkZG4vjx4/jii0dzN5mbm2P//v1YtWoVpkyZgqioKNjZ2SErKwsHDx7Erl27EBYWBplMhilTpsDNzQ0WFha4cuUK3nrrLSiVSqxcubLOeK9cuYL8/Hz4+Pjg66+/boIWITJyFl3Eh4KEc+NS28KuAUREZDguroaOgIgIKpUKqpJiw3ypVDrFqlAoEBoaij59+sDNzQ1bt24V66g65NPLywsAEBQUBLlcDi8vL8TExCAgIAAA4OPjA7lcjps3b9YY8rl48WLMnj0bkZGR8PDwgKurK1atWoWysjKxTElJCdavX4/BgwfD2dkZEyZMQFJSEgCguLgY/v7+CAkJEctfv34dLi4uagkvTc6ePQuZTAZvb29YWlqKX6ampjAzM4OlpSWsrKxgbW0Na2trcX/Hjh0hlUrVjnnw4AHWrFmD2bNnIzw8HMOGDYOdnR28vb3x3nvvYcmSJQAAe3t7TJ06Fa6urrC1tcWoUaPw/PPP4/Tp0/W+H/Hx8fDz88OZM2ewdOlSFBQUQC6XQy6XY9u2bQCAvLw8vPnmm+jXrx+cnJwwY8YMZGRkAKgYtlnbcQcPHsTYsWPh4uKCgQMH4rXXXkNubm69MRE1Lw7xpLaLPdSIiMgwJBLOs0FExqG0BMrXX9T5sBI9nFqy80ugnZnW5Q8cOIBp06bh8OHDuHjxIkJCQiCXyzF9+nS1crGxsRgwYADCw8Ph7+8PqVSKDh06wMbGBtOmTcP3338PGxubWnu1JSUlwdLSEgcOHEBmZiYWLFgAV1dX8TyrV6/G1atXERERASsrK8TFxWHGjBlITEyEo6MjduzYgWeffRYBAQEYOXIk3njjDfj6+mLatGniOeRyuTgss1JCQgJGjhypl8+Hw4cPo7S0FAsXLtS438LCQuP2zMxMHD9+HGPHjlVrjylTpuDUqVOws7MTtx89ehRz587FkCFDsG7dOrz33ns4ceIEgEe9/pYsWYLMzEzs2bMHjz32GDZt2oR//OMfOH78eJ3HlZeX46233oKTkxNyc3Oxbt06LFmyBPv372902xDpi9DLGbrdFiBqPZhQIyIigxC8Rhg6BCKiFsfGxgbr1q2DIAhwdnbGb7/9hqioqBoJtcpEmYWFBSwtLcXtXbp0EfdX3V6dhYUFNm7cCKlUCmdnZwQEBODkyZOYPn06srKyEBMTg9OnT6Nnz54AgPnz5+PYsWOIiYnBypUr4ebmhpCQECxfvhyTJk1CVlYW9u7dq3YOJycndOqkPvQ/Pj4eYWFhDW6fqjIzM2Fubq42tLUuEydOxKVLl1BSUoLp06fjrbfeEve1b98eTk5OMDF59OdTdnY2UlNT4e/vD1NTU5ibm0MQBLV2zcjIQEJCAr799lt4elYsxLNjxw54enoiLi4Ozz77rMbjAKglH+3t7bFhwwaMGzcOhYWFRjNElwiydhXfLa0NGweRATChRkREhsF5NojIWJi2q+gppiOZTKY2DLKh59bFoEGD1HpvDR48GLt27YJCoWhcHNW4uLiI85MBgJWVlThpfmpqKhQKBYYPH652TGlpqZiwA4B58+YhLi4Oe/bsQXR0NLp27apWvrJHVqW0tDTcuXMHTz31lF5eg0ql0qmn20cffYTCwkJcuXIFGzZsQGRkpNi7zcPDo0a8CQkJ8PT0rLWnGwCkp6fDxMQEgwYNErd17doVTk5OSE9PrzOeixcvYtu2beI8bUqlEkDFPHguLi5avy6iJqX8+3ePSf0LeBC1NkyoERGRQQj9hxg6BCIiAKhIuugw7FI8TiZrtZNwa1rdsnKutsLCQkilUhw5ckQt6QaoL26Qm5uLjIwMSKVSZGZmwt/fv85zJiQkwNfXF2Zmur8Xmjg6OqKgoAB37tzRqpeaXC4HUJFMVCgUCAkJwbx582q8xkpHjx7FqFGj9BJrdX/99Rdefvll+Pn5YefOnejWrRuysrLw8ssvo7S0tEnOSdQgqopEL7hyO7VB/F9PREQGIQweZugQiIhanJSUFLXn586dg4ODg8akj0wm03vPNQBwc3ODQqHAvXv34ODgoPZVddjismXL0KdPH2zfvh0bN25EWlpanfXGx8frNUE1fvx4mJqaIiIiQuP+/Pz8Wo9VKpUoLy8Xe4VVV1hYiKSkJIwePVrcZmpqWqO9nZ2dUV5ejnPnzonb7t+/j2vXrqF37961Hpeeno4HDx5g5cqV8PLygrOzMxckIOOkZEKN2i7+ryciouZn72zoCIiIWqSsrCyEhYUhPT0d3377LT799FMEBQVpLGtra4uTJ08iJycHeXl5eovByckJkydPxqJFixAbG4sbN24gJSUFO3bsQGJiIgDgs88+wy+//ILt27dj8uTJGDNmDF5//XW13lW+vr44cuQIgIrebBcvXlRbbbSx5HI51q5di927d2PZsmVITk7GrVu3cObMGYSEhGD79u0AgK+//hqHDh1CWloafv/9dxw6dAibN2/GxIkTxZ56KSkp8PX1RXZ2NgDg2LFjcHR0VFugwNbWFoWFhfjpp59w//59FBUVwdHREaNHj0ZISAhOnz6Ny5cv480330TPnj3FZJym4+RyOUxNTbFnzx78/vvvSEhIEOMlMiqVCTWBqQVqe/i/noiIml8tw2eIiKhugYGBKC4uxoQJExAaGoqgoCDMmDFDY9k1a9bgxIkT8PT0VOtJpQ/h4eEIDAzE+vXr4evri6CgIFy4cAFyuRzp6enYsGEDNm3aJA6j3LRpE+7fv493331XrOPatWsoKCgAUDF8cuDAgTXmWWusWbNm4fPPP8ft27cRHByMESNGYPny5TA3N8f8+fMBAFKpFBERERg/fjxGjhyJ999/H7NmzVKLtaioCNeuXUN5eTmAit501ZN/np6e+Mc//oEFCxagf//+Ys+48PBw9O/fH6+88gomTpwIlUqF/fv3i8k6Tcd169YN77//Pg4fPgx/f3/s3LkT//rXv/TaNkT6oEz6oeLB73XPCUjUGgmqyskQ2rC7d+82fkJZDQRBgLW1NbKzs8Fmbh5sc8NguxtGS2p3xZyJ6ht694M0ZLNhgmmkltTurYkxtbtMJkOPHj0MGgNpr7brvIKCghorTOpKL4sSEICKxNfQoUPFRQDqYuh2Ly8vh7u7O6Kjo+Hh4WGwOJpS1Z8PY/r925a0lHaveo0njTpkwEj0o6W0e2tjTO2uy3Uee6gREVHza6WTeBMRUcMMHToUkyZNMnQYWsnLy8OcOXMwcOBAQ4dCREQGxFU+iYio+f33V0NHQERERkSbnmnGonv37li8eLGhwyAiIgNjDzUiIiIiIiIi0omqnEPeqW1jQo2IiIiIiIiIdKI6fUJ8LPg8bcBIiAyDCTUiIiIiIiIi0k1ujvhQ8PA2YCBEhsGEGhERNSlDr9RDRERERPqhKit9dG1XZcinqqTIQBERGQ4TakRE1LRUSkNHQERERESNpLrzB5QLA6HatbViQ9mjhJpgY2+gqIgMhwk1IiJqWgom1IiIiIhaOuWODQAA1S8/V2woL32005YJNWp7mFAjIqKmpVQYOgIiIiIiagTVrevAnaxHz29mAg8LxOeCRGqAqIgMiwk1IiJqWgom1IiI9CEwMBBr1qypdb+XlxeioqLqrSc6OhpDhgyBra0toqKisG3bNjzzzDP6DJWaWVJSEuRyOfLz8w0dCrVSyqj31J+vX/SopxpRG2Vi6ACIiKiVYw81IqJmERsbiw4dOojP5XI5du/ejTFjxojbHj58iNWrV2Pt2rUYN24cOnXqBKVSiVdffdUQIWu0ZMkSnDhxArdv325UPeHh4Zg6dSp+/vlnREZG4ty5cyguLoadnR38/f0xd+5cWFtbIz09HStWrEBaWhoePnwIKysrPPfcc1i6dClkMlm95/H29sbWrVvh6+vbqHi1FRgYiH79+mH9+vXNcj4iAMAfNwwdAZHRYUKNiIiaFhNqRETNolu3bvWWycrKQllZGQICAmBlZSVu79ixY1OGpjWFQoHExER8/PHHcHJyErcHBwejT58+WL58OUxMTFBSUgIAkEorhplFRkbi+PHj+OKLL8RjzM3NsX//fqxatQpTpkxBVFQU7OzskJWVhYMHD2LXrl0ICwuDTCbDlClT4ObmBgsLC1y5cgVvvfUWlEolVq5cWWe8V65cQX5+Pnx8fJqgNdSVlpbC1NS0yc9DpLO+7oaOgMggOOSTiIialoZFCSRLNxggECIizVQqFYrLlbp/lTXgmGpfKpVKp1gVCgVCQ0PRp08fuLm5YevWrWIdVYd8enl5AQCCgoIgl8vh5eWFmJgYBAQEAAB8fHwgl8tx8+bNGkM+Fy9ejNmzZyMyMhIeHh5wdXXFqlWrUFZlRb+SkhKsX78egwcPhrOzMyZMmICkpCQAQHFxMfz9/RESEiKWv379OlxcXNQSXpqcPXsWMpkM3t7esLS0FL9MTU1hZmYGS0tLWFlZwdraGtbW1uL+jh07QiqVqh3z4MEDrFmzBrNnz0Z4eDiGDRsGOzs7eHt747333sOSJUsAAPb29pg6dSpcXV1ha2uLUaNG4fnnn8fp06frfT/i4+Ph5+cHExMT9O/fH4cPHxb3PfPMM/Dw8BCfnz59Gg4ODigqKgJQkdx89dVX0bt3bzzxxBOYN28e7t69K5avfF8+//xzeHt7w9HREYsXL0ZycjJ2794NuVwuvoeVLl68iLFjx8LJyQkTJ05Eenp6va+BqC4qlQrKL+oZSp56oXmCITIy7KFGRERNq1oPNcF3NATeySQiI1KiUGFqzFWDnDtmqgvMTAStyx84cADTpk3D4cOHcfHiRYSEhEAul2P69Olq5WJjYzFgwACEh4fD398fUqkUHTp0gI2NDaZNm4bvv/8eNjY2tfZqS0pKgqWlJQ4cOIDMzEwsWLAArq6u4nlWr16Nq1evIiIiAlZWVoiLi8OMGTOQmJgIR0dH7NixA88++ywCAgIwcuRIvPHGG/D19cW0adPEc8jlcnFYZqWEhASMHDkSgqB9m9Tm8OHDKC0txcKFCzXut7Cw0Lg9MzMTx48fx9ixY9XaY8qUKTh16hTs7OzE7UePHsXcuXMhCAK8vb2RnJyMCRMmIC8vD+np6TAzM0N6ejqcnZ2RnJwMd3d3tG/fXhxm27FjR3z11VcoLy9HaGgoFixYgIMHD4r1X79+HbGxsfjkk08gkUhga2uLjIwMsbceUNEzsTKptmXLFqxZswbdunXDihUrsGzZMnz33XeNbktqw25kQPXDvw0dBZFRYkKNiIiaVrVFCST/eM1AgRARtXw2NjZYt24dBEGAs7MzfvvtN0RFRdVIqFUmyiwsLGBpaSlu79Kli7i/6vbqLCwssHHjRkilUjg7OyMgIAAnT57E9OnTkZWVhZiYGJw+fRo9e/YEAMyfPx/Hjh1DTEwMVq5cCTc3N4SEhGD58uWYNGkSsrKysHfvXrVzODk5oVOnTmrb4uPjERYW1uD2qSozMxPm5uZqQ1vrMnHiRFy6dAklJSWYPn063nrrLXFf+/bt4eTkBBOTR38+ZWdnIzU1Ff7+/gAqev1FR0cDAP7zn//A1dUVlpaWSEpKEhNq3t7eAICTJ0/it99+Q3JyMuRyOQDggw8+gL+/P86fP4+BAwcCAMrKyvDBBx+oJT6r9tar7p///Kc4/PS1117DzJkzUVxcDDMzM22bjUhd8V+GjoDIaDGhRkRETUr146PhL0LwMgNGQkSkWTupgJipLjofJzORoay8rP6C9ZxbF4MGDVLrvTV48GDs2rULCj2vqOzi4iLOTwYAVlZWSE1NBQCkpqZCoVBg+PDhaseUlpaKCTsAmDdvHuLi4rBnzx5ER0eja9euauVPnDih9jwtLQ137tzBU089pZfXoFKpdOrp9tFHH6GwsBBXrlzBhg0bEBkZKfZu8/DwqBFvQkICPD09xZ5u3t7eWLNmDe7du4fk5GQMGzYMPXr0QHJyMl566SWcPXtWrC8tLQ02NjZiMg2oaHMLCwukpaWJCTW5XK7V3HiV+vXrJz6uTCTeu3dP7TxEulBdTzN0CERGiwk1IiJqUqqf4sXHEq8RBoyEiEgzQRB0GnZZSSaTQNpKpyTWtLpl5VxthYWFkEqlOHLkiFrSDVBf3CA3NxcZGRmQSqXIzMwUe3LVJiEhAb6+vnrrTeXo6IiCggLcuXNHq15qlUknFxcXKBQKhISEYN68eTVeY6WjR49i1KhR4vO+ffuic+fOSE5OxqlTp/DPf/4TPXr0QEREBM6fP4/y8nIMGTJEp9dQddVWbVTtQVdJqaw5lymRtlQHP6u/kPvQJo+DyBi1zisAIiIyHo87GzoCIqJWIyUlRe35uXPn4ODgoDHpI5PJ9N5zDQDc3NygUChw7949ODg4qH1VHYa4bNky9OnTB9u3b8fGjRuRllZ3T5f4+Hi1BFVjjR8/HqampoiIiNC4Pz8/v9ZjlUolysvLa01GFRYWIikpCaNHjxa3CYIALy8vxMfH4+rVqxg6dCj69euH0tJSREdHY8CAAWKCrHfv3vjjjz+QlZUlHn/16lXk5+fDxaXu3pIymYxJMjIqQqfOhg6ByCCYUCMioqYlafzE0kREVCErKwthYWFIT0/Ht99+i08//RRBQUEay9ra2uLkyZPIyclBXl6e3mJwcnLC5MmTsWjRIsTGxuLGjRtISUnBjh07kJiYCAD47LPP8Msvv2D79u2YPHkyxowZg9dffx2lpaViPb6+vjhy5AiAit5sFy9eVFtttLHkcjnWrl2L3bt3Y9myZUhOTsatW7dw5swZhISEYPv27QCAr7/+GocOHUJaWhp+//13HDp0CJs3b8bEiRPFnnopKSnw9fVFdnY2AODYsWNwdHRUW6AAqJhH7bvvvkO/fv3QsWNHSCQSeHl54ZtvvhHnNgOA4cOHo0+fPnjjjTfw66+/IiUlBYsWLYKPjw/c3eteuMfOzg4pKSm4efMm7t+/z+QaGZwwNdjQIRAZBBNqRETUpITBTxo6BCKiViMwMBDFxcWYMGECQkNDERQUhBkzZmgsu2bNGpw4cQKenp5qPan0ITw8HIGBgVi/fj18fX0RFBSECxcuQC6XIz09HRs2bMCmTZvEYZSbNm3C/fv38e6774p1XLt2DQUFBQAqhk8OHDiwxjxrjTVr1ix8/vnnuH37NoKDgzFixAgsX74c5ubmmD9/PgBAKpUiIiIC48ePx8iRI/H+++9j1qxZarEWFRXh2rVrKC8vB1DRm05T8s/b2xsKhQLDhg0Tt/n4+EChUKgl1ARBwJ49e2BhYYHJkydj2rRpePzxx/HRRx/V+5rmzZsHiUQCPz8/9O/fX62XG5EhCO246AW1TYKqcjKENuzu3bsoK2vchLKaCIIAa2trZGdng83cPNjmhsF2N4yW0u7KMyeh+ngr0LsfpCGbDR1Oo7WUdm9tjKndZTIZevToYdAYSHu1XecVFBTUWGFSVzKZrEmuIduiWbNmYejQoeKk/XUxdLuXl5fD3d0d0dHR8PDwMFgcTanqz4cx/f5tS4yl3RVzJtZbRhp1qBkiaR7G0u5tjTG1uy7XeeyhRkRETUv59/w9Uq6DQ0REmg0dOhSTJk0ydBhaycvLw5w5c8SVOImIqG3iXzdERNS0VH/P7SLhPRwiItJMm55pxqJ79+5YvHixocMgIiID4183RETUtJR/d9tmQo2IiIioVRHGvmDoEIgMhj3UiIioaVX2UBOYUCMiIiJqLSRbdgNduhs6DCKD4V83RETUtIqLKr4LgmHjICIiIiL96dIdAq/vqA1jQo2IiJqU6ouoigcXzxg2ECIiIiLSjYNLrbuYTKO2jgk1IiIiIiIiIqpB6NP/0WO/sY92dLNs/mCIjAwTakRERERERERU09+LSwlPT4Bk+oJH27ty7jQiJtSIiIiIiMjoeHl5ISoqytBhELVpqjMnKr7fyarY8PcwT8HFzVAhERkNJtSIiKjJqO7eNnQIRET0t23btuGZZ54xdBg1xMTEoG/fvoYOg4g0uZ9b8f1yCgBA8nYkhJfmQhj/ogGDIjIOJoYOgIiIWi/Vv78QH0vW7TRgJEREZIzKysoMHQIR6UCwtIbw9ARDh0FkFNhDjYiImozqetqjJ+07Gi4QIqJWIDAwEP/617/w9ttvw9XVFQMHDsS2bdvE/fn5+Vi+fDn69++PJ554AlOmTMHly5cBVPQCCw8Px5UrVyCXyyGXyxETE1Pn+davX4+ZM2eKz6OioiCXy3Hs2DFx25NPPonPP/8cAKBUKvH+++9j8ODBcHBwwDPPPKNW9ubNm5DL5fjuu+/wwgsvwNHREV9//TWWLl2KgoICMa6qr6moqAhLly6Fi4sLPD09ER0d3bhGJCIi0hMm1IiISK9UfxZApaqYwBZK5aMdinLDBEREVA+VSoXycsN8ib8vtXTgwAF06NAB//73vxEaGor3338fJ05UzHE0b9485ObmIjo6GkeOHEH//v0xdepUPHjwABMnTsS8efPwxBNPICUlBSkpKZg4cWKd5/L29saZM2egUCgAAKdOnULXrl2RnJwMAMjOzsb169fh4+MDAPjkk0+wa9curFmzBkePHoWfnx9effVVZGRkqNX7zjvvICgoCMePH8eTTz6JdevWwdzcXIxr/vz5Ytldu3ZhwIABiI+PxyuvvIKVK1ciPT1dpzYjIiJqChzySUREeqO6kgLl+2shDPWFMGc5IFTZ+Vgng8VFRFQXhQI48lW+Qc499gULmOhwRd63b18sXboUAODo6IjPPvsMJ0+ehJmZGc6fP48LFy6gXbt2AIA1a9YgPj4e33//PWbMmIGOHTtCKpXC0tJSq3N5eXnhzz//xKVLlzBgwACcOnUKCxYsQFxcHAAgOTkZPXv2hIODA4CK5NfChQsxadIkAEBoaCiSkpLwySefYNOmTWK9wcHBGDdunPjc3NwcgiBojOvpp5/GrFmzAACvvfYaoqKicPLkSdjb22vfaETUcJY2QM4fEKa8auhIiIwOE2pERKQ3yvfXAgBUp08Ac5ZX/JX6N8GsvaHCIiJqNapP3m9paYnc3FxcuXIFhYWFcHNTX3mvuLgYv//+e4POZWFhgX79+iE5ORkymQympqaYPn06tm3bhsLCQpw6dUrsnfbw4UPcvn0bnp6eanUMGTIEV65cUdvm7u6udQz9+vUTHwuCgB49eiA3N7dBr4eIGqBLNyDnD6BLd0NHQmR0mFAjIiK9UGVeVX9e+BAoKjRQNERE2pNKK3qK6UomkzV6Un2pVLfyJtW6swmCAKVSicLCQlhaWuLgwYM1jrGw0P21VfLx8UFSUhJMTU3h7e2NLl26wNnZGadPn0ZycjLmzZunc53t22t/g6W210tEzeTvKTsEXX9ZEbUBTKgREZFeKDctV3++eLqBIiEi0o0gCDoNu6xkYiJApRLqL9gM+vfvj7t378LExAR2dnYay8hkMp2TUT4+PoiJiYGJiQn8/PzEbd9++y0yMjLEHmrm5ubo2bMnzpw5I24DgLNnz2LgwIF1nsPU1FScp42IjEzlz6aUqQOi6rgoARERERFRCzd8+HAMHjwYs2fPxv/93//h5s2bOHPmDDZv3owLFy4AAOzs7HDjxg1cunQJ9+/fR0lJSb31Vs6jlpiYiGHDhgEAhg0bhm+++QZWVlZwcnISy86fPx8RERH47rvvkJ6ejk2bNuHy5csICgqq8xy2trYoLCzETz/9hPv376OoqKgRLUFEelWZUJOwhxpRdUwzExFRkxOGjzJ0CERErZogCNi/fz+2bNmCpUuX4t69e+jRowe8vb3RvXvF3Efjxo1DbGwsXnzxReTn5yM8PBxTp06ts97OnTujT58+yM3NhbOzM4CKJJtSqYS3t7da2aCgIDx8+BDr16/HvXv30Lt3b+zZsweOjo51nsPT0xP/+Mc/sGDBAjx48ABLly7FsmXLGtEaRKQ3lau0c8gnUQ2CSte1uluhu3fvNnr+C00EQYC1tTWys7N1XhKdGoZtbhhsd8MwpnZXXToH5QdhtRewkkP69kfNFk9TMqZ2b0uMqd1lMhl69Ohh0BhIe7Vd5xUUFKBTp8atPqyPOdRId2z3plf158OYfv+2JcbS7oo1rwHZNyFZvhHCE/0NFkdzMZZ2b2uMqd11uc7jkE8iImoU1bXf6k6mAcCdrGaJhYiIiIj0iD3UiGrFIZ9ERNQoqutphg6BiIga4Ouvv8Y///lPjftsbW1x7NixZo6IiIxOTnbFdy5KQFQDfyqIiKhxCh8aOgIiImqAUaNGwcPDQ+M+mUzWzNEQkbFRlVcZWs1h1kQ1MKFGRESNovr3F/WWEca80AyREBGRLh577DE89thjhg6DiIxV1SRaCVffJaqOc6gREVHT62lr6AiIiIiISBdK5aPHnEONqAYm1IiIqMkJboMMHQIRUQ2GXkmMyBgpqyZRqG2rXJAAACy6GS4OIiPFhBoRETU5waKLoUMgIlLTrl07FBVxCBNRVUqlEg8fPkSHDh0MHQoZA4VCfCjIHzdgIETGiXOoEREREVGb065dOxQWFiI/Px+CIDSoDlNTU5SWluo5MqoP271pdezYESYm/DOR8KiHmqmpYeMgMlL8TUlERERE+Oabb3D69GlkZWXB1NQULi4umDFjBmxsbMQypaWl2LdvH5KSklBWVgZ3d3cEBwejc+fOYpnc3FxERUXh8uXLMDMzw4gRI/Dyyy9DWmX+ncuXL2Pfvn24efMmunXrhhdeeAF+fn5q8cTFxeHf//438vLyYG9vj9mzZ8PZ2Vmvr7ljx44NPlYQBFhbWyM7O5tDR5sR252oGVX2UJMybUCkCYd8EhERERGuXLmC0aNHY+PGjVi9ejUUCgXefvttFBcXi2X27t2LX375BUuXLsW6devw4MEDbNu2TdyvVCrxzjvvoLy8HG+//TZee+01HD9+HDExMWKZnJwcbN68Ga6urti6dSvGjx+PyMhInD9/XiyTlJSEffv2ITAwEFu2bIG9vT02btyI/Pz8ZmkLIiLCox5qXJCASCMm1IiIiIgIoaGh8PPzg52dHXr16oXXXnsNubm5yMjIAAD89ddf+PHHH/HKK6/Azc0Njo6OWLhwIf773//i6tWrAIALFy7g1q1beOONN9CrVy94eHhg6tSpiI+PR3l5xR9mCQkJsLS0xMyZM2Fra4sxY8bA29sb33//vRjL4cOHERAQAH9/f9ja2mLOnDkwNTXFsWPHmr9hiIjaKvZQI6oTfzKIiKhJCf94zdAhEFED/PXXXwCAxx57DACQkZEBhUKB/v37i2Xkcjm6d++Oq1evwsXFBVevXsXjjz+uNgR04MCB+OSTT3Dz5k04ODggLS1NrQ4AcHd3x2effQYAKC8vR0ZGBp577jlxv0QiQf/+/cXEnSZlZWUoKysTnwuCgPbt24uP9a2yzqaom2rHdm9+bHPDMIp2V1Ym1KRt5v03inZvg1pquzOhRkRETUay/XMIHR8zdBhEpCOlUonPPvsMTzzxBB5/vGJlt7y8PJiYmNSYd8zCwgJ5eXlimarJtMr9lfsqv1duq1qmqKgIpaWl+PPPP6FUKmvU07lzZ/zxxx+1xvzNN9/g4MGD4nMHBwds2bIFPXr00PZlN0jPnj2btH7SjO3e/NjmhqGvdi++cBbFv/yMTtPnQdLOTKtjSvJzkQNAatoO1tbWeomjpeD/d8Noae3OhBoRETUZJtOIWqbdu3fj5s2bWL9+vaFD0drzzz+PCRMmiM8r73LfvXtXHG6qT4IgoGfPnrh9+zYnx29GbPfmxzY3DH22u/Lsz1BGbgYA/FlSCulzM7Q6TnXnDgBAoQKys7MbFUNLwf/vhmFM7W5iYqL1zTgm1IiIiIhItHv3bpw7dw7r1q1Dt27dxO2dO3dGeXk5CgsL1Xqp5efni73JOnfujPT0dLX6KhcSqFqm+uIC+fn5aN++PUxNTdGpUydIJBKxR1slTb3fqpLJZJDJZBr3NeXFuUqlMvjFf1vEdm9+bHPD0Ee7VybTAED1w2GoJk3X7tzlfw+jl0rb3HvP/++G0dLanYsSEBFRowg+/o+eOPV59NjxieYPhogaTKVSYffu3Th9+jTWrFkDS0tLtf2Ojo6QSqX49ddfxW1//PEHcnNz4eLiAgBwcXHBjRs31BJmFy9eRPv27WFrawsA6N27t1odlWUq6zAxMYGjoyMuXbok7lcqlbh06ZJYhoiIGqhPf6jKSrUqqsqt6KHGVT6JNGNCjYiIGkeo+CgRhjwFyZINjzbbOxkqIiJqgN27d+Onn37CokWL0L59e+Tl5SEvLw+lpRV/eHXo0AFPP/009u3bh0uXLiEjIwMRERFwcXERE13u7u6wtbXFzp07cf36dZw/fx5ffPEFRo8eLfYeGzVqFHJychAdHY2srCzEx8cjOTkZ48ePF2OZMGECfvjhBxw/fhy3bt3CJ598gpKSEvj5+TV7uxARtSopp6BcGAjlTwl1FlMmfgfVvp0VT25mNkNgRC0Ph3wSEVGjqLJvVjyw7QWhXbtHO0y1m/CWiIxDQkLFH1dhYWFq2xcuXCgmsl555RUIgoBt27ahvLwc7u7uCA4OFstKJBKsWLECn3zyCVavXo127dphxIgRmDp1qljG0tISK1aswN69exEbG4tu3bph/vz5GDhwoFhm2LBhKCgowJdffom8vDz06tULq1atqnPIJxERAaor56GMjoAwbgokTz1Te7l9O6Ea6AXVqeMQ/MZBqDJkXpWTDVXM7maIlqhlE1QtaYBqE7l7967aMuv6IggCrK2tkZ2d3aLGAbdkbHPDYLsbhrG0u2LOxIoHnTpDum0flD8chuo/xyFZFNYqFyUwlnZva4yp3WUyWZOvHEn6w+u81oXt3vzY5obR0HYXr8sASN6PhvBYJ7VttZHsPCDeGNVUXhp1SOsYWjL+fzcMY2p3Xa7zOOSTiIj0oyAPACAJmADpqvdaZTKNiIiIqMW4nqZ1UdX/flT7TnMLPQRD1PowoUZERERERETUyqj+vtmpVdnkHyu+KxQ19kne+URfIRG1KkyoEREREREREbU219MrvrsN0v6YspIam9TmyCUiERNqRESkF4LXCEOHQERERNTmKA/sgfLrfTXmnlId+x4AIJh31r6yv1d2riQELWlseEStFlf5JCKixulpC9y+BeHJkYaOhIiIiKhNUeXdgyrhGwCA0NtVcyFlzWGctdZ38qj4WLL2fyDY9mpMeEStGnuoERGRfkilho6AiIiIqE1R/eeE+Fj5P+s0F1Iqta/vm/3iYybTiOrGhBoRETWOorziu4QJNSIiIqLmpDq4p/4y55K0q0uHxBsRMaFGRESNVXnxJeUsAkRERERGR8PKnZUEv3HiY+XWFRDGvlDxxMWtiYMiavmYUCMiosap7KEm5UcKERERkbEQfMdo3iG3r/jeux+E8S8+2n7tN6iOfFVxbHerJo6OqOXjXz9ERNQ4lXc92UONiIiIqFmoHtyD8kQ8hGEBtRcyaw+Vht5pkpDNEIKXQfLmWsDMTHP9ST/oK1SiVot//RARUeNUXqhxDjUiIiKiZqHcsBh4mF93IZVS44IEQoeOELxGVBQpL2+C6IjaBvZQIyKixqlcip1DPomIiIiaR33JNABQqaD67n/Vt3U0V39eyyrtkg0RDQyMqO3gXz9ERNQ4pSUV3znkk4iIiMh4qFRQxX+ttkkYPEz9uSBoPFToadtkYRG1FkyoERFRg6lKSx4NJeCQTyIiIiKDE0ZPrnhQVqa+fcJUCIGvGiAiotaJ3QmIiKjhMtMePa5lyAARERERNRMXN6jOJQEAVCfi1HZJJk03RERErRZ7qBERUSOoHj2U8COFiIiIyJCE/oOBu7cNHQZRm8C/foiIqOE0rBxFRERERE1H9defNbYJE6YBA70h+I42QEREbROHfBIRUcN1eOzR4+qrRhERERGR/hVqSKg9Ow2SBo4WEIaPguqnhMZGRdTmsIcaERE1XGUPtS7da10lioiIiIj0SMPK6kI9yTRh1PO17/vHa5C8t1ftORHVjz3UiIio4ZSKiu8m/DghIiIiahYqVZ27hZfmQvX/Plbf5j609vKCAFh0gSQ8Gvg9Heg3UB9RErV6Ov0F9M033+D06dPIysqCqakpXFxcMGPGDNjY2IhlSktLsW/fPiQlJaGsrAzu7u4IDg5G586dxTK5ubmIiorC5cuXYWZmhhEjRuDll1+GtMoKcZcvX8a+fftw8+ZNdOvWDS+88AL8/PzU4omLi8O///1v5OXlwd7eHrNnz4azs3PDWoKIiHSnKK/4zhU+iYiIiJpH5Q3N2miYhkNwca23WsG8E+A2qKFREbU5Og35vHLlCkaPHo2NGzdi9erVUCgUePvtt1FcXCyW2bt3L3755RcsXboU69atw4MHD7Bt2zZxv1KpxDvvvIPy8nK8/fbbeO2113D8+HHExMSIZXJycrB582a4urpi69atGD9+PCIjI3H+/HmxTFJSEvbt24fAwEBs2bIF9vb22LhxI/Lz8xvRHEREpBPF3xd0GoYeEBEREVETqLoolIkMkoWr1Pdn/Ld54yFqo3RKqIWGhsLPzw92dnbo1asXXnvtNeTm5iIjIwMA8Ndff+HHH3/EK6+8Ajc3Nzj+//buO76pcv8D+Oc5Sbp3aSlQ9hBEZCiioBe3iChuuOp1IDhwXn8ORPS6975eFQQHDoYTUJwoDsQFskfZLbMtbbqbdZ7fH6c5SZrRpG2Sjs/79dKe85znnPPN0zQk3zyjVy9MnToVW7duRV5eHgBg7dq12Lt3L2655Rb06NEDQ4cOxYQJE/D111/Dbtd6OnzzzTfIzs7GlVdeidzcXIwZMwbHH388vvjiCz2Wzz//HKeddhpOOeUU5ObmYsqUKYiJicEPP/zQXG1DREQNcSbUGjkJLhERERGFyNlDLTEZyisLIYYe73m8gp1MiCKhSV0KqqurAQBJSdoqbzt37oTD4cCgQYP0Ol26dEGHDh2Ql5eHfv36IS8vD926dfMYAjpkyBDMnj0bBQUF6NmzJ7Zt2+ZxDQAYPHgw3n77bQCA3W7Hzp07cf755+vHFUXBoEGD9MSdLzabDTabTd8XQiA+Pl7fbm7Oa3Ki7shhm0cH2z06WkK7S1n3DanB2G5+/y2h3dsjtjsREVEd52daoxHCx7QbsviQZwHnuiUKi0b/ZamqirfffhtHHHEEunXrBgAwm80wGo1ITEz0qJuamgqz2azXcU+mOY87jzl/Osvc69TU1MBqtaKyshKqqnpdJy0tDfv37/cb86effoqPPvpI3+/ZsyeeeuopZGVlBfuwGyUnJyes1ydvbPPoYLtHR3O1e/Uv38G6ZQNSr7nF55szn+fsTsFhADFx8ejYqVOzxNFa8PkeHWx3IiJq92xW7WdMrO/j9UcO1I0EI6Lm1eiE2pw5c1BQUICHH364OeMJqwsuuADjxo3T953fchcVFenDTZuTEAI5OTk4ePAgZAMrsVDzYJtHB9s9Opqz3dUflkJ9/zUAQFV6FpSRpwZ3XnExAMCqOnDgwIEmxdBa8PkeHS2p3Y1GY9i/jCMiIvLLmVAzxfg+bjRFLhaidqxRCbU5c+Zg9erVeOihh5CZmamXp6WlwW63o6qqyqOXWllZmd6bLC0tDdu3b/e4nnMhAfc69RcXKCsrQ3x8PGJiYpCSkgJFUfQebU6+er+5M5lMMJl8v7iE8825lDLqb/7bG7Z5dLDdo6M52t2ZTAMAuf4vyBNOCe7e9rohB4qh3f3u+XyPDrY7ERG1d+pzM7SNslLfFbj6OlFEhDSLtJQSc+bMwR9//IEHHngA2dnZHsd79eoFg8GA9evX62X79+9HcXEx+vXrBwDo168f8vPzPRJm69atQ3x8PHJzcwEAffv29biGs47zGkajEb169cKGDRv046qqYsOGDXodIiJqpO69g6+rr/LJN25EREREEVVV4bu83urryq0PRCAYovYnpITanDlz8PPPP+O2225DfHw8zGYzzGYzrFaty2lCQgJOPfVUzJ07Fxs2bMDOnTvx6quvol+/fnqia/DgwcjNzcUrr7yC3bt3Y82aNZg/fz7OOussvffYmWeeicLCQrz33nvYt28fvv76a6xcuRLnnHOOHsu4ceOwbNkyLF++HHv37sXs2bNhsVhw8sknN1PTEBG1T/Kjt+GYch5kYeAhnPKvXyDfelHbcQ49ICIiIqKoUs651LNg4NDoBELUxoU05PObb74BADz44IMe5VOnTtUTWVdddRWEEHjuuedgt9sxePBgTJ48Wa+rKAqmTZuG2bNnY8aMGYiNjcXo0aMxYcIEvU52djamTZuGd955B0uXLkVmZiZuuOEGDBkyRK8zcuRIlJeXY+HChTCbzejRowemT58ecMgnEREB8nAR5PZNEH0HQmR08FtPve96GN5YDKmqEPUmt5WqA+rMp10FeRvDFS4RERER1ZGqo+FKObkeu0LhSAKicAgpobZw4cIG68TExGDy5MkeSbT6srKycO+99wa8zsCBA/H0008HrDNmzBiMGTOmwZiIiMhFnXYtAEACMLyxOGBdx5TzAADiX1Oh/MP1eqtef0HY4iMiIiIiF1lSDPWeSUCnrlDufFQvF5de6/sELkpAFBEhDfkkIqK2JahvOQHId1+F3JWnbVst4QyJiIiIiNyo90zSNg4UQC5ZoJeLE8/wfYL73Lap6WGMjKh9Y0KNiKg9swY/95n61D3aRkmx1zHlmbebKSAiIiIi8pCWqW/K5Utd5XHxvuu7JdSUSf8OV1RE7R4TakRE7YhX7zK7LfiTnSt6Gr1nCxBpGU2IioiIiIj86tXPZ7EQwn957/5AUgrQf1A4IyNq15hQIyJqB2RluZZMs9R6ln9bN4faUcMavIZwrhhlCyEJR0RERERNs3plyKco9zwF5fl3uSABURgxoUZE1MbJinKo/74C6k2XAA6757GldYvNqLLh63xRV7e0yKNcmbWoWeIkIiIiau9kVWWzXEcI4bcHGxE1DybUiIjaOLlhlWv7hy/9VFKDvp76wn/0bWXmZ3yzRkRERNQM1MXzoN5+GdTff9TL5Nb1UYyIiALxngiHiIjaFPnmC65tZ4+0+hzBrfZZn1D4vQwRERFRc5BL5mk/Zz8H2bs/Ds+bCceyz6McFRH5w4QaEREBeRuCqiZrqwGhhNSjjYiIiIhC43jkdlRXV/mvMGRE5IIhIp+YUCMias+692mwijjhVMiV3wMA1LuvhRh5CuSKZRDnXxHu6IiIiIjaJx/JNHHSmUBCEtC5K8Txp0QhKCJyx4QaEVE7JoafBGn3sWpnnwHA9s0Ql1wDcdQxekINNVWQK5Zp2yp7qRERERFFinLlzdEOgYjcMKFGRNRGycL9kHkbgYFDgY1/+6+4fbNXkXLHI8DePUD33kDxId/XX/wBcO7E5gqXiIiIiIio1WBCjYiojVLvu6HhSlIFktO8ioUpBujZV6tiNPk+Ny2zCdERERERERG1XlyejYioPZMS6ruveJYddYznvsl3Qk158L/hiYmIiIiIPCj3PhPtEIioHibUiIjaM1UFdmzxKBLdenvW8dNDTSQmhSsqIiIiInIjeh0R7RCIqB4m1IiI2iEx6jRtQ0qvcjH2Ys/KRs4OQERERERE5I6fkoiI2hlxwimQO/MAAHLVrx7HlKtv865violIXERERETtlQywerpyxyMRjISIgsUeakRE7U2/o4ADBdr23l3RjYWIiIiIAIfD7yExYHAEAyGiYDGhRkTUBkmLxbtw4FAgPgHi6GN9n1R/MQIiIiIiigyHLdoREFGIOOSTiKgt2rDKq0i5+X5AdUDExPo+x3w4+OvHxTcyMCIiIiLyYvX+MjRmwNFwXHhVFIIhomCwhxoRURska6u9yoTR6D+ZBkAE6KGmzHgB4rzLXHVPHde0AImIiIjIJd97Go60yf+G6NkvCsEQUTDYQ42IqA0SGVmQgSp07QkUeL5xE0NG+L9e994Q3XtDTUyCXLUCYsxFzRMoEREREQEpaV5Fxk65QHVt5GMhoqAwoUZE1BbVX5kzo4PHrjjhVMiCOZ51uvdu8LLKqeMA9k4jIiIial6WusRZdico198DYbfBkJoOVB+IblxE5BcTakREbVK9/mmdunru11R5nSGMpjDGQ0RERET+SHOJtlF6GKJbLwghohsQETWICTUiorbIoeqb4vhTIC680uOwXP5lpCMiIiIiIj/kknnahs0a3UCIKGhMqBERtUWqQ/vZqSuUa//tfbyizHO/z5Hhj4mIiIiIfDtQEO0IiChEXOWTiKgtstu1n6Ygh3HW1oQvFiIiIiLySx4ujHYIRNQITKgREbVFNov2MyY2uPp7vZdqJyIiIqLwkMWHoH6xELKqEuqsZ6IdDhE1Aod8EhG1QdJm0za40AARERFRiyDNhyHX/QVx/MlQn5kOlBRBfvaeRx1x6bVRio6IQsWEGhFRGyRnP6dtcIUoIiIiohZBvesabWPfHqCkyGcd0bNvBCMioqbgkE8iojZGqq4VPrF5bVDnKA+8FKZoiIiIiMid/P5z/wczsiMXCBE1CRNqRERtTVVlg1XEMaM897v2DFc0RERERO2edC4Y1ZCU1PAGQkTNhgk1IqK2piSIlaKGHh/+OIiIiIhIU14aVDXB+W+JWg0m1IiI2gBZXQXH43dC5m0Eig/p5eLq23zWF8Gu/klERERETVdS3GAV5YZpEQiEiJoLFyUgImoD1Nv+qf185l6IM8br5cqo03yfYHL79rN3/3CGRkRERNRuyYoyqP99BCgrabgyRxAQtSpMqBERtTHy20UNV0pM1jeVm2aEMRoiIiKi9kVKCfnz1xA5uZCrfgV25QV1nlA4gIyoNWFCjYioPeqQ49pOSvZfj4iIiIhCs3kN5LuvQgZRVVx9K+TvP0K54Z6wh0VEzYsJNSKidkgkp0C54xEgJhZCiGiHQ0RERNRmyP0FQdVTZrwA0b03MOr0MEdEROHAhBoRUTslBgyOdghEREREbY8Mpm8atGQaEbVaHKRNRNSKyJIiqCuWQdptrjKrJYoREREREZGH/B0NVhFT7oxAIEQUTuyhRkTUiqgP3grUVAHmw8C4CVrZ2y9HOSoiIiIicpK/Lfd7TLnzcYgjjopcMEQUNkyoERG1JjVVAAC5bAnU3J448MDbQc/TQURERESRJ04fDzFwiPaerd/AaIdDRM2ECTUiotaoogzqK49ADVBFmf5cxMIhIiIiIo049kTIv34BACg33gscfSyE0QRx1DFRjoyImhMTakREbYjywEtAajpgioGIT4h2OEREREQ6abMBkBCmmGiHElZS1n3leeRQiGEnRDcYIgobJtSIiNoQ0bVntEMgIiIi8iJVFerUiwAAytNvQaRnRjmiMFr1q/Zz09/RjYOIwoqrfBIREREREVF42VwrlKt3XxPFQIiImgcTakREbYQ4ZWy0QyAiIiLyTTqiHQERUbNiQo2IqI0Q4yZGOwQiIiJqB+S+fMgdW/wfl9K70NH4hJrP6xERRRkTakRErYSsqQ54XKSkRSYQIiIiatfUB2+G+uTdkGWlXsccz98P9brxkHt36WVSVYEDBR713I8HvNf3n0O9bjwcU85rWtAR4p78E+Mvi2IkRBRuTKgREbUWVku0IyAiIqJ2Tqqqa6ekWCvbtwfqnBcgLRZg81oAgPrQbZBSQu7dBfX686E+Nc3jOupDt0Hu29Pw/ebNar7gI0DOfUXfFgOHRTESIgo3JtSIiNoAMfn/oh0CERERtQf5O9x2JKSUUB+8BfK3H6DefIln3bV/QH3oNr+XUh+8BXLN71BX/gDHv6+AtFkD3lpWVTQh8PCTUkL+8q2roEff6AVDRGHHhBoRUWvhsPs9pIwYHcFAiIiIqL1S33zRtZOUAvXuSf7r/u+xhq/3v8cg33wBqCyHOvVij2OOlx/2rFxdFUqoYSOlhCzYBVlv9IBc+b3HvhAikmERUYQZox0AEREFye4/oUZEREQUEe5zoSkGwHy4WS8vpYQQAuqXHwPr//I82ELeC8kfvtCHohreWOwqf+ulaIVERFHAhBoRUWvhZ3Us5YqpEQ6EiIiICABCWH2z75EQo84ANq6G/PNnv9XUmy4B/A39tLWM+WTd53VzTJsMHC6EcscjHnWU1z6JdFhEFGFMqBERtRJy63qvss7vfY3CWiuXkyciIqKwq/9+Q331iaDPNdz9pHaNkadCHDMSUpWQs572rhhgHjW5eS1Et95B37O5ya0bIOv3mjtcCABQn7/fo1gY+VGbqK3jHGpERK2FyeRVpCQlRyEQIiIiao/k9194FngsUFAno0PAawghII4ZBdH7iKDuqTw1x3X/j96GtNuCOi8c1GenQ37NnmdEpGHanIiolRDJqdrAiqwcICEJIiUNwhQT7bCIiIioDZPlZlRv3wj7k9MC1hNX3gxUlEEZewkcU87zPHbxNd4npLsSb8qsRVCvG+99zWvvgMjI8izcnw9EsZdaMJT7X4x2CEQUAUyoERG1EtJSq22kd4By52NQFHYyJiIiovBy3PEvBLPsgHLSmfq2GHsp5NKFrv3RY7zqCyE8JvT3On7y2VCOP9n7QIgLE0gpgYJdQOeuEEbv3v7NTXn1I37hSdROMKFGRNRa7Nyq/dy3h8uwExERUYslzjxfT6gFSpoFvMbwk/Rt5T8vQ33oVm3HzyJN9am/fKutQFpRDvn9502KBQBkVaUrttPOhTQfhnL1rYDdDvXBWyCGHg/xz+sh+IUnUbvBhBoRUSshv6t7E1hVEd1AiIiIiNyIf5zluZ+YBGXWopC+AFQenwW5ZR1QWQGUFAF9B7qul9sD6NxNG+4Z5Bxq8p3/epU5ppznM6kmpWwwVvX2y1zxjJsAJSlF3zc8+05QMRFR28KEGhERERERETWK8uRsiMxsr/JQe9OLrByIrBz/FZzDNR0ND/l03H+j32Pqiu+gjDrdVfeua7SebACUm+6D+r/HoDz0CkTnbnodaauXxEvkolBExIQaEVGrIPfujnYIRERERAAA5aUPgN3bgf6DIBRDZG7qTNDV1gSsJivKgIP7/FeoS8zJ0sPA3l16Mg0A1P89pv38z80wvLEYsqQI6j3XQoy/zOMSnHqDiAAm1IiIWixZeABywyqIk86C+tQ90Q6HiIiI2jlx8tkQF10FEZcAHDkksjffsx0AoM58GoZjT/RbTb3jX4GvU3QA6vuvQy5fGrCaNJdAvedabXvRB3q5mDA5yICJqK3jjIlERC2IrCyH3LwWUkqo998IOW8W1KkXeXwbq9zxSBQjJCIiovZKufxGLZkWZTLIhQkAQIwYDcV9jjPF0GAyDQDUu672Wa6cfl7Q9yaito091IiIWhD1nmsBqwXi2jsAVfVdqWPnyAZFREStiiw6CGRkQRgiNBSPKMLUGy7wWlxAOhxQb7jAo0y5/wWIbr096336btjjI6L2gQk1IqIWQjocgNWibc953n/FlLTIBERERK2O494pQPEhAPC5miFRYxnueSq6AWRkaat/1pGqCtisQHEh1DeeARTPwVfK9Oe8kmm+iAuvghhyHJCVA/XGiwJXzu3ZqNCJqG1iQo2IqKU4uDeoasK5yhUREZEbuepXPZkGAI4p50F58X0IrkgYFVJKAG1nAntZVYloPhJx0hkec5mp158f+ASl4WjFv6ZC+ccY1ynTnob65N2u/QdeArp0AyorgNg4iNi4kOMmoraLCTUiohZC7t4e7RCIqJ3btGkTFi9ejF27dqG0tBR33nknjjvuOP24lBILFy7EsmXLUFVVhf79+2Py5Mno1KmTXqeyshJvvvkmVq1aBSEERowYgWuuuQZxca4Ponv27MGcOXOwY8cOpKSkYMyYMRg/frxHLCtXrsSCBQtQVFSEnJwcXH755Rg2bFj4G6EVU19/0rvs9suhzFoEFOwE0rMgklOiEFn7pL7+JHBoP5QZz7faL8Pk/nzXjt0avUAAyL9/D+2EnK4eu+L4kyF/W+5ZlpLuud+7P8Sl10JuXQ9l6r2uFUw5OoCIfOCiBERELYTI6NBgHeXGaRGIhIjaK4vFgh49euDaa6/1eXzRokX48ssvMWXKFDz++OOIjY3FY489BqvV9UH75ZdfRkFBAWbMmIFp06Zh8+bNmDlzpn68uroajz76KDp06IAnn3wSV1xxBT788EN89913ep2tW7fipZdewqmnnoqnnnoKw4cPxzPPPIP8/HyQN/WN5+CY4n+idPnHT1Af+TfUO66AzN8ZwcjaudUrgX17IN/5b7QjaTR13ix9Www9IYqRAMjfEVJ1ERvrsa9ceweUl+d71hkywus85YzxMNw8w5VMIyLygwk1IqIok3Y75OqVUJfMa7hytN/MElGbNnToUEycONGjV5qTlBJLly7FhRdeiOHDh6N79+64+eabUVpaij///BMAsHfvXqxZswY33HAD+vbti/79+2PSpEn49ddfUVJSAgD45ZdfYLfbMXXqVHTt2hWjRo3C2Wefjc8//1y/19KlSzFkyBCcd955yM3NxcSJE9GrVy989dVXkWmIVkRKCfnHj4HrzH5O31YfuT3MEbU+sqpCH54ZluvX6xXVqpSb9c2oL3IRxNBZ5c7HIU46E8qLH/g8LuIToEx7Wtu+7PpmDY+I2h8O+SQiijL59SeQn70XVN22Mg8LEbU+hYWFMJvNOProo/WyhIQE9OnTB3l5eRg1ahTy8vKQmJiI3r1dE4EPGjQIQghs374dxx13HPLy8jBgwAAYja63oYMHD8aiRYtQWVmJpKQk5OXlYdy4cR73Hzx4sJ6488Vms8Fms+n7QgjEx8fr283Nec2ovy5XV4VWv99RgNUCx0sPQnTIgXLNbdF/DCFo7nZX1/4B9b+PAAAM9z0H0bOfz3rSaoGIifV5LBi+4pWV5UC5GaJzt0ZfN5yklIDbkM9oP08Mz78Lx7+v8HvcOHuJttF/UMDriD4DoDjrtmAt5jWmnWG7R0drbXcm1IiIokCaS6A+fBsQFw+UHg7qHOX6uxuuREQUJmazGQCQmprqUZ6amqofM5vNSEnxnKPLYDAgKSnJo052drZHnbS0NP2Ys26g+/jy6aef4qOPPtL3e/bsiaeeegpZWVlBPsLGycnJCev1/ZFSourbxSh782WvYzmvf4SKxfNQtfRjr2Npo8+E+aZLtGvkbUT6Geci/rgTwx5vc2uudi+Y/Ii+7Xjs/9D1i7+86tSu+QNF902FsWtPdHr9w+CvXffT2K0XDC/+B8nnX4b4E06GWl0Jy4a/UfzQvwEAWY++grihxzfpcYSDWl2FfW770Xqu6zp1gvxsJfae791bP/2W+5DkNpdjWxL1dm+n2O7R0dranQk1IqIokEvmAxVl2n+BHDUMyhU3ASVFEH2PjExwRESt0AUXXODRq835LXdRURHsdnuz308IgZycHBw8eDCswwX9Uf/4CeqsZ3weKyovh+yY6/NY2eb1HvvFD93u6tnTCoS73ffv3w8hBNQfvoD6/usex+wFu1BwzrFAbByM/wucWHOPzZ6/E3YAlg2rodxyv94jzqloxs0t8ncg671HidZz3Utyqtf7p/KjR6DiwIEoBRQe0X6Naa/Y7tHRktrdaDQG/WUcE2pERNFgC26lLOWW+7VJcTPD28OCiKghzl5kZWVlSE93rYxXVlaGHj166HXKy8s9znM4HKisrNTPT0tL8+pp5tx3r1NW5vmBuaysTD/ui8lkgsnkeyXFsM6PJWV0Emp+kmkAIBOTgeNPBt56yfvYL996l7XCD43hand1xTIoo07zSqZ5sNQGvLf6yTuQX3r3DgTglUxzauixqO/8F/KXbyFOOBXKpNsD1m0u6g9L9e3O875DYVVNi3iuKE+/pc3tlpIGGAx68rwlxBYO0XqNae/Y7tHR2tqdixIQEUWDVBuuk9GBK0wRUYuRnZ2NtLQ0rF/v6uFUXV2N7du3o18/bd6pfv36oaqqCjt3ulaS3LBhA6SU6NOnj15n8+bNHr3G1q1bh86dOyMpKUmv434fZ52+ffuG7fG1VFJ1QG5eC1lbrX3QKD0M6TZRvG7AYH1TxMRCKAYoj8/yrufrHls3NFO0bcDeXY0+VR4uhPrGs36TaUFdo3A/5MF9nmVVFXoiVK78HnLXtkZfP6gYNq+FY8p5kItdE/sbUtLCes9QCKMRIqOD9rOVzbdERG0Le6gREUVBwBW/OnYBEhKhTL4jYvEQEQFAbW0tDh48qO8XFhZi9+7dSEpKQocOHTB27Fh88skn6NSpE7KzszF//nykp6dj+PDhAIDc3FwMGTIEM2fOxJQpU2C32/Hmm29i5MiRyMjIAACceOKJ+PDDD/H6669j/PjxKCgowJdffomrrrpKv+/YsWPx4IMPYsmSJRg2bBhWrFiBHTt24Lrrrotsg7QAcsUyyLmvAH0GQBx1DORn70GMGO1RR7n5fojBwyGtFsAUo5eLrODmolGfnQ7DG4ubNe5Wq9cRQVVzPHI7DPe/6FGmTpvc6Ns6/u9KKFdMhfrq43qZMmuRljA6UOBZubw06OtK1QHI4FfolKoD6vP3B319IqL2jAk1IqIWRrnnSYjk1IYrEhE1sx07duChhx7S9+fOnQsAGD16NG666SaMHz8eFosFM2fORHV1Nfr374/p06cjJsaVxLn11lsxZ84cPPzwwxBCYMSIEZg0aZJ+PCEhATNmzMCcOXMwbdo0JCcn46KLLsLpp5+u1zniiCNw6623Yv78+Zg3bx46deqEu+66C926tczVEMNFrvlNS6YBwPbNkNs3a+W//6jXUV6aB5GQCAC+V6FMTQfKfCRg+h4JbNvU7DG3RNLhALZvAnr0hYiNC1zZbaXYgPJ3NlwnkKQUiDPPh/xE+xtDudkjmQYA6nXjIYafBPnnzx7lsqQIwfTLkhYL1Ju1BSiQmQ3Dk7MD1zeXQL3raq9yw3Nzg7gbEVH7w4QaEVELoNw4DcjMBiy1TKYRUdQMHDgQCxcu9HtcCIEJEyZgwoQJfuskJSXhtttuC3if7t274+GHHw5Y54QTTsAJJ3iv5tdeSIsF6v8eb7CeM5nmj/J/j0H95B0o51wKVJRB7tsDcdaFEELAMeW85gq3RZOfvgv59SdAdicYHpupl6srvvOubLNCWiw+r6Pc+h+oL7sSznLVr0BmFkSPhociK3c+DnHEUXqbK/c9BxwuQkMzBdVPpgGA/GAm5KBjITp0dJWVFAMpqRBGbR5Bx2P/B+x2Gxp6uBByze9QX3sCUFUo/34I4sihHtf1lUxTXnwfIinFq5yIiJhQIyKKju59gD3bAQDKAy8BuT04DwgRUTsiK8oAux0iPdN3hdKiZrmP6JQLw033ufYHHeuznvr7j1DqDSVtK+TXn2gbhZ6rQMq3X/aubLcDh/b6vI4YdIzHvvr6k1r5JZMguvXyrJyWCXHx1ZArv4dy4hkQRxwFAFBmfgY4HBAmE+Sh/aE/GOe9750C5ak3gV1boS76QB8Wqtz7DNCzn2cyzXnO/x5zbb/wH21j0LFQrr/bZ29FDgMmIgqMCTUialGkqgJCtP3kkqKtCSNOOxeia88oB0NERJEkpYR6x78AeA7ZdKc+dU/Y41AenwV1ujYvnZz9HFAvoSYryvQ4AUD57wKIuPiwxxVOjvtvBA7ug/LyfM8D/Y4C8jZAzp8FnDrO6zxl+rPazzse8ZpjTH74pkdPM2X6s9rwUiG82lQoiv4eALU1rnOmTvca8ulx3iljId1W3QQA9Z5JXvXUJ+7yew2f1v8F9eZLvYqV6c+Fdh0ionaICTUiajGk3Q71xgsBaN/gCqUNL0S8Kw8AIJctASZOiXIwREQUUVbXkEL50VsQV97sXaeyQt9UHvwvUG6G3LcbcsEcV/mN9zYtjszsgIfdk2kAoN4yofX3WqpbQVO9daJHsUhN15Ni8vvPXQcGDIZy5gUQPbWVbOGvR6G77r2D+2Iw1m3Ou75HehxytrOsrgQMRojYOMhTz4V6/40NX7eJlFvuh+jZ/lbUJSIKFRNqRNRy1A2BBAD1gZtgePS1KAZDRETU/ByvPwms+tVVkJrhVUfaPSfGF126A126QwwYDHnqOKjXXwAx/CSIYU2bY67+F1fycBFEZhbk1vWQm9Y26dqtiXLjvZB//eLzmOGOR+pVbmC1zMxsiIbqOB05FOLsi7Q52OJdvRTF6DGu7YQkV/3sHCC9A1BaHNz1oSXmZEUZ5M/fQJx+nr5whbTZoE69yKu+8sirEDm5QV+fiKg9Y0KNiFoO90lvD+2LXhxhJlU12iEQEVGEqQvnQH67yPtAZpZ3WYDVN4ViaNZeYmLkaZC/LgMAqNOuheGNxVCfva+Bs9qYocdD/vx1cHUNgZNlyn3PB31boSgQF17lXX7Bv3zU1n73ylNzvBaUUO57DnA4oD55t2f9kadpP5NTIcZe4nnMZILy4geApQYiw8dzkIiIGsSEGhGFhTNpFNKwzfaSaHKbM4WIiNoHn8k0APKHL+B457/AgMGu3lBuQ0KV+18Mb1x1yTQnx/1Tw3q/lkgIAWxYHVzl2DjXeRdeBfnJO57H4xs/x5zy3FzAZoNITPZbxzmUVJnxPNRZz0K54xGIuqSsMvNTCMUAKaU2tDWnS8D7icQkIDEpYB0iIvKvDU9QRETRIlUH1Idvg/rk3dqbuvrHV6+E+u6r3sccdq/rBHW/HVvgmHIe1N9/bHTMEVXlNi/O1OlRDISIiCIhYM/k/J3az81rIeuGgqqvPKof9lo9MtwOeq9wKS5y9aKSPo63Zsqd/hcCUJ6c41UmklIgzrkUYtxEn/OMCaOp0bGIlDQ9OdZg3e59YHjsdY/6zqGmQgiITrltf4EnIqIoY0KNiJpf0SFg3x5t4v1688AAgPraE5A/fQX1uvEAAFlRDscd/4L60K0e9eRfK4K6nXOIg5zdOlakkit/cO0MHBq9QIiIKDIstUFVU19/MsyBNI74x1n6tvwpyKGRrYQ44ij/x/wkt5Tzr4Ay/jKgzwDP8jD3JiQiopaFCTUianZy51a3Pe3bUceU8+CYch7k1g2edW02qPddD1SUeV/njWe1c+p6srWVucfkknn6tnNyYCIiarvqrygZiPvcWEhJa/5g6hH/vM7/wYFDoTz9lufE+D1a9+qP4mq3L+86dGzatYwmKP9dAGXG81BmLYp8b0IiIooqJtSIqNnJD9/02FeXuZafV5/1HOIov1sE1FQFvt7Xn2hDOq8/H3KVZ681WVvtuR/kMNFokSXBr8xFREStj+OBm7Qvg/bu8nlcmfY0DG8shvLkbKCBIXmG5+aGI0QPYuSpQHZn7wN9j4Th9ocg0jO1/SMGhT2WsDMYIU44BeLKm4HszlAem+m3qvLsO36PuRNx8RDd+3B4JRFRO8SEGhE1v3q9zeT8WX6ryk8a/rAgP3a9qVVff8pVrjqg3lLvW3+b9xDTaJDlZjievQ9y9zaPcvWeSVGKiIiIwk2qKnCgAACgPnQbpN0OmbcR4qQz9Tqid3/tZ2Y2DLMWQXntY221xSgRcQlQHn0Nyj1PQpx0JsTFVwNHDYNy+0OeFSvLtZ8+epS3ZNJtuK3y2OvaSpknnanNP+a2cJLyxBueJ7qvPE5EROQDV/kkolZHSgkhBNTrL/A+aLV6rMAVLer/Xan9fOz/oMxaBNisXsM7lWlPRyM0IiIKl3q9pNUbL/TYF2df7HWKMJoAXxPZ9z+6WUMLRAgB9DkSos+RWsFZF3pX2rcHACDnvwGcdm7EYmuy0sOu7bgE//XcEmjKTdMhDIYwBkVERG0BE2pEFF6O4HuMKU+/BcQnQK79I+ACA87FDHyyWUKJLiKc8Yp6H1CcvRSIiKiNqKkJeFju2eH3mHL/i1AfuV3b6dIdhv971G/daFMXvQ9l/OXRDiM4bokxkZjkt5qIi4c47VzAYYcYcnwkIiMiolaOCTUialayxnNOM68hmQE452kRI0YDI0ZDblkH9bkZod3/w7cgrr87pHOakzr/DchlS3wek19/EuFoiIgokuS6PwJX2PS330OiWy8or34EGE0tfj4u+fkCyBEnQ+R0iXYoDbNatZ9BDOFUJk4JczBERNSWcA41ImpW6h3/arCOcudj3oW+VjKrW90TAMTlN/i/3uufuk756xd9VdBo8JdMIyKitk++80rA48rj/ucUBQBhimnxyTQn9f4bvRYGapHMdUM+nXPAERERNRMm1IioyeT+fJR/PBf2yecCdv9DPJVpT2uTHvtYKUy5+X7vE/ofDeTkAoOPg3LyWN/X/O9873lOagMPuQmXUBJ5ymsfhzESIiKKCqn6LFauvxvKzM8gsnIiHFDzEedd5lUmP3wrCpGERu7bHe0QiIiojeKQTyJqMscDNyGYNb885gzrNxDI26ht9z8a6NHHu74QMDzyqt/rKf9+GMLXBMM1VUB8gImH65GqA8jfCXTt1bRJiG3WoKoZ3ljc+HsQEVGrIo4bDXHsidEOo+mSkr2K5E9fA/+6KQrBNEzabVBvvMhVkN4hesEQEVGbxIQaEUWFMvlOqHdfA2R0aPzEy7k9XNf7z8tQH7pV27EFtxCC+sMXgNEE+eNXwJ7tAJqY7Dq4T98UV98K7N0Ncck1gNUK9bUnoYy5EGLA4MZfn4iIWp8OHaMdQfOIT/Quy8iKfBxBkKoK9d9XeBaWFkcnGCIiarOYUCOiiBBX3uy5n54ZcvJKeeRVyIJdwP4C7Rpu866J3B7ahMOV5YDd3uC1ZFUF5AczvcrVt1+CcvVtIcWln+tcnQ2AMup014G4eBj+/VCjrklERK2D3LpB31aenA112mQAgDjrgmiF1KzEsSdCznnes7CkKDrB+CGl9LsSuLguegsWERFR28SEGhGFV3IqlPueg8jMbvKlRE4uRE6u/wrGupc0R+AealJKqE/f6/vYimWQ50zwmOfG8cBNwAEtiafc+RjUWc9AeXwWRGyc6zyrJchHQUREbY1c9yfU/z7iKkhJhzJrESAlhNI2piwWRiOUGc9D7syD/OD1sN9PqmrIbSc/e99nufLwqxCdArx/ICIiagQm1IgoLJRHXoPcvQ3iuJMglCbMSxYKQ91LWkM91PJ3APvz/R+vKAPqEmrSZtWTaQCgPnuf9vPmS2F4Y7GWSNu8FnL39iaFTkRErZP9qWnAto0eZcJkqttoHSt2Bkt07wPRvQ/U6krIz96DGH5Ss99Dlpuh/t+V+r4y7WnPOVgDiY/3KlJe+gAiIam5wiMiItIxoUZEzctogvLEGxBpGRA5XSJ778OFAAD523KIXkf4raY+ekfg65SXQv1tuffQlnqkxQL15ku8ysW5/2w4ViIiatWk6kDt2r+8k2kXXxOliCLImTAMQ+87+ck7Hvvqk3dDXH2r51QKzrp2G7DuT2DQsRCmGK+FB7gIEBERhRMTakTUrAyvfRztECB/+AK47PqQzlFumAb19Se1HVU2mEwD4DOZBgDKeUyoERG1ZbLoINTp18HXDGLixDMiHk/EORNpUjb7peWKZd5lb78M2edIID4eIjVDK6uphnrLBL2OuPxGoLZa2xl0LAy3PtDssREREbljQo2I2iS59g+Iwcd5lkkJ9eWHPcqUG6ZBHDPSo0x97Ymwx0dERK2X+s5/vcrECadCmXR75IOJBlGXUFPVJl9KqirkWy8BnbtCOftiv/XUGTdot575Gap/+Q6OJ6Z5Xuf911w7G1Y3OS4iIqKGtI1ZUikk8sBeOG6ZALl5bbRDobaibu4y5bb/RDkQF/WVR/VtabdD7tgC+ev3wIZVerm45navZJo/4phRXCGMiIgAAMrN93mXtZdkGqD3UJOy6Qk1bN8E+dsPkJ/Mhczf0XB982EcrpdM89IccRERETWAPdTaGXm4COoDUwEA6vP3Q7lxGsSw4BIKRL5IKQGHtgiAyO0Z3WDSOwClxfquXPUrZMFOyC8W+qwuhoxo8JLKTdMhhhyv7zu++lhb1MApMxvKpZOgvvYkxHH/gJhwbePjJyKiVkHEJcA4ewnsk8+NdijR4VxsIcghn7KqEtiyDjh6uGvBBieLa5Vs9ZF/u25x8lggpwvk/Dc8qjvuCeLf2WAXMSAiImoCJtTaGXWa55sQ9bUnoTz8P4hOXaMUUfsmt28ChBL86lUt0X7XCpjOxFrUxMZ67OpzovkTn9DwNeMTPXaV/3sE6m2Xadv3PAXRZwAATnxMRETtiBJ4yKe64juIhCSIodoXUurz9+tfRonxl0EZN9Gttu+knDj/Cv1L4IChvPSB/u+yk2Ha0w2eR0RE1FQhJ9Q2bdqExYsXY9euXSgtLcWdd96J445zzVMkpcTChQuxbNkyVFVVoX///pg8eTI6deqk16msrMSbb76JVatWQQiBESNG4JprrkFcXJxeZ8+ePZgzZw527NiBlJQUjBkzBuPHj/eIZeXKlViwYAGKioqQk5ODyy+/HMOGDWtMO7R5UlWBnVt9HlMfuAnKc3OhPnsfxNAToFxwRYSja5+kpRbqU9qQBeWVDyHqJYNaC/Wzd107qenRCwQADu4LqbpwfsNeR7nnKahP3eNZqe+RnuckJDF5RkREAIBOsz/DgcfuhjJhcrRDiSzhf1ECWXRQW0QA2pdN6vKlHj275aIPIM+6yNVTTfXTyy0uHsr9L0K962q/YSgP/hciIalxj4GIiKiJQp5DzWKxoEePHrj2Wt/drRctWoQvv/wSU6ZMweOPP47Y2Fg89thjsFqtep2XX34ZBQUFmDFjBqZNm4bNmzdj5syZ+vHq6mo8+uij6NChA5588klcccUV+PDDD/Hdd9/pdbZu3YqXXnoJp556Kp566ikMHz4czzzzDPLz80N9SG2elBLq9ed7JwrcqPdOAQ4UQC5dCLnm9whG147Zbfqm+vyMKAbSRG7PF2GKiWIgoRFjvVfoFH0GeCXLhGKIVEhERNTKGDvlwnjfc627p3ljOL+Q8tVDraxU35QV5ZDvv+5VRX1Rm3NVqg6orzzi+xYGA0RaBpCV4z8OZ0+5jA6uopmfBY6diIiomYTcQ23o0KEYOnSoz2NSSixduhQXXnghhg8fDgC4+eabMWXKFPz5558YNWoU9u7dizVr1uCJJ55A7969AQCTJk3CE088gX/961/IyMjAL7/8ArvdjqlTp8JoNKJr167YvXs3Pv/8c5x++ukAgKVLl2LIkCE477zzAAATJ07E+vXr8dVXX+G6665rVGO0WcWHGq5jdZu/Yt4sGIKYW6q9kFaLtlpUj74Qbm/Ymsy9d5Sf3oMUotwewN7dfg8r9z0H5PYESoogsjv5r/fyfMivPoE48/xmD5GIiKjVcyayfE7+79bjrGCn7/PzNkD9/nPIebN8X/4e15QNyqOvAaoK9caLvCsatC+9lCfnAA47hNHkXYeIiChMmnUOtcLCQpjNZhx99NF6WUJCAvr06YO8vDyMGjUKeXl5SExM1JNpADBo0CAIIbB9+3Ycd9xxyMvLw4ABA2A0usIbPHgwFi1ahMrKSiQlJSEvLw/jxo3zuP/gwYPx559/+o3PZrPBZnP1ChJCID4+Xt9ubs5rhuPaIXFLljmJwcdBrv3DZ3XlnEuA4kNw3DsFyMqB4fFZ0X8MQQpHm6vz34D8+RsA2pwczjmz3EkptTeNnbtDGIP9s/KM0VfM6t8rgTIzxOgxLfJ3IKurPPajHaPhvufheOoeYPc2r2Ni+ElQevbTdjp2DngdkZAIXPivcITYrFrMa0w7w3aPDrY7UQviHPLpb7hmHVlu9n/MRzJNeXwWRL0eaUIxAIpBmytt9vMQXXtCLv1QO5at/XsuhACYTCMioghr1oSa2WwGAKSmpnqUp6am6sfMZjNSUlI8jhsMBiQlJXnUyc7O9qiTlpamH3PWDXQfXz799FN89NFH+n7Pnj3x1FNPISsrK8hH2Dg5OQG6qoeRWluD4gdvh2X9Kq9jne97GgduuBhqSbHXsfROXXD43inaTtFBdLBWI6ZHn3CH26yas80L6pJpAOB48m50/eIvfV867BAGI0peegRV3ywCAOR+/mdQH/gcFWXYX7cde/SxsEw+F6nX3IKUi6+C/cBeVP3wJcrf14ZCZw0agrijWt78gJYNf6PQbT9az3V36rNzsO/if3iVd7rjPzCkpEU+oAhoCe3eHrHdo4PtTtQCBOqh5jYMVM553vNYv6OAvA3aKpw7tnhecvpzXsk0dyIhCYZbHwBUFTGlxbBwQS0iIoqydrXK5wUXXODRq82Z9CgqKoLd3vyrEwohkJOTg4MHD2o9mCLMMf8NSB/JNAA4WFIKecKpwBcLvY6VrPzRY//QTRNhnL0kLDE2t0i0+YEDBwAAjmfvg9yyzuv43nHDgeRUGF94z+81pJRAZbm+b1mnJenK3vovKiw2qB94zjdSNOt5GO99pjnCb1ay3PUY4k86I2rP9frEMaMgV60AEhKB6iqIf5yFwqoaoKom2qE1q2i/xrRXbPfoaEntbjQaw/5lHFGLFmAONfWZ6T5PUaY9DVSWQ83b4JVMAwCYguthJgwGdJjxDA4cOBD11wIiImrfmjWh5uxFVlZWhvR012p/ZWVl6NGjh16n3O1DOAA4HA5UVlbq56elpXn1NHPuu9cpKyvzqFNWVqYf98VkMsHk5x/rcP6DLKWM+D/4UnVAfhdgJUKDAWLcREgfCTXp1iNLL2tlb1jC2ebqji0QvY7wmUzTVZQFvL/6xrOQf/zk+9gH3pP3wmIJeD0pJdSn7wW2b4K48EooZ1/sP7Zm5Phkrr6defdjOHjoUIt4rig33APpcEAYXAsKtIS4wiUarzHEdo8WtjtR9AlF0WZKC2V17e69IVf+4P94K1rYiIiICGjEKp+BZGdnIy0tDevXr9fLqqursX37dvTrp81d1K9fP1RVVWHnTtckpRs2bICUEn369NHrbN682aPX2Lp169C5c2ckJSXpddzv46zTt2/f5nxIrYJUVcg92yHr2ktaLMCmtd4Vh53gsSuMRihB9nqSwSxs0F44HI0+VRYegPrhW36TaX7t3eW6RlUlZE215/GdW4Htm7Tjn8yFrPBMNjc39bcf4JhyHrDZ9TwTSrO+nDSZezKNiIiImo90ruRZVgLpa6VPH4TRFHBBIMQnNENkREREkRNyD7Xa2locPHhQ3y8sLMTu3buRlJSEDh06YOzYsfjkk0/QqVMnZGdnY/78+UhPT9dX/czNzcWQIUMwc+ZMTJkyBXa7HW+++SZGjhyJjIwMAMCJJ56IDz/8EK+//jrGjx+PgoICfPnll7jqqqv0+44dOxYPPvgglixZgmHDhmHFihXYsWNHu1zhUy5bArlwDsQp50CmpEEueh/i+FM86ih3PQ7R7yhIu91j0nzR64ig7qHeOwWGNwL0eGtPguwZoa5YBmXUaZ5l913f6Nuq338OkdMF6gv/0cv030lZiWflshIg2XOOweYi7TbIOS+E5dpERETUChxy65lWUgR06BjceT7mSFNu/Q9gs0K00XlOiYio7Qo5obZjxw489NBD+v7cudqQr9GjR+Omm27C+PHjYbFYMHPmTFRXV6N///6YPn06YmJc3bhvvfVWzJkzBw8//DCEEBgxYgQmTZqkH09ISMCMGTMwZ84cTJs2DcnJybjoootw+umn63WOOOII3HrrrZg/fz7mzZuHTp064a677kK3bt0a1RCtlTSXQC6co23/8IWr/DdXl3rltY/1ZcSDX4ESQGqGd6KmDZPVlRAJSQ1XtFuDu97bLwH1EmqhEKeMhTy4T+8FJufNQv1UnmPKeVD+8xLU1570PFBbG1yMUkK9bjwAQLnrCYh+AwPXryjThpbWY/jfh0Hdj4iIiNqA1AzXdrkZ6h8/QeT2gDh6uJZc8zOyQWRkAYOOBda7FngSg44Jd7RERERhEXJCbeDAgVi40HveLSchBCZMmIAJEyb4rZOUlITbbrst4H26d++Ohx9+OGCdE044ASeccELAOm2ZtFig3nV1g/VEA8uIK7c9CPWjt6BcdSvknz8BDgeUf14HqTqgXn9BM0XbsqnfLYZcMBti4hQop52rl0tfk+ZarZBWi8/riDEXQX71set8mxWibk4QqQYeKqpcfzcwbCTU68/Xr4VtmyA3+xi+6x77Q95/S+pT90CZ+VnAYZjqvFmQ33/u2n/mXiizFgF//wZ18QdQZjzv9dxR7/iXd9zPvAURGxcwRiIiImqbZP4OyE/fhURdz3m7LWB9w60PaNNGEBERtXLtapXPtkb+/WvDlTKzG6wijhoGw1HDtO2erjnohOI5B5UsPQyRnhlakK2EXDBb+zn/DcAtoaY+ebd3XZsNWPunz+soF10Fh1tCTZ2qLQ6gPD4LqPVeYVIcfwrknz9BjL8c4tgTtbrPvgPYbRAZWZCFBxr9mNTrz4fhjcWQ5aXAnp1QX9Z6liovz4eIT/BIpunn1PVWAwD1xou0+q9+DGEy+ZwjhcOAiYiI2iHpek8g36+3mJIzoZaYDFRV+DxdTP4/yNnPacM9iYiIWikm1FooqapaMiYlDYan3/RdqSiIhQIOFzYpDnHeZZCLPwAAqP97DIYZz9eL0wG5cjlgtUAcOwoiTPN2RZL66zKgpgbKaeM8D+R00Vaz2rkV8DGprnLzDACAOPtiyC8/8rzmK48C+/P1fXHFVIjjT4GIjYWcdDuEc/l5ACI13e1Et15tAYZQANBiqpeA8/UNsHrrRP/X8EGdepHPcjHh2pCuQ0RERG2DOOE0yEUf+D5YqSXRlOnPQv62HHLJPIizPd9LKCNGAyNGhztMIiKisGJCraU6UAA47EBpMWRJkTbnhBupOvREFwCI40ZDblytJX3chimKM89vUhjimJGu++zZ7nXcfUio/OB1iH9NhfKPMU26Z7TJt14CAKhuCTAAQEo6cHAf5HeLvM4Ro8dADD5O2+5/tFdCDRbPOc2U0a42ck+mebG55mtTbrgH6qN3uPZf+0SbN29XHsTYSyBye0D9+hPIj94O9PCahXLbgxB1vRqJiIiofRGZWT7LpcVtSgy7Dcp5/wTO+2eEoiIiIoosJtRaILl5LdTn73ftr/0T4pSxnpXqDR9Upvyfq/6mNVBfeEArv2QSmiQnV98UJ58NKSWEENp8YDbvOTLku68CrTyh5iR/+krfVm66D+q33ok0wHvYoyw3e1dqZE9Bcco5kD9/AzHydI/kmhh5GoTRCHHGeM/6g44NOaFmeGMx1C8WQn72HpQbpgEdOwOlh6H+8i2w2vewYibTiIiIqD715ktcO8Gu/ElERNRKMaHWgsjyUshlX0Au9Vz0QWR5vyGR61f5vY44cgiU1z4JbUVPf9dSFGDYCcDqlZDLvwS69IA4+Wz/ixW01UTL0cOB73zMF9a9j3eZLfAqoMpzc4O+rcjMhvL8exAGA6TZteKquOwG3/U7d4NyxyOQh/ZDvv+a64DRpM134vC9MIJyzqXAOZe6CnJ7wDDoGDgeuAk4UADlsZlAagZEbGzQsRMREVH7JWL4noGIiNo2JtRaEPX/rvJ9wOGALDoIpKS5VlMsKdYPC7dJ9PWyZkim6dwm4JfvvwY57Hj/dTesbr77RoiUssE6QlGAreu9D/gYBivSMxHoiiIlLfjgAAiDtjiESMuAOOtCID4hYGJLDBgMMWAwHN9/DhwogLjseiinnANZVQH523KI4ScB2zdD/fgdKHc+FvDehof/F1KsRERERERERO0BE2othKyq9HtMfeVRfds5vFBuXqOXKROnhC0uANpcbu7x+Ev8tVbSe/XKYInr7vIu7NzNdfzCqyA/eafR169Pufjq4Os+9Iq2WERdElYkJruSr8NOgGHYCc0WFxEREREREWn2HzBjxcoKnDeuE2JjmHZpq5RoB0B16k1a74/6xrPaxua1YQwmdOLEM/RtWbg/ipE0ghq4h5oyy/fcaQCgDD/Jq0xkZEG5/0UoT7wBZHTwPJiU0qgQG0MI4erRSERERERERBGx6icgzpaMbz7133GGWj8m1FqKogNBVZN//OS5glILIS6/Ud+Wu7ZFMZJGaKCHWsBVOP2d060XRIeOEMNPdJWdfTGUZ94K+VpERERErYk4lyt7ElHrc2CfGYvnl2LmC5tQcrjxiTC7zd5wJWoTmFCLAmmzQtabHF599j6PfXH+FX7Pd19BSbnjkeYNzgcxbqLfY8rz78HwxmKPOdv8LaXeYtXroaa89olr+7YH/Z7mc7hn/TqKAYY3FsPwxmIoF14JYTQ1OkwiIiKiFm/AYIhz/b93JCJqqf76xbX9y7e2kM8vr7Rh5eqDKK2o8Si32hs/xRC1bEyoRZi026BOvRjqDRdA7tvjs47hjcVQzrkUyn8XQIwYHfB6YsDgcITpeY+zzgeGjfQuv+4uiGS3IYydumo//awk2WK59VATJ4+FMBq1IZu33A8RYNVSceyJfo8RERERtUeGOx5pVO9+IqJoslXWNFypAV99XoHibXH47VvPz8Nff1ze5GtTy8TZ8SKt+JC+qT54C5RXFgLbNwNDjgfW/OaxYqeIi4eY/H/A5P+D3LEF6pN3RyNiiLgEGG6cBlleChQdgiwpBgr3eyeUig5qP6sqIh9kU+zdrW+Kf16n/ezWC+jWy6OactuDUF960FWXbxaJiIiIiIhavb3LVgE4yqOsutqChITYoM7/cMEhxIvAdQsKLfj5jwokVxkxckwiMlObb/TSkgVmffuM8xIQFx/TbNcm/5hQizSDZ5OrN1/qeTyrk+/zevbzKhKT/t1cUQVFpKQDKekQvf1UsGvdYtW3XoLBR4+2lko6E4EAhBKg02Zikr7JudCIiIiIAOTkAgf3RjsKIqIm2VCTBtTrL/H9lwUYd1Efr7oV5VpvtuSUeACAzWZHHAIn05wJr+S6FMyvX1UhbYCCk45u+qJ1v3y7CUBnff/bxdUYMLQUffp1bPK1KTAO+Yy0ygZ6b9VU+SwWigLl7idd+xdeBeWEU5ozsuZTWwP1l2+jHUXQRLafJGZ9PfpCnHgGxPjLIdIywxsUERERUWsQ5MJaREQtUXVFpZbsErlex6S9g1eZzWbH8i8tWP6lBRaL1qHkr98LGnVv82YVSxaY8fUfZr91VNX3/GsWiw1LFpixZIEZpSWdvY5v/jsWPy7b0ai4vGJwOHBo5SbUFvuPs71iD7UIk8sWBz7++QJg3ASfx0TfI6G8+jGEqeVPbC/f+S/kyFMhFEO0Q2mYtW7V1M7dAlYTQkBcdUsEAiIiIiJqJY4cCqz/S9tOy4huLEREIVq2NPCKnEsWmJHVpRTHn9gTAFB62NUB5qtPSpGQWobaiqYtymfdBdiPUWE0aP2dDuwzeyyQ4OSQFYBUcP4/u6BgTwnQQK+48uLQOoHYbTZ8+UkVYtVinPlPV8+8r+cXwm7sDOQDgBnDOuajy8lHh3Tttoo91CJM/v5jwOPKs28HPN4akmlO6sO3Q0rZcMUok+VmbWN/flTjICIiImp1hIA46UwAgBLh6UiIiJrbbrXWq6xoXzoAYOOf2/H7j67Pt4oS45VMq5GuBQkOy+BXCi0u1xJ7P32/w2cyDQAMIhkGJRFLFpix+W/vZJoqAycH/fnzpy1YOK8QX36iJQstSgdU5BcBACrzD8FujPeov/pQ4I4o7Ql7qEWQ9NNdEwDE+MsgzrwAIia4SQ9bpMxs4HCha3/fHmD1r8Axo6IXUzA2rYl2BEREREStk5QQ/7oJ4qKrIdzmmyUiao1+VsvQQ4nzeWznTu8hoO4KVAu+VkvRXcQiDgq2yhpMNuboxysS7bh4TAY+XVaCBLNnKmZNXjWO62NFWVHjphYqKvsFlbU7YVQS0TXrIr28psaK+AYWKDh4IAfx9bpaLV9pAlaa0VAvuPaOPdQiyU9CTRx/CpRxE1t3Mg3QEmr1qMu/jEIgwZFSQv3zF8hfl2kFvftHNyAiIiKiVkgIwWQaEbVqWx2VmG0/CAt8j7DyN5eZu2WqGQCwR1qwVWoLF/xQtR6p/RWcc0kKLhvXATFGBRPO6oAxF6UguZ8rHWMpqMSK77x7mCmm4oD3NMQWIVX5HJW1OwEAdrUKx5/iuu53i6uxZIEZ69f4Xjzmz3dXNfi4fDm8ZnujzmtrmFCLJLvVZ7E4dVyEAwkPkZXjXZbbI/KBBEm++z/IWU+7CtxW+yQiIiKiIAjRcB0iohYqtUxLRO1VqwPW++LD8oDH37Lthd1HMk46yvCPwSlQFM/Ui8mo4OShKShTtCSa4vD8UiI9uwRjL07GORf2QYfOJT7vefwpCsae39er3GTynsd899Yk/Pid5yIF9qpaHIzpHfBx+bN1re/FFNsbJtQiyTn5PQBlmlsiJyNw19HWQpwx3qtM7tgShUgCU7/5FI5/XwH58zce5eIfZ0UpIiIiIiIiIoqUksOVKCosR1lqLwCAzW3ub+k2D1owpHK40XGkqr5n4Ro+sisMBi0xdsJJvXDuhDScOyENp5yjDUft2M2MrOwUn+f6SqgBQPnhTFRWuuaI2/r3vkbHfTiuO5YsMGPf8nVB9d5rqziHWgSp/3eVvi1694dy/wuA3Q6Rmh7FqJqP6NId4rjRkDu3AMWHtMJdedENqh71j58gP3zL+0ByKpTxl0c+ICIiIiIiIgo7c2kVNq47hJIiCTg85yqTbr3LYhPKYK3xv2rxmI7v4mDPy7DmNwOOGSXRObc33n5vg8+6oS7Rt7/kK0yafDFMJt+pmqSkOJw7IQ5Amt9rxMT6T/PsyCvG4GG5WPlLHor3eU7ZtMxSgBRjGoYbkj3Kj0zYjqQOCSjYXoMD9Xq0rT7UDcN+2tBuV/1kQi0CpOqA47rzvcpFt8Z1r2zJlCn/BwBw3H45UFUBMfykZr+HdDigvvIocKAAyp2PQXToGPy5bzzrVabcdB/EkBHNGSIRERFR29a5G7A/H+L4k6MdCRFRQA6HA3/+thlFe3MB+E6UVUg7nOmRY0/ogl+/r/F7PYNiR9fumejavfljtdgK/SbT/JH10nbu5zvUKhiURH2/uFAbNVc/mbbSUY5M6yH8ZTAhtqYct4xMRUq/3LqjxwIAOp4ALFlg9rr/gQIbuoQUcdvBhFqYqdVVXsk0ceb5Puu2JWLU6ZDffBqW4axy+ZfABm3yRPXeKQAAwxuLg4tr/GWQiz5w7V9+I5NpRERERCFS7n0GOLgX6N4n2qEQEQW09KMKALkB61jhgDM9MnBQdyjGHUhJias715NBqZ/CarwSWwUyTFqPsM4HVmBXM03Kde6EtLoVPtNQW2PFt4u1OeKK4IDF6j2kdaOsxrF1w17zHWVI6Tcw6HvV77XWnnAOtTCRUsL+739h3yWjvY6JE06NQkQR5pygNgzjqeX8WV5ljv8+4rOu+vuPcEw5D3J/vlZgcq2kqrzwHpSTz272+IiIiIjaOhEXD9GjLwQXJSCiVu60H2/yKsvISILBYMAxfz/mdUxRQptjLZBDNrO+ffSm2c12XQCIj48BAMTFx8Aitc/l0pyJrz7xXGChsOh3AID+as7X9aAxoRYm8s0XgQqz94Ehx7folS+bjXMVE9k8uXvZUGJu3Z9QZz0DdcUyrb7dDrn2D8jZzwEA1P/cDMdzMyB//BIAIE46EyLJ9ySORERERERE1LY51BqMWXYNYm0Vfic7U1GJk1f8X9hiOOgoR7G0IWXvt4AM3+T+h6UNAJAkDFDcEmabKrfDHmev22v4s/uQzD3hCK/VYkItTGR5qVeZMmsRDDdNj0I0USCaL6EmK8uh3nU11HdfDVzvz5+hvvUiAMBxwwXaPGvutqwDig5qdX/9vslxERERERERUculOrx7kzlXzDz/n52g1K3oKdySSe49b4UAEmqKmhBB4N5eDqHiM8dhJB9aidCXMAhemvA921eVwztvESiKLqccha6ObTg6bZdH+cp31jQhutaLCbUwUW5/COh1hKsgM7t9dYlXnEM+m94dVv78LVBuhvzpK8hNfzdc325vsA4cQdQhIiIiIiKiVstRb76wo49rOGnl8bndR/VwfKqXYbmqS4IweJUtsR/2GN4ZTASKwYAhlw1H97OGIvfgL3p5cVwPHPjZ90qnbRkTamEihIBxutuKkocLoxdMNITYQ00WHYT601eQdpuvo/qW+sJ/9G3lpQ8gTjjFq/be8cc3eD/l+ruDiouIiIiIiIhap8L9np/Du/dM91lPNNA7bEj+O0iOLcXZ/eY1W2zafX1vR8IhOD97a3cWdZ/dg03udSpa5bH/1/7ACz+0RUyoUXjoPdS8X5iklFC/+hhyvesPUJ1+HeS7r0K98SKoyz73PMHP/GkiIQly5Q8BwxCX3wjl1Y+9y489sYEHQERERERERK3Z6r+S9O1B+7wXtwtWcnINLj1qFjon5zdHWDrh1gElGuPZmjLINBneK6C2N0yohVnO6x9BjDodymMzox1KZOk91Hwkwzatgfz4HagvPwQAUBfO8TjstYpnlf8/VOWW+wOHccxICJPJs4wrexIREREREbUrJlEb8jnBJ7l81ww2YRXuIZ/B3LehXnr1pcZYEV/t6gHY0bKz2eJqLZhQCzNT1x4wXHMbRHanaIcSWc6x2D56l8kDrqy+VB2Q3y7yquPeS83Xcd2RQwCD93hwneL9FFcuv9F/fSIiIiIiImrVqqtqsWSBWd/fr1qa1B0rXMsFONNZMsz5tI9th7BarYQ1eRuqhAM71MN+64aS3Dvl1ztx1Oa3miPEVsn3Ug9ETaUEmEOt3Ozatlh8ni7nz4JqMEAuX+r78ve/AAAQRhOU/y4EBKDeeJF3RVOMVn/WIsDhgDDyKU9ERERERNSWLfvcszfaNhm4d5rwN/d3IxJdAjKEpFS9+wY5B3mopKzAalXi9LgqjBqbib8+/hk/1PbXDjY6mafFqqhWAMCh2F7Y9vcu9B3as+kBtxLsoUbhEWjIp/sKI37mRwMA+f5rwL49HmXKtKdheGMxRLfersuZTFpibcYLgNGEmAFHu47FxGo/hWAyjYiIiIiIqB2KayBr5Peoz1U+w5H0iuyQT71nnHuZc1GCEEMx2l3Jyi156ajYdaBpwbUiTKhReAQY8okO2fqmevtlvs8f4r1Sp/L4LIje/f3fsntvGF//BB1mPAukZkCcMT6kkImIiIiIiKh1Uw8f8irbK61NvGp4El6uxFbjrx9Mes87gSZ9HG1cLJVJXTz2f1oZYEqmNoZddig8Agz5lJ8v8H3Kvx+GLD4E+e7/gDW/eVeIjQ3q1oa0DBiefTvYSImIiIiIiKiFsdnsMJn8pywcNiuqiouQ3LETRN3nzy2/bMS2fV286pbC3sBwysBpqXDNoVZf2Pup1bVBcyTynI0SaynzKFYNMY2/ZivDhBqFhwgwh1pJke9TjhwCuegD/9c0Bv+HKYSADNP4cyIiIiIiIgqPTZv3Yse6JABAbGIRzhzX16uOo8aMpYsBIBFAOU45uQY/LI8H4JlMe9e6BxYluI4ZvjQ9wRXcFcL9yTUcq4jKuqg7HfoN64+8ttmv3xpwyCeFh8MGAJB//hzaedWV/o+ZTE0IiIiIiIiIiFoyh6rqyTQAsFRloWCPtiKlw1ILc/4eSNVRl0xz0ZJp3oJNVDVnuimUa+nzsQlfs5o1P+fVfcXY2LnhFNXe6HhaO/ZQo7CQHzZy6dye/XyXZ+VAmNpP11EiIiIiIqK2yF5rwVefVaIDCnH8xAEex378dDPq9zJb85sBcY48/PZnNoBUABWIN1aixp6EQM4f8Cbmrj2tWWIOV5qrfmIrsksTAFK47qgvShBiFMLHQoRWu4oYY9vvv8WEGoVHUgpQWQ4AkHYbYDBCiAB/mEkpAAAx+DivFyvDG4u96xMREREREVGr883C7ZCxXVCETtj2zSb0PfNI2CqrseLjfFTFec9/BqAumebiL5l2/DGHkRNbhUzLvJBiEn6mC3J9gnU7HkJ2rcGqMuRLNitf9w09Fu8zvv64HOdOSMOB/WZUV1nRJTcNcfFtr4NM208ZUnR06+XaLimGetc1UD+Zq+0bfeRx65JvIj4ByvTnIhAgERERERERRdoeUwd9e0tpZxz4ZQO++sKKirgcj3o7a3cFdb1xFyfj5H9UY9wlKcjq0xtC8d2Ro7FDGp1nN39NV0x6r7AwZdYC9YSTgTq+BFJvgYP6liww46+fgU2rY/Dt4mqs/Nnz91l4qAxLFpjx9efbGnf/FoAJNQoPo2u+M/nlR0BZCeSXH2kLBdgDj7EWPb0nnSQiIiIiIqLWy1xlx/wvi70SMH/ty/VZ35RQEdR1hcGA5E6d9ZU+/aY5ItQNrDUsjeeVyIOv+dyCN2ztizhq46yAdYr3p6OosByfzd+LJQvM+H25dj9rVRYWzy+FxWKFqmrDR8uLzXDYbCHHEWkc8klhIdIy9RcS+cu3rgMOVzJNTJgMuWC2z/OVVz+C/PNniONPDl+QREREREREFBE/f16JRBiRqDSchphrP4ST640QPK7bFxjYYSPeWn23XnbV0OdQgkc8Kza2x5VP0u3/za9pveZCF+huTWm1nKLVsAkDNgy8LmC9335QYRC+h+u+/er2eiVVyE4pQmF5ll4yfHAhcvr7mXc9CthDjcJCjJvg+4DdlWUW/zgLyn9eAlLToTz0iuf5phgoI0+DUAzhDJOIiIiIiIjCZMVvm7Br1wF8+N7GBuuO6HEA/eQyzLYfhBUSgEB+lXZevlqLLmnboQiJ08/SejGN7PY1jEoIK0w2KnflI80kAl9I+N3xzznssjEJNj/TvwXka01R16IEYbxxiNyTaQDw59psLFlghupoGSuLMqFG4ZGW4bt8Z55r22iCyO0Jw7PvQHTuFpm4iIiIiIiIKCze+PoQZs8vRGmFHW99thclezpjwx/xiDP5XmzAaUDCdmSPGIC4FM8OFYcse/CxvRjfqma9LD4tA9cOexYDstb4vlgjeqhFuqeY677Rvoj3yaGu8ulurv2QZ8H+L3DuhDS/9Uee1rgONF98VAmpeq8uGmkc8klh4W9FT/WFB1x1FOZziYiIiJqTQ1WRf7AaG352fXsf6MMMEVFzWbLAjBzEAgL4ZWklOsD30D6n4vLt6JDSB2eNMSIm9VitUHivq1kK7fUs+A5RzTfkU/ga8hlS7q2BWELtFdZU9RpR6wfovuff4cOHceDAAQwcONDv531rvWtU1uQD0P4dWjS/CIowwaHW4KQzkpDZIbnuGCClxOcf7QHUtKAfSmlBPjK69wi6fjgwoUZERERE1Iod2GfGur+LUKHEI7bC+wPs2g/+RL5BW/SJyTWits1aVYGvP3cAiOzf+4YFfwBoeG6rPxwVMABYI6twjtyPcycc67du/Z5Swfackn6r+U8YNXxl4XOz+Qi3/4ef8NrwZDab8fHHH+Occ85BamoqVqxYgU2bNgEAysrKMGrUqCDv42rz8ROdwzfTvOopioLJN4/A7Je36mUn91yM5bvOQ0JMJU4elw2DKQZSVfH5h+UAgJTs7KBiCCcm1IiIiIiIWqjqKgu2bDqErt3TEJ8Qgx15RThqcGcs/aj+6ndZiPVzDWcyDdB6kByZuAObqnoDAE4eXo3kXp3DEzwRRZwzmQYApfkFSO/WtVmuK6XE2hV52HcoHhnZDnTqkoS0ZBN+/t5Zo+Fkml1KrJNVuMyxC38bevq+j48eak5q0OmmRoyEarD7W3j6kEUqgaZzLuTp4/G4xzJ37lwAwMKFC6Eoir76JgCsWrXKZ0LN+bv7zlGK0w3p2jX9Zze9GAwGXNX7ESipsVDq5qlLPu5KuCfghKK0qC+GmFAjIiIiIooAi82BWJMBm979AjtitA8jZ1+UBEVRoNSbCmPJArPbXgr27VQB1AJIRv62+sm00DiTaQCw/M8EHF+zC1kDfX+4JaLms+uLv1BbrWLAJceF5fqF368EMEDfX/l7PMbWTVVtr6mFtaQCCV2yfJ/cgLULV6IARwIAivdr//kzbPA+dOk/ED8vX4H/7uuA00QSYmp24LM4rUdR0CmW+jmfIHNaMgKT5QciBIKO1eeQ0jDyblK334Y+/NTzN6T6mKvsr7/+wgg/7bxbWrDcYUaxtGFsqI9MQE+mtQZMqFF05PJNGxEREbUfHgmyGNc3+19+XNks109MK0aVuQN+dJTBpB7AJpGGIRX7cGz6MQ2e+9uGdJw7sFnCIGqQVB0QSuMmIm8tLNVV+GaJzceRPgCA7QvMGJyxG93OGOJVQ3U4ULXnIOJzMoO+X+Weg/jhtzi4J9MAwKFqH/e/mr8XNpEEwATArB9PqT2AQUNikNQzBzEpiR7nlqzdgRVb3GM4MqhYZtsP4rW6JEqcQYUFEj/bD2Jo9UbAmVDTEzGBJ8RvZD7N47oedwhwgebsKdaYfJ6+ymeEkoHOjoDSfUePpeHzf/31VwyX/hcF2C5rAQBKiAm1RqwnEVVMqFFUiLEXRzsEIiIiomblUFX88e02lJSm4uxLs7H83e2oimv6HC9qzBbEx2Qip3Mc4uJM2LzWCEUYMebCJJhMzrfzabjl3ZXIV9IxsO4DjE2ROPu8OOz9cRO6nnoUli04AEtMKgDgxIGl+GVjun6PtT9sxeBTjgAA2K1W2B02xMTEQjG0/o8LVeXl+G1ZEQYPiEF6rxxImwPGxLhoh9WqHS6ugMOhIruj9nyy2x0oM1cjs0MyHA4H1v+9D336ZyMpSWvnksOV2LHtMA7uSfW4jgNmHHFULIwGBbu2l2PIcO0cU4wBVZUW7M03Y2deLYSaifiUYpx+dp+IP9b6VFWFoijYnncIWR2TkZwcB3NpNTIyk6CqKn74Zieqyzo0eJ21JT2w1i3R/o/B5fj9T1n3N5oIrUeqNmeV0V4DuzEeACBUO6RS/+/S//N5yQIzIHwvDlAe1wkrtgDYYoN7ok0TfEJv1K+3IfXZObj6vTVATJKenHLPjbinVQLlTDyTOaJegi3IbItw9fj1noK/cVpPnyng7bffRnl5Oa6//nrExnpOBhDocTR1pVNfZ4e8Ymgry6i1/n8hqVVShp8U7RCIiIiImoVqs+KLlUXAgUQAHQEBfPFhud4bwx+HvQwGYyqUmGKoVs8P4IkpO/BySSIkgPs61eK4f7jmQes3AD45P4Yodcu2SQgY4+PQY8wwAMCZ/+ruVjsNl+17Ch+YrwcA5Bd2RL7HMFMA8OxhM/bCBBhMMQEfUyTZ7Cq+WmnG6GNSkJKgfawprbCjyuLAhh01OFxgR7d+MTBvVgFkYuVaAGur6s6u9XnNs8+LgzGeyTZ3xWs2Y/H8Uj9HzX72U1Cwoxae7ZyK+gxIw/YNzr0s/L5cAqhxq5EIAa3nVE15ByxZYMYpY+OQlOz7d6SqKmprbFj2eY1HuZQqVFmDjl2tGDGyO2w2B1b/uRelxXbExQvU1kjYbQogBYQiMeKkDoiNM2H71iLY7RLJyTEoKqxBebF7kikWm2EFYK332BtOpvny09oUwM+flzOZBsBHMi2yjl19Dzo+8RoURUH5wteR+O1SlCamwmBQ3OY/q5daEfV3pc9yrazpPdSkW81g0zP+kkk+Q2wgEvchnw3F3NQklq+blJdrk/bPnDkTt956q/NGfk71/QidzjzzTCQlJaG4uBhVVVVYtWqV3/v6EvLja135NCbUKPIMbyyOdghEREREjWYs2oUly5NgV92TDIl+6/vyfelfeHp0EkwDusPXimfFO+MgV2rbQX8cEc4fwc3JE+q03Us/qca4S4wQSiMm/A7Ryk9XoqSmJ7obCnDkJcdAURTkffontlr7AnBvdwU/7vc9bDYFxrpkWvC+XFyLkQMP4mByNnb+ZoWSK9Exy4QDf9v1OsbuwOnHpqDGquqJPKf1u6qx+w+rR5mUDghhgEOthMFkARyZcKjVUEQchFDQe2A1MjITkJIaj8NFlTh4oBKdc1OQnpGAuDgTAOCX5btgqZE4akgHdOqSFvTjcTgcqCivRVp6cM9Pm82OykoLCnaXQEqBXVsNMCg5Qd+vsVRphyKC+2j6w9L6ibqGCaHAIBJRvC9RS3YDALQemlWWujrOyg7UJfascCYBCwEACSHdMytxH8494j0U9X1Cu8/3HyFu3lxYDCZsPfFe7DU1b2+7I2K24ehhDqTYFkF1SLy1ZprH8bO/uwrmrn3R4YFnoTrsKPp9G9IHdkVtcRl+/Mv38+OMUwFbRRUSunZE+TsvIOX3n1GUnqPP+SiEZx8010BObcs5F5as19MscM7E/1EZ9AT34X+Nam4h9+QK0ssvv6xtZDinGtB+J4HTaJqsrCz0798fAJCbmwsAOPbYYzFz5kyPa/m/QiOGsLKHGhERERFR25W3qQp2Nb3Beim1B3DSv45A9dRLcflJjwIAHkrbif+Ye6EnRMCJdqRo/AdCPaHW0AcTAZzUfSl+3jM26Gvv37oD6d26ISHRNYzIOQTOub3882J07WJF32O0D2BSVaE67H57t9lrayCEgGIyoWDdFqzd2gXAAMAA7EJf7NITIH19nh+KbMsuFMYGnsv3141pcPY6UvcKHNhr9zhu3wN8tafc+0Q/hNDmCzMoSYAjqW7blZzZsTEBOwBoPbMMAFJxcI9z39nTSusZ9dcvQP1eYQ5ZBiHjoSgxkNIBVdpgUDx7cKmyCIAzuadAwABVWgE4AMWu9cyScVAU5+81uS7O4B6jqloQn1IOS42CmDgVDjvgsLgmv+9zVDVyOycg99CLiDVasKcoB/En3AZAS+KZTEYcPGDGb8trkdahEhaLxGljekFRBBRFQXWVxavXWbQ4ZCWyO1uRnhmHmBgjbDYHKiussNlU5HZNQXZOCqyrXkLv7MMe5+l9twQw9IpjMbRuX3U4sPy9Hfrw8NP+4UBCp0xIKdGlSxccOHDAa5J99785l+FAwR/6vf551MsotXSEeW01LIVmCEg9aaMYjOg4UuvqGpOahHPr1imx11pQW2hGUreO+lXjstK0uINKltRdX9/3/RonAx5tjh5qjRH4rMYnvBpIH0Z4LKkz7Sl9LDTgWpRAc8YZZ3hViY2N1Xu9ifuvD+KOobVbK8unMaFG4SeO+wfkHz9FOwwiIiKiZjG0y25sPJjrUbZarcSIjjvxxr4M2BUj3h1WjZQBw+qOuj4hKHWTOKtCAAEmdHb/UBH8B7m6Sa31HmqBsyECAv06rEdP6zpMPXwutlV3wPS+h/DgtmxIAHOPr0BS9yEwGE36ogqr12UB69wTPXUxCq1Hk4J0ADHYsj0GW7ab692xuoH4awF0CeJxul1ROpAgPCe4r0i0Y9jRCchINmLl+gK8W2CHDRKX1m7Dudee6/M6f7//J/Yam56wiwaDSHX1ThQGGIT3hP+KMNVVMLmd5xpGCAGvz73uvcb6530AY1wJet41I0AkHQMcS4O01iDW6OwO5soiOOcBzOmUhvP/6fvshMRYnHm+gm8+q/JdAYCqWpHVpQqHixw4/qRsZGWn1DuuIn/3YUgJFOwpR1y8Aced0MPrOmVl1SgprkJ2x2TEJ7iSwK4EVlqAx6nxtRyB62/as6EVgwGnXtXPq753wiyIY26J+HhjFRJid8NiSIc1yCS7MS7WI5nmee26H+5JNI8RnlJ7XQt0MqAlbRo58X5Te3HVT165Jyr9JrYCZ/983yeUmPTXa+frd7hoV66uqsHLL7+Mf2Rl1t3XmzOWDh0aN3zZnSIldu3ahaKiIgwfPrxer0a/YbYaTKhR2EmrJdohEBERETUbNSEe1x7zFBz2ZOHWmvEAACbRSURBVOTFTMCcjfuwujwbZ8XUuvUO832uM6EmG+ih5nGBRk5B0+DoKLcPw6oQsEDCZJAwCgds0gBjrBEGo5aA6ZbyG/LLj/d/KZnR7J+DfnOU43iDZ1Kkj2EbuvXrCOO2r3GfMhDbqjPw9JGHkN7jBGSmGmGol2g4MscOW0HDQ2CHXj4cR1eW4bElFRiqJMGcWIXzT8vBtoIapCYb0bNjDBRFgaqqWPhRKRKlAeVxdqTUuj5OVSTYceSR8RjQLR6xJgX/fHcNqpU4PNy7GIOPP9HnfXftKEJKahzSMxKxbvVeFBdZcepZvWC12GE2V8NgUGAyGZCYFAuTyYgfl+1AWQmg2hNgUOJhiC1CTpdYOBwSVZV2xMQI2KwSVZUqOneNQ9fuaTAYFGzPK0ZqahysVjscDhXxCTE4XFwNo1GBlIDRKGA0KlAUgR69MpFYt5iAo+IwcMc1AIDCfkN9PoZgeQ4VDj2hEhtrwrkT0hp9f0VR0KOX1muuZ+8sv/VSUxOQmhra8E5vPnpzBTkUu2m3dbuvCsDgM1faKD57qHklR+qtVOnxI7goPJN+9VafDOoKDd/r8OHDeP/99wEAN/k4Pm3aNH1C/4j1mNLvE/hRWq1W2Gw2JCYmQkqJefPmIUMN/vO2d68/1zPEfb6zW265xVVHShSUW9ElOQZ2VSLW6Duh668/4pIlSwAAv/32G0aOHAmHw4Hff/9drxEXF4fLL79c22llXdSYUKOwEx1yXH9c8aHNL0JERETUUhkUoEPvPsCm/QC0D5HOD5Kq6vpo4bG6nTOhJhpIqCmuWa3VED+CK9Kzx4NfdR9cJFxzExmg6h+qVLeM3IAOq5AQE4stxU1LqviTEleGjNQqlFba8VZZAkpgR+fawzj32lEe9YQ4Dp06dUJl2TdQtVGMMAmJ7HTfw0m1xxXknHIxMfjbUYZVaiXmHm1CcrwBw/p5ro6oKAomXhrc6oeudvRfxz2xM+TYbvp2XHwMcuK9H9Po03rXK0kLKpZjR3i/B+97RMPnCRjc5sVqqta0TmIYROLhu0/oL+uSacKV4GpwGHjAazt/ePdQc5ao9XpZCbduX9LX+RCoqKhAcnKyz1t6JX+CnEPNf085jTOZ5q7+5PkzZ87E1KlT3StERU1NDfbu3YvevXvjlVde8VknI8MzAXzUUUdh1KhRsFgsUFUVX+62Ahv2etRp6OGc/8HWBmNbHNSiBJ5+/fVXrzq1tbWYM2cOAODh0zyPHThwAFu3bsW6des8yidNmoSkJN+r10YSE2oUfjGuNwPizPFRDISIiIioOQiPn868mBASio/kjXtiy7OHWqAhn26JoKAn4q4/5DPIoTVSwuGc+0i4cnkO6fnhfFT3b9A1cTt2r6zGts5X+bxkn4wf8Y8ev6Gg9AjEDLsctppqxCZ7r+7oLQ0AsGvVtygp69pg/O7zQQVcRU743fFR13dPjcZz/v6a4VJRIgyKqyUCZQaD4d4QrblRGsmZ42lSUqsBrrkXXcMqtWd103vHOeMWHvm0ekM59aSZq0b9kvreeustTJ48GbNnzwYAnNqrm8dxGWDVzwDB+jvgmqQ/CK+++ir+qf8duz9/tcn+e/fujV69euHvv/8GMNjvdQLdU8T2r7ukKxkZSoy+vHSgM176aJdHmXPWT5sSg02JAzEKxdBv6BTiE0QNx99xvb+PDz/80Ge1N998ExMnTkR2duDVtMONCTWKAAHlhnsgV/8GccYF0Q6GiIiIqGnqveF3/yCk9w7z8zlDcespInxNCu26idv1PUkpMWvWLFgs9Yb5ZIzQznS7x8svv4wjjzwSZrMZxx57LHr06AEAsNs9J9p3Ju0UqK6koNuDcH+MybaDevk2axGuv6ArtucVYdCQLsDfayGEFrNiNAWZTHOLw+1xB+xloih6DzolwKfAUD7uKcKtfoBkZ7Cc0Tc1DxVdvntaNlnrGtXVCP4fYDifDtJz8kUtEiH1JFjT5iBrKNnsluSu+/tx3derqke5M5kGaD2y6lXz2n/55Zfx6Nmu8k2bNqGgoABbt2q9qrrE2HHj6YEfDQCMHj0aWP9dwxXh2evYaceOHdixQ1tSBBlBXcYv6bURnL2xueglgl8kBQAOxHUB6hJqnquvNq0no+/zJG699VZUV1dj3rx5qKqqQr9+/XDqqaeioFLFXUu344y0Mth2rtJiCOHWMTG+eyZHEhNqFBHimFEQx4xquCIRERFRq+EaMom6LT2Z5V0NAGCt1iZVVyGw5u/V+PWndejSpQuys7NRXV2N3r17o6CgAGX7dwM4AQBQZi7TeywkJyejoqIiUDh6gsk5/GrTpk0AgMWLF3tUn3Gka9vZQ81utUCp+xRckL8X+WZtSNYx+lz2EhACXzhK0FvEYae1GIlJfTF4WF2vMmcvnEZmDTwSaoEWVRDCNbwswDJ50s+23/p1PXyU5kio+XoutDLuc2Q5kyQOhwPLly9HRkYGhg7VhgDb7Xbs3LkTWVlZSEhIgKIoMBgMHpPnV1e5L0rRmlulYR45AYcDMBjcenaFJ5uoqqrn313djvYbCDy3YyAOhwPr1q1DF/3iWi+vk08+GX3der7Nen0mZPLJAIA/fv8Ned/9jGF9e8G1WEWQNw8wh5rnvmv7xx/+BACYDFoC3woVQFHA21x55ZVIS0uDw8exc889D0uWLPZxRFMjG1rwJXjB9IbdGd8LXWr3Ii+xPwpjc7yO/wO/BX2/YIZwSwCLLu/v+5iU+GFXOTITjFA2aVP1BSKkwPj3t2g7caOAOGDZYeC1D3fWVTDiq7JMIPNMAMAj4i2P8/fF5qLGEI/8uO6QQkFqnAG1tRZM6J+EpJTQvrAJBybUiIiIiIhCIOs+kljsErt2HIC97hOFkBKi7uPFrt35+GDZCgDAVe49ANx7qNV9stm3bx/27dsHAHoPi66Zrrlh3JNMfpNpboIerqh3H5H6B/Gamir9/OLiw9jwt/ahZ9hxzuGtDtisVhyQ2n/ZPj+OahNnVxw6hNjYWOTn5yMhIQFJSUkwGAxITk6GlBLV1dVwOBzYuXMnVFVFVlYWrLWuXndSCHz//ffIycmBzWZDUlKSPhF3gmrTe6jZbRbs3bsXQghYLBbY7XZUVVXBaDSirKISzsFOEsCKFSuwY8cOHH300XA4HCgpKUFcXBz+/vtvJMYIIPkMAEBh4SHsrYW+EEFNTQ2EEDAajTAajUhOTq5LYEgYDNqqmg6HAxaLRf/P2Y4lpWa8/fbbyM3NRWJiIgwGA1RVhaIoqKmpQVxcHFJSPBdfEEJAURQYjUaoqgqHwwG73Y7Y2FjExsZCURQ4HA7Ex7tW6lQUBUIIOBwO2Gw2VFVVeUxg7l4vNjYWBoMBRqPRI2nmcDhgNpshpYTdbodSUw59qjXpPRTt559/9vn79yXeCNx3pmvfPUnscDhgMBiQkJCAmJgY/bHbbDbY7XaUlZWhqkpLRvfr1w9SSnTv3h0xMTH6PEqKokBKCZvNBrPZjNjYWKiqCiEEYmJiYDKZEBsbi7i4OD3R5/ydSSn1/xwOBxISEjxXgBQCNTU1+nNWVVWoqorDhw+jpqYGBQUF6Nq1K7KysiClhPuanZ8t/gwSCk5wHEaW1oz6Y+/WrRtMJpP2nK577FJKrF27FkajEVarFb169UJCQgKMRiMURUFlZSXy8vJw9NFHQ0oJo9GINWvWQEqJYVkSFw53/b4AQEpVfwWREB6/w+zsbCiKgvLycmRnZ8NoNOptcPDgQdTW1up1L4n37HW2fPlydEyxQV8HUrgl7Orazma16vd1JXKkKzi3594ll1wCk8mE4tV/6Lmw+q9krnkdBSwOB2Lrshk2R5lHvSLfL0uABG699Vafh9xfN29fUYOM5GGoMiThn/s2aqe6ZaBKVAUH+o/HsZkSHdNToCpGLP+lwN8tsSzzTCTHKKiwqkgwKeiaGou0OAP+PlCFI8t219Vz/ZaWZZ7pdZ1dCX38PCggrt4iAYsu749PNh7GO2uK8MLZPdA1NRZT5v0NAEiNNWDh2H7IX7weqKlrY5/z2/kmhMCpvVLrtr2OetcPNXmueF5jS9KRHvtltQ4ARszdUotenasxtFN052hnQo2IiIiIWqSvvvoKS5YsgdlsRvfu3TFp0iT06eP/Q0WkHCwuR694oKi6Bou/W4jqLoMAAGaLQx/SaS4z6/XdP04oqgNQtA9Pzg8jzkRTWZn2odBoNML9g0lcfDwGDRqELl26ICUlRU+IpKZ6fjt/6/t/AHAl7VQhMGnSJKxduxbDhw9HXl4edu7cidTUVOTk5EDZp628VmpMgl1flEDCUPeJudKRiPTUJEhI2KSWWKv/can+B/RbTtRqbD1swNpPfgi2SevkIz7TlSBShYINGzZgw4YNXjVv6WfXP4BWlpdj+a+f+Lxih5xOcCXUBFat0oYV/fTTTxBCeCRN3D/HbVy3Druqm9aLSqSNBAA4HCrKy8v1noKtSYpBdSXUmtirzGOqJrd2d08SB5MwzsvLAwBs27atSfE0ty1btmDLFq0nTu4Qg15+cN9eWFWBIeluC5LUyc/P93s9a10yaufOnT6P15+kHQDUuks7hIJagxFJUOsGYvruLVlYWKhv7969228sGlfCR1evK6p78t9kMgXsPepUP8FVYvBMUbhf098AZJMxua7M2UVWhSq9cjMAJH4rqECCSUGsUcGhShucY6g8QhUCJTFaqtCVI3SPQ2JTUQ02FQGAlnQ0ILAKq/b7r7ap2Fpc413Ba/453zommWCxq5g+OhdHdNBeL79f+IdXvQsHZuLCga4FVNyv67lKp/eMdwBgs0ks/7IcOV1M6NItBkkpCowmAcW9UYPoihwooRZjEHj13F6Yt64Y/+iZAkN8MlDotzqyE40YmJ0Ak0HAYpcYktPU1Xibjgk1Cr82P0cCERERNbdff/0Vc+fOxZQpU9C3b1988cUXeOyxx/Diiy96JZIizVxpBeLr5gkTsfoHrWqba4XMLlnJGNSzbuW6za5zE+NiAauW7Bo86CgMO9azJ4KUEkIIHN63C28v13prGYxGnDR6dN1nF61/nJSAxe5c4MDzc42+uiQAFK7G4E6Ade9v6JEA9Dwqra7WIRQ4FPQGUCxU2OvmBzLGJ+vn7ywthVI3n1GVVftZblU8hwHWaxtHXRzm2sOotVU13Jj11Nak6p9QHBA4TY/XRVEECq0V+lCjSpsBiYmJeg+k6upqmEwmpKSkwBAb55wqCFZhxdnDshAbG6v1iFIMsDscqK17jIoQ+ExbsBVpqenompkIu90Og8GA2NhYANrvp6amRu8B5+zV5OxRZjKZ9J5Wq4rq2jRGO7d3795ITNSu6ex9tGvXLmRmZtYlUV2cvYQcDgcURdH/q6mpgcPhQGlpKQDoPc2cPdO09tE+LFdXV6Nr1676dZw96Zy9vpzlHklFRUFMTAysViuysrJgtLr9DqWKs88+GzExMUhLS8O+ffu0NjYYoBgMSE1Nhc1mQ1xsLEpLS7Fp0yaYTCYkJCSgc+fOSI03AYXPaLE5FEycOFF/HHFxcdrvsrISUkrU1lr03oDGGBNKS0qwf/8B7Nvr3QvIOcRUlRKQgNVmhd1mAwBkdsiC1apdy+FwoLKiQv+dOZliYmAyGgEhIKC1o9Vm1dtLQEBVHYiNjUNCYiKUuqGsltpaxMXFITEpGYcPFyM9IxOqqvXgtKquJNhxo0ZBVeKQukcblmgwAEP6ZwOQSE1PB6REZUUFrFYLFMWAuPh4lBQXQzEokKpEZpbWiwxSAoqCkqJC1NTWID4+AYpBgclogtmstWN2x0QAB+ueQ9r9q0QsICu1xxqr4vyxJ6C2tgZWqxXx8XFQVQmH3aY/f0Td88dus6Omphpp6RlITkuH4edvtONSYsKkG1BmNsO4/AMAQIwJOHfcKHz1o/aK0H/wMejXczBK1q0AKpxJRM+EFADYYcf+XX/or2FSAmZbpVs9T879E8ZMBNQX9fIR518Cq0PC6pCwOVTEFe0BsN3ruQIAT/y0z2O/wUmJ9E5xvoebxhoEEmMMKKt2+DjquZcRb8QJXZPQKTkGO0pqsdtsgajwTjjdeFxH9O8Qj+wkExJMDaXqvJUetkN1ADFx2hc3HqFLoLZGbTA//tUn2hc8u7dbsXu71Wedc4PKsQv8p1dXKIqA0QQIE2BSFP3fs8M77Tg7NR2KWSDZ4dljFgAeG6glNlUp6xKkrsdksTgQFxfdlJaQMoi0YhtXVFQEW92LbnMSQqBTp044cOAA2mMzO6acBwAQ51wK5fwrInLP9t7m0cJ2jw62e3Sw3aOjJbW7yWRCVlZWVGNoD6ZPn47evXvj2muvBaDND3TjjTfi7LPPxvnnn+9V32azebyfE0IgPj4eRUVFXhPwN9WPC17DF2V99f382ExUG+Jw+/4v8U72SSg1JqFnbSFiVS0eh1CwLb4TAOCpPR/gnu6Xwaja0af2kN972IUB2+O1uXJyLYeR5Kj1W9epIDYTVYY4nGbegGVpRyFGtaNXgHu42xHXETbFiCf2zMfTXcZ5PYb6tiR00bf7V+/zWacxqgxxKIjVelUYpAN9aw76rbs9riPsihHP7P4AfSy+H+eahG54qOtFAIB0eyU6Wst81gMACIEt8Z0BAHO3vYpk1eK/bhDu6vZPbI/PCfr3R9TaOV8X5myfhQxHFXbHdsC/e/wLBulAD0sxdsR1RKxqw4Ulf2Jeh5FIs1chx2r2uEaVIRYFsVoSJd1eiQSHFftitRn/A70mNRQTEPi1qqmvaVvjO+u9D2NUO6yKlug5t2Q1JhX96Pe8h3MvwN+JPdDJWopUe7Xfek21My4bVsWEBws+xuDqfKyPz8UD3S5BjGpDvGpFmTERp5vX47u0QRBS4oia/SHfQxUK8ur+rXPqbC1Bit1Hj7xmcE52CUaff2mzX9doNAb9Po891Ch84uKB2hqIQcdGOxIiIiJqRZwTnLsnzhRFwaBBg/ThXvV9+umn+Oijj/T9nj174qmnngpL8rPCGuvx4csp89BudEgaiNKUJOyKy/Y6nmYpR8a+nVC6qbArRp/X8GVvbGbDldz0O7QFy9KOgjWEewDapPPp+3ciM73E72PwJZR7hMIhDA1eW5Eq0vbvhLT6XuWua0wlUJdQKzUmodSY5LOeuwR7DWL3F0BKfxMxBSczowjb43NC/v0RtWZxDgvi9++BVG1INZXD0M0Bh2LAjjhtcYIOtaXIKN4LdADMxkSYjf7nwNL+Zl37wb4m+RPsa1VTX9POy1+OX7MHY39CFgbs3wBZfMBv3YyUQiCxBw7EpONATHqT7huMjAO7IKsLkRFvA7oBVsUEq6KtOtO9aCdikvvDajA12+v6/pgM7A/TYpxjHUXo1KlTwxXDiD3UwB5q4SKrKoGSIoiuPSN2z/be5tHCdo8Otnt0sN2joyW1O3uohV9JSQluuOEGPProo+jXzzW993vvvYdNmzbh8ccf9zonkj3Ufln0HszlFtgdrtlhMpUa9BIlqEQMtiNLXy1So0JIie4oRQZqsB8pKERSgzNS1cIEKwxIQfC9m1JgQS8cxl6kokgkAYFWyoQ2xsru0AYldTJUIhdlqEAM8tRsOOr9rRkNbsNJpYJCJCFHeCeyVFXoQz8bwygk0pRamBHvY9Zr174CB7JlJTrDdzLN6QCSkY90mKD6mkW7jmuBhq4wIwuhD1etrwZG5CELahuY/0SgedblrBux1ehVYFsTUddo3sMWBSACrxLZJFKFqmr3NirSYzJ97XWp8c9HbaVX71+e1jNLAaSKXJQhG65hm/uRgkNwJbJ7oQTJqEUeslAtYn3HU7eirPP1phoxgJSIl3bY3W5vEPD7WqPA9fgrEIdkaak3UZr749IWHaiVJsQJu4/HKLSFZCBhgAqHn+VSs1EJK4zojhJUIhZFSEIvHA7Y4hYYsBUdYRdCn/8yXDJQje4w6/sFSKv7d0IgQdaiH4pQiCQcEKmQAf/tcBFSrVtkx7nYhEA3lGK/TEUhkpAhQnstdW8ru0OBGuCVZ8ixR6LnUceHdP1gsIcatQgiMQlIbPhbQCIiIqKmMplMMJlMPo81dxJ21HmXB0zyHuHnPKfOzRqNb03tW9DQY4iWxiTXOwM4Jrxh+dU7SvdtTi3pC432pK20u7/Xu9yIRhG85mz3/kHWi1z3E0++fjfN9XsZFGL9xrZ7tP82wpgaJyIiIiIKnXMlS7PZ7FFuNpuRlpYWlZiIiIiI3DGhRkREREQtitFoRK9evbBhwwa9TFVVbNiwwWMIKBEREVG0cMgnEREREbU448aNw//+9z/06tULffr0wdKlS2GxWHDyySdHOzQiIiIiJtSIiIiIqOUZOXIkysvLsXDhQpjNZvTo0QPTp0/nkE8iIiJqEZhQIyIiIqIWacyYMRgzZky0wyAiIiLywjnUiIiIiIiIiIiIQsCEGhERERERERERUQiYUCMiIiIiIiIiIgoBE2pEREREREREREQhYEKNiIiIiIiIiIgoBEyoERERERERERERhYAJNSIiIiIiIiIiohAwoUZERERERERERBQCJtSIiIiIiIiIiIhCwIQaERERERERERFRCJhQIyIiIiIiIiIiCgETakRERERERERERCFgQo2IiIiIiIiIiCgETKgRERERERERERGFgAk1IiIiIiIiIiKiEDChRkREREREREREFAIm1IiIiIiIiIiIiELAhBoREREREREREVEImFAjIiIiIiIiIiIKARNqREREREREREREIWBCjYiIiIiIiIiIKARMqBEREREREREREYWACTUiIiIiIiIiIqIQMKFGREREREREREQUAibUiIiIiIiIiIiIQsCEGhERERERERERUQiM0Q6gJTAaw9sM4b4+eWObRwfbPTrY7tHBdo+OltDuLSEGCh7f57VNbPfIY5tHB9s9Otju0dES2j2UGISUUoYxFiIiIiIiIiIiojaFQz7DqKamBvfccw9qamqiHUq7wTaPDrZ7dLDdo4PtHh1sd2pp+JyMDrZ75LHNo4PtHh1s9+hore3OhFoYSSmxa9cusBNg5LDNo4PtHh1s9+hgu0cH251aGj4no4PtHnls8+hgu0cH2z06Wmu7M6FGREREREREREQUAibUiIiIiIiIiIiIQsCEWhiZTCZcfPHFMJlM0Q6l3WCbRwfbPTrY7tHBdo8Otju1NHxORgfbPfLY5tHBdo8Otnt0tNZ25yqfREREREREREREIWAPNSIiIiIiIiIiohAwoUZERERERERERBQCJtSIiIiIiIiIiIhCwIQaERERERERERFRCJhQIyIiIiIiIiIiCoEx2gG0VV999RWWLFkCs9mM7t27Y9KkSejTp0+0w2qRNm3ahMWLF2PXrl0oLS3FnXfeieOOO04/LqXEwoULsWzZMlRVVaF///6YPHkyOnXqpNeprKzEm2++iVWrVkEIgREjRuCaa65BXFycXmfPnj2YM2cOduzYgZSUFIwZMwbjx4/3iGXlypVYsGABioqKkJOTg8svvxzDhg0LfyNE2Keffoo//vgD+/btQ0xMDPr164crrrgCnTt31utYrVbMnTsXv/76K2w2GwYPHozJkycjLS1Nr1NcXIw33ngDGzduRFxcHEaPHo3LLrsMBoNBr7Nx40bMnTsXBQUFyMzMxEUXXYSTTz7ZI5728vfyzTff4JtvvkFRUREAIDc3FxdffDGGDh0KgG0eKZ999hk++OADjB07FldffTUAtn04LFy4EB999JFHWefOnfHiiy8CYJtT68bnVPD4Pi/y+D4vOvg+r2Xg+7zI4Pu8OpKa3YoVK+Q///lP+f3338uCggL5+uuvy6uvvlqazeZoh9YirV69Ws6bN0/+/vvv8pJLLpG///67x/FPP/1UXnXVVfKPP/6Qu3fvlk899ZS86aabpMVi0es89thj8s4775R5eXly8+bN8pZbbpEvvviifryqqkpOnjxZvvTSSzI/P1/+8ssv8vLLL5fffvutXmfLli1ywoQJctGiRbKgoEDOmzdPTpw4Ue7Zsyf8jRBhjz76qPzhhx9kfn6+3LVrl3z88cfljTfeKGtqavQ6s2bNkjfccINcv3693LFjh5w+fbqcMWOGftzhcMg77rhDPvzww3LXrl1y9erVctKkSfL999/X6xw6dEheccUV8p133pEFBQXyyy+/lBMmTJB///23Xqc9/b38+eefctWqVXL//v1y37598oMPPpATJ06U+fn5Ukq2eSRs27ZNTp06Vd55553yrbfe0svZ9s1vwYIF8o477pClpaX6f2VlZfpxtjm1VnxOhYbv8yKP7/Oig+/zoo/v8yKH7/M0TKiFwb333itnz56t7zscDnndddfJTz/9NHpBtRL132ipqiqnTJkiFy1apJdVVVXJyy67TP7yyy9SSikLCgrkJZdcIrdv367X+fvvv+Wll14qDx8+LKWU8uuvv5ZXX321tNlsep333ntP3nbbbfr+888/L5944gmPeKZPny5nzpzZrI+xJSorK5OXXHKJ3Lhxo5RSa+OJEyfKlStX6nX27t0rL7nkErl161YppfYG+dJLL5WlpaV6na+//lpeeeWVeju/++678o477vC41wsvvCAfffRRfb+9/71cffXVctmyZWzzCKipqZG33nqrXLt2rfzPf/6jv9Fi24fHggUL5J133unzGNucWjM+pxqP7/Oig+/zoofv8yKH7/Mii+/zNJxDrZnZ7Xbs3LkTgwYN0ssURcGgQYOQl5cXxchap8LCQpjNZhx99NF6WUJCAvr06aO3Z15eHhITE9G7d2+9zqBBgyCEwPbt2/U6AwYMgNHoGuU8ePBg7N+/H5WVlXod99+bs862bdvC9vhaiurqagBAUlISAGDnzp1wOBwe7dGlSxd06NDBo927devm0W13yJAhqKmpQUFBAQBg27ZtPtvUeY32/PeiqipWrFgBi8WCfv36sc0jYPbs2Rg6dKjH6wnA53s4HTx4ENdffz1uvvlmvPzyyyguLgbANqfWi8+p5sX3eZHB93mRx/d5kcf3eZHH93mcQ63ZlZeXQ1VVjycGAKSlpWH//v3RCaoVM5vNAIDU1FSP8tTUVP2Y2WxGSkqKx3GDwYCkpCSPOtnZ2R51nL8js9ms1w10n7ZKVVW8/fbbOOKII9CtWzcAWpsYjUYkJiZ61K3f7vWf5872c6/jq01rampgtVpRWVnZ7v5e8vPzcd9998FmsyEuLg533nkncnNzsXv3brZ5GK1YsQK7du3CE0884XWMz/fw6Nu3L6ZOnYrOnTujtLQUH330ER544AE899xzbHNqtfg+r3nxfV748X1eZPF9XnTwfV7k8X2ehgk1onZuzpw5KCgowMMPPxztUNqFzp0745lnnkF1dTV+++03/O9//8NDDz0U7bDatOLiYrz99tuYMWMGYmJioh1Ou+GchBkAunfvrr/xWrlyJX8PREQRwvd5kcX3eZHH93nRwfd5GibUmllKSgoURfH6tstXBpYa5myzsrIypKen6+VlZWXo0aOHXqe8vNzjPIfDgcrKSv38tLQ0n78T93ukpaWhrKzMo05ZWVmb/r3NmTMHq1evxkMPPYTMzEy9PC0tDXa7HVVVVR7fLLi3R1pamj7Uwv2485jzp682jY+PR0xMTLv8ezEajcjJyQEA9OrVCzt27MDSpUsxcuRItnmY7Ny5E2VlZbjnnnv0MlVVsXnzZnz11Ve477772PYRkJiYiM6dO+PgwYM4+uij2ebUKvE51bz4Pi+8+D4v8vg+L/L4Pq9laK/v8ziHWjMzGo3o1asXNmzYoJepqooNGzagX79+UYysdcrOzkZaWhrWr1+vl1VXV2P79u16e/br1w9VVVXYuXOnXmfDhg2QUurL5fbr1w+bN2+G3W7X66xbtw6dO3fW55Po16+fx32cdfr27Ru2xxctUkrMmTMHf/zxBx544AGvYRK9evWCwWDwaI/9+/ejuLjYo93z8/M9XuTWrVuH+Ph45ObmAtC6AvtqU+c1+PeiPV6bzcY2D6NBgwbh2WefxdNPP63/17t3b5x44on6Nts+/Gpra3Hw4EGkpaXx+U6tFp9TzYvv88KD7/NaDr7PCz++z2sZ2uv7PCbUwmDcuHFYtmwZli9fjr1792L27NmwWCw4+eSTox1ai1RbW4vdu3dj9+7dALQJanfv3o3i4mIIITB27Fh88skn+Ouvv5Cfn49XXnkF6enpGD58OAAgNzcXQ4YMwcyZM7F9+3Zs2bIFb775JkaOHImMjAwAwIknngij0YjXX38dBQUF+PXXX/Hll19i3Lhxehxjx47F2rVrsWTJEuzbtw8LFy7Ejh07MGbMmIi3SbjNmTMHP//8M2677TbEx8fDbDbDbDbDarUC0CYEPvXUUzF37lxs2LABO3fuxKuvvop+/frpL06DBw9Gbm4uXnnlFezevRtr1qzB/PnzcdZZZ8FkMgEAzjzzTBQWFuK9997Dvn378PXXX2PlypU455xz9Fja09/LBx98gE2bNqGwsBD5+fn6/kknncQ2D6P4+Hh069bN47/Y2FgkJyejW7dubPswmTt3rv5837p1K5555hkoioITTzyRbU6tGp9ToeH7vMjj+7zo4Pu86OD7vOjg+zyNkFLKiNypnfnqq6+wePFimM1m9OjRA9dcc02b/AasOWzcuNHn3AKjR4/GTTfdBCklFi5ciO+++w7V1dXo378/rr32WnTu3FmvW1lZiTlz5mDVqlUQQmDEiBGYNGkS4uLi9Dp79uzBnDlzsGPHDiQnJ2PMmDE4//zzPe65cuVKzJ8/H0VFRejUqRMuv/xyDBs2LGyPPVouvfRSn+VTp07VX3ysVivmzp2LFStWwG63Y/DgwZg8ebJH99mioiLMnj0bGzduRGxsLEaPHo3LL78cBoNBr7Nx40a888472Lt3LzIzM3HRRRd5vcC1l7+X1157DRs2bEBpaSkSEhLQvXt3jB8/Xl+NiG0eOQ8++CB69OiBq6++GgDbPhxefPFFbN68GRUVFUhJSUH//v0xceJEfSgM25xaMz6ngsf3eZHH93nRwfd5LQff54Uf3+dpmFAjIiIiIiIiIiIKAYd8EhERERERERERhYAJNSIiIiIiIiIiohAwoUZERERERERERBQCJtSIiIiIiIiIiIhCwIQaERERERERERFRCJhQIyIiIiIiIiIiCgETakRERERERERERCFgQo2IiIiIiIiIiCgETKgRERERERERERGFgAk1IiIiIiIiIiKiEDChRkREREREREREFIL/B3F9MCFB80eQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1500x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(15, 10))\n",
    "\n",
    "fig.suptitle(\"Performance\")\n",
    "\n",
    "axs[0].plot(np.arange(len(data[\"coin0:close\"])), data[\"coin0:close\"], label=\"price\")\n",
    "axs[0].set_title(\"Trading Chart\")\n",
    "\n",
    "performance_df = pd.DataFrame().from_dict(env.action_scheme.portfolio.performance, orient='index')\n",
    "performance_df.plot(ax=axs[1])\n",
    "axs[1].set_title(\"Net Worth\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "orDict = OrderedDict()\n",
    "for k in env.action_scheme.portfolio.performance.keys():\n",
    "    orDict[k] = env.action_scheme.portfolio.performance[k][\"net_worth\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame().from_dict(orDict, orient='index').plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_workers': 3,\n",
       " 'num_envs_per_worker': 1,\n",
       " 'create_env_on_driver': False,\n",
       " 'rollout_fragment_length': 200,\n",
       " 'batch_mode': 'truncate_episodes',\n",
       " 'train_batch_size': 4000,\n",
       " 'model': {'_use_default_native_models': False,\n",
       "  'fcnet_hiddens': [256, 256],\n",
       "  'fcnet_activation': 'tanh',\n",
       "  'conv_filters': None,\n",
       "  'conv_activation': 'relu',\n",
       "  'post_fcnet_hiddens': [],\n",
       "  'post_fcnet_activation': 'relu',\n",
       "  'free_log_std': False,\n",
       "  'no_final_linear': False,\n",
       "  'vf_share_layers': False,\n",
       "  'use_lstm': True,\n",
       "  'max_seq_len': 32,\n",
       "  'lstm_cell_size': 256,\n",
       "  'lstm_use_prev_action': False,\n",
       "  'lstm_use_prev_reward': False,\n",
       "  '_time_major': False,\n",
       "  'use_attention': False,\n",
       "  'attention_num_transformer_units': 1,\n",
       "  'attention_dim': 64,\n",
       "  'attention_num_heads': 1,\n",
       "  'attention_head_dim': 32,\n",
       "  'attention_memory_inference': 50,\n",
       "  'attention_memory_training': 50,\n",
       "  'attention_position_wise_mlp_dim': 32,\n",
       "  'attention_init_gru_gate_bias': 2.0,\n",
       "  'attention_use_n_prev_actions': 0,\n",
       "  'attention_use_n_prev_rewards': 0,\n",
       "  'num_framestacks': 'auto',\n",
       "  'dim': 84,\n",
       "  'grayscale': False,\n",
       "  'zero_mean': True,\n",
       "  'custom_model': None,\n",
       "  'custom_model_config': {},\n",
       "  'custom_action_dist': None,\n",
       "  'custom_preprocessor': None,\n",
       "  'lstm_use_prev_action_reward': -1,\n",
       "  'framestack': True},\n",
       " 'optimizer': {},\n",
       " 'gamma': 0.99,\n",
       " 'horizon': None,\n",
       " 'soft_horizon': False,\n",
       " 'no_done_at_end': False,\n",
       " 'env': 'TradingEnv',\n",
       " 'env_config': {'data':        coin0:open  coin0:high  coin0:low  coin0:close  coin0:volume  \\\n",
       "  0         7195.24     7196.25    7178.20      7180.97    202.942868   \n",
       "  1         7180.97     7186.40    7175.47      7178.45    128.242654   \n",
       "  2         7178.19     7185.44    7176.23      7179.56     83.487458   \n",
       "  3         7179.35     7183.98    7175.46      7177.02     97.141921   \n",
       "  4         7176.47     7194.04    7175.71      7190.86    103.520522   \n",
       "  ...           ...         ...        ...          ...           ...   \n",
       "  51192    37551.93    37676.28   37371.58     37485.83   1482.366650   \n",
       "  51193    37485.83    37569.99   37307.28     37327.88    909.243137   \n",
       "  51194    37327.88    37500.00   37308.78     37433.91    630.870914   \n",
       "  51195    37433.92    37646.49   37129.41     37167.13   1617.834241   \n",
       "  51196    37167.14    37286.52   37019.00     37180.34   2150.343351   \n",
       "  \n",
       "         coin0:diff_close  coin0:soft_close  coin0:diff_open  coin0:soft_open  \\\n",
       "  0                   NaN       7176.263094              NaN      7176.204453   \n",
       "  1             -0.000351       7182.167203        -0.001985      7181.653846   \n",
       "  2              0.000155       7187.739422        -0.000387      7186.813606   \n",
       "  3             -0.000354       7192.979751         0.000162      7191.683731   \n",
       "  4              0.001927       7197.888189        -0.000401      7196.264222   \n",
       "  ...                 ...               ...              ...              ...   \n",
       "  51192         -0.001762      37498.112677        -0.007075     37555.936801   \n",
       "  51193         -0.004222      37471.460642        -0.001762     37539.791935   \n",
       "  51194          0.002836      37443.603363        -0.004222     37523.170312   \n",
       "  51195         -0.007152      37414.540841         0.002837     37506.071932   \n",
       "  51196          0.000355      37384.273076        -0.007152     37488.496795   \n",
       "  \n",
       "         coin0:diff_high  ...  coin3:diff_close  coin3:soft_close  \\\n",
       "  0                  NaN  ...               NaN        204.539822   \n",
       "  1            -0.001370  ...          0.000000        204.752039   \n",
       "  2            -0.000134  ...          0.002889        204.953776   \n",
       "  3            -0.000203  ...         -0.000538        205.145035   \n",
       "  4             0.001399  ...          0.005123        205.325816   \n",
       "  ...                ...  ...               ...               ...   \n",
       "  51192        -0.007201  ...         -0.003296        586.073954   \n",
       "  51193        -0.002825  ...         -0.002432        585.205870   \n",
       "  51194        -0.001865  ...          0.001611        584.295371   \n",
       "  51195         0.003899  ...         -0.004720        583.342456   \n",
       "  51196        -0.009608  ...          0.001049        582.347125   \n",
       "  \n",
       "         coin3:diff_open  coin3:soft_open  coin3:diff_high  coin3:soft_high  \\\n",
       "  0                  NaN       204.423290              NaN       204.932100   \n",
       "  1            -0.002790       204.630283        -0.001859       205.149719   \n",
       "  2            -0.000490       204.827751         0.002250       205.356450   \n",
       "  3             0.003134       205.015693        -0.000538       205.552293   \n",
       "  4            -0.000391       205.194110         0.006189       205.737250   \n",
       "  ...                ...              ...              ...              ...   \n",
       "  51192        -0.006881       586.992862        -0.008669       588.942953   \n",
       "  51193        -0.003381       586.219583        -0.001978       588.207812   \n",
       "  51194        -0.002878       585.408814        -0.000974       587.436570   \n",
       "  51195         0.002159       584.560553         0.003582       586.629226   \n",
       "  51196        -0.004994       583.674801        -0.004659       585.785781   \n",
       "  \n",
       "         coin3:diff_low  coin3:soft_low  coin3:diff_volume  coin3:soft_volume  \n",
       "  0                 NaN      203.934801                NaN         504.810985  \n",
       "  1            0.001522      204.167718          -0.948680         484.519435  \n",
       "  2            0.000441      204.389480           0.436332         465.417755  \n",
       "  3            0.001372      204.600086          -1.394931         447.505945  \n",
       "  4            0.000539      204.799537           2.198206         430.784005  \n",
       "  ...               ...             ...                ...                ...  \n",
       "  51192       -0.005047      584.668748           0.065307         913.979605  \n",
       "  51193       -0.002146      583.887286          -0.687514         914.278253  \n",
       "  51194        0.000258      583.069941           0.001739         913.244467  \n",
       "  51195       -0.002030      582.216711           0.259641         910.878246  \n",
       "  51196       -0.001223      581.327598           0.175647         907.179590  \n",
       "  \n",
       "  [51197 rows x 60 columns],\n",
       "  'window_size': 10},\n",
       " 'env_task_fn': None,\n",
       " 'render_env': False,\n",
       " 'record_env': False,\n",
       " 'normalize_actions': False,\n",
       " 'clip_rewards': None,\n",
       " 'clip_actions': True,\n",
       " 'preprocessor_pref': 'deepmind',\n",
       " 'lr': 5e-05,\n",
       " 'log_level': 'WARN',\n",
       " 'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       " 'ignore_worker_failures': False,\n",
       " 'log_sys_usage': True,\n",
       " 'fake_sampler': False,\n",
       " 'framework': 'tf',\n",
       " 'eager_tracing': False,\n",
       " 'explore': True,\n",
       " 'exploration_config': {'type': 'StochasticSampling'},\n",
       " 'evaluation_interval': None,\n",
       " 'evaluation_num_episodes': 10,\n",
       " 'evaluation_parallel_to_training': False,\n",
       " 'in_evaluation': False,\n",
       " 'evaluation_config': {},\n",
       " 'evaluation_num_workers': 0,\n",
       " 'custom_eval_function': None,\n",
       " 'sample_async': False,\n",
       " 'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       " 'observation_filter': 'NoFilter',\n",
       " 'synchronize_filters': True,\n",
       " 'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "  'inter_op_parallelism_threads': 2,\n",
       "  'gpu_options': {'allow_growth': True},\n",
       "  'log_device_placement': False,\n",
       "  'device_count': {'CPU': 1},\n",
       "  'allow_soft_placement': True},\n",
       " 'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "  'inter_op_parallelism_threads': 8},\n",
       " 'compress_observations': False,\n",
       " 'collect_metrics_timeout': 180,\n",
       " 'metrics_smoothing_episodes': 100,\n",
       " 'remote_worker_envs': False,\n",
       " 'remote_env_batch_wait_ms': 0,\n",
       " 'min_iter_time_s': 0,\n",
       " 'timesteps_per_iteration': 0,\n",
       " 'seed': None,\n",
       " 'extra_python_environs_for_driver': {},\n",
       " 'extra_python_environs_for_worker': {},\n",
       " 'num_gpus': 0,\n",
       " '_fake_gpus': False,\n",
       " 'num_cpus_per_worker': 1,\n",
       " 'num_gpus_per_worker': 0,\n",
       " 'custom_resources_per_worker': {},\n",
       " 'num_cpus_for_driver': 1,\n",
       " 'placement_strategy': 'PACK',\n",
       " 'input': 'sampler',\n",
       " 'input_evaluation': ['is', 'wis'],\n",
       " 'postprocess_inputs': False,\n",
       " 'shuffle_buffer_size': 0,\n",
       " 'output': None,\n",
       " 'output_compress_columns': ['obs', 'new_obs'],\n",
       " 'output_max_file_size': 67108864,\n",
       " 'multiagent': {'policies': {},\n",
       "  'policy_mapping_fn': None,\n",
       "  'policies_to_train': None,\n",
       "  'observation_fn': None,\n",
       "  'replay_mode': 'independent',\n",
       "  'count_steps_by': 'env_steps'},\n",
       " 'logger_config': None,\n",
       " 'simple_optimizer': -1,\n",
       " 'monitor': -1,\n",
       " 'use_critic': True,\n",
       " 'use_gae': True,\n",
       " 'lambda': 1.0,\n",
       " 'kl_coeff': 0.2,\n",
       " 'sgd_minibatch_size': 128,\n",
       " 'shuffle_sequences': True,\n",
       " 'num_sgd_iter': 30,\n",
       " 'lr_schedule': None,\n",
       " 'vf_loss_coeff': 1.0,\n",
       " 'entropy_coeff': 0.0,\n",
       " 'entropy_coeff_schedule': None,\n",
       " 'clip_param': 0.3,\n",
       " 'vf_clip_param': 10.0,\n",
       " 'grad_clip': None,\n",
       " 'kl_target': 0.01,\n",
       " 'vf_share_layers': -1}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv4",
   "language": "python",
   "name": "venv4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
